{"start_date": "1543582678", "end_date": "1543878357", "thread_id": "t3_a1rp4s", "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 66, "text": "<combinatorylogic>: A microbenchmark demonstrating how little optimisations JVM JIT is doing in order to keep compilation time tolerable is actually valuable. Why is your thoroughly dumb comment so upvoted is beyond my understanding.<pron98>: Except the benchmark does not demonstrate that at all; it's your prejudice that does. This particular optimization (sequential reduction) exists in C2, but was disabled because it was shown to be harmful in cases that were deemed more important. Now this decision is being reconsidered. See [my other comment for the details](https://www.reddit.com/r/programming/comments/a1rp4s/why_is_2_i_i_faster_than_2_i_i_java/easdflt/).<combinatorylogic>: It's a known fact, that C++/CLI code is generally faster than the equivalent C# code. Simply because the former is doing some simple optimisations before generating bytecode. It's obvious that JIT will always be inferior to a standalone compiler that have nearly unlimited time to do all the optimisations. And yes, this example perfectly demonstrates it. A standalone compiler can try sequential reduction, and if it's harmful - roll back. JITs will never be able to afford backtracking.<pron98>: &gt; It's a known fact, that C++/CLI code is generally faster than the equivalent C# code. Don't compare .NET's compiler to HotSpot's C2 and Graal, or to Zing's (LLVM-based) Falcon. It's generations behind. &gt; It's obvious that JIT will always be inferior to a standalone compiler that have nearly unlimited time to do all the optimisations. It's hard for me to understand how something that is counterfactual (see the reasoning behind disabling the optimization, which is not because of cost, and the benchmarks in the talk I linked to) can be obvious. While it is true that a JIT may choose to invest less in optimizations, the availability of a profile as well as the ability to speculate and deoptimize opens up many more possibilities for optimizations. An AOT can have all the time in the world, but as many of these problems are undecidable, all the time in the world is literally not enough. A JIT can (and does) just guess based on past information, and later back out if it's wrong. Also, HotSpot uses tiered compilation. It has a fast low-tier compiler (C1), and a slow, top-tier optimizing compiler (C2 or Graal). This is precisely so that the optimizing JIT can take its time. &gt; And yes, this example perfectly demonstrates it. The only thing this example perfectly demonstrates is that the change to disable this optimization has been successful, and that it is, indeed, disabled. &gt; A standalone compiler can try sequential reduction, and if it's harmful - roll back. JITs will never be able to afford backtracking. Except JITs do exactly that, while an AOT cannot back out, because it doesn't know if the optimization is harmful. The entire idea of HotSpot is that it can speculate optimizations and later deoptimize (although in practice it doesn't deoptimize based on profiles, but rather chooses an optimization based on the profile collected by the interpreter and the low-tier compiler). AOT compilers have a much harder time backing out because they cannot speculate on real executions. <combinatorylogic>: &gt; It's generations behind. It's making different trade-offs. &gt; e.g. see the benchmarks in the talk I linked to We all know how limited benchmarks are. Performance on a very warmed-up hot path is what JITs can do well - but it does not necessarily translate to the overall performance. So, comparing with C1 or even bytecode interpretation is totally justified. &gt; because it doesn't know if the optimization is harmful. In many cases it does - with a decent performance model. &gt; The entire idea of HotSpot is that it can speculate optimizations and later deoptimize It's good for some very locally scoped optimisations, but I do not believe JITs will ever be able to do it on a global scale. The costly optimisations I'm talking about include partial specialisation and inlining, condition hoisting (potentially, very many levels up the call tree), heap to stack demotion, and, worst of all, memory reshaping. Standalone compilers can spend hours finding the best path, but JITs do not have hours of warming up and collecting profiling data, even on your synthetic trivial benchmarks where you actually have a hot path and optimising it may (over some long time) be beneficial for the overall performance. <pron98>: &gt; It's making different trade-offs. Mostly trading off person-centuries of effort. :) &gt; We all know how limited benchmarks are. I agree, but you just said that one benchmark \"perfectly demonstrate\" an \"obvious\" thing (that just happens to be factually untrue), and now you say that a benchmark that shows the opposite is suspect because benchmarks don't demonstrate anything? &gt; but it does not necessarily translate to the overall performance. Sure, and sometimes it does. But you said something was \"obvious\" and it is clearly not. &gt; In many cases it does - with a decent performance model. In fewer cases than a JIT, as the effectiveness of optimizations often depends on information that can only be known at runtime. PGO AOT compilers can do some of that, but [not as well as a JIT](https://www.reddit.com/r/programming/comments/a1rp4s/why_is_2_i_i_faster_than_2_i_i_java/eash8cy/). &gt; The costly optimisations I'm talking about include partial specialisation and inlining, condition hoisting (potentially, very many levels up the call tree), heap to stack demotion, and, worst of all, memory reshaping. C2 and Graal do all that, and I think that it is not a terrible overgeneralization to say that they do more of the optimizations you mentioned than clang/gcc precisely because they can speculate. I.e., they can (and do) *speculatively* inline *virtual* calls many levels deep, speculatively elide branches etc.. &gt; Standalone compilers can spend hours finding the best path They can spend all the time they want, but they don't know what the best path is without runtime information. I think you grossly overestimate the kind of insight that can be gained with, say, a 10x or even 100x additional effort on a problem that is generally undecidable (and in practice is well more than exponential in the code size). On the other hand, speculation and deoptimization can elide the hard problem altogether (of course, there are many complications and special cases, but saying that AOTs are \"obviously\" better than JITs is ridiculous because neither theory nor practice suggests they are). <combinatorylogic>: &gt; Mostly trading off years of work. :) It's the warm-up time. .NET gives you a decent performance pretty much straight away, and allows to run AOT and cache the results. JVM is slower until it warms up - which may be too late. Actually, CLR is not really a JIT at all. &gt; I agree, but you just said that one benchmark \"perfectly demonstrate\" an \"obvious\" thing And how is it not obvious? Something that would have been just an instcombine case for a standalone compiler is considered too costly for a JIT. &gt; as the effectiveness of optimizations often depends on information that can only be known at runtime This kind of optimisations have a much smaller potential than those I mentioned. &gt; C2 and Graal do all that That's very unlikely - as I said, they don't have hours, and there is no magical runtime information that can cut hours to seconds. &gt; than clang/gcc precisely because they can speculate Of course clang/gcc don't do them at all - as I said, they're very expensive. &gt; They can spend all the time they want, but they don't know what the best path is without runtime information. The fact that hoisting a condition removes tons of virtual calls is known in compile time. The fact that a condition in runtime happened to be true for few thousands of loop iterations won't help you much when it'll turn out that is's just an aberration and you must waste more time deoptimising. No magical runtime information can really help any such optimisations. It can be somewhat useful, but to a very limited extend. &gt; On the other hand, speculation and deoptimization can elide the hard problem altogether As I said, I do not really care about hot path performance. It's not interesting for most of the load profiles I'm interested in (which is real time, for example - I need my timing constraints to be known statically, long before code is executed). I also care about an overall performance (actually, latency) for the load profiles that do not have tight loops with distinct hot path. The only case where this hot path optimisation is somewhat relevant is HPC. The overall performance without warming up cannot be improved with all this runtime information. Yet, it can be dramatically, often - in orders of magnitude, improved by doing those global static optimisations I mentioned above. And yes, JITs are very limited in theory, and even more limited in practice. I cannot see how they ever will be useful for anything but this niche hot-path kind of a load profile.<pron98>: &gt; Something that would have been just an instcombine case for a standalone compiler is considered too costly for a JIT. Because these are just not the facts. This optimization was disabled because it was found to be more harmful than useful overall, not because it is too costly, and now they're reconsidering and may refine its use. That it's not used because it's too costly was just speculation on your part (plus, why do you think it's more costly?), that you are now repeating despite evidence to the contrary. &gt; This kind of optimisations have a much smaller potential than those I mentioned. No. They precisely enable those optimizations you mentioned in many more cases. &gt; That's very unlikely - as I said, they don't have hours, and there is no magical runtime information that can cut hours to seconds. I don't know what to tell you other than learn how C2 and Graal work. There is no magical algorithm that can turn the trillions of years required into hours. Runtime data, however, can make solving the problem itself moot -- you no longer need to prove that the optimization is correct in all cases, just for the cases you've seen. &gt; Of course clang/gcc don't do them at all - as I said, they're very expensive. C2 and Graal do them all the time. They're not expensive for JITs because they can speculate and don't need to prove correctness (which cannot be proven in hours any more than in seconds). E.g., [here](https://twitter.com/ChrisGSeaton/status/619885182104043520) is an optimization Graal does for Ruby code. &gt; The fact that a condition in runtime happened to be true for few thousands of loop iterations won't help you much when it'll turn out that is's just an aberration and you must waste more time deoptimising. That's an empirical hypothesis (like the generational hypothesis for GC), and you're also making an empirical hypothesis. Neither of these is obvious, but one of them has been tried and tested and found worthwhile for two decades now -- for the use cases it is recommended. &gt; which is real time, for example - I need my timing constraints to be known statically, long before code is executed Hard realtime is one of the cases where JITs are bad, which is why nobody recommends using JITs for hard realtime. Even realtime Java relies on AOT compilation for realtime threads. But this is completely different from \"must waste more time deoptimizing,\" which is an empirical statement. Here, it's not total time you care about, but you cannot possibly allow for the nondeterminism of deoptimization, and that's a completely different requirement. Realtime often trades off performance for determinism, and AOT vs JIT is one of the immediate consequences of that. &gt; And yes, JITs are very limited in theory, and even more limited in practice. AOT are very limited in theory, and even more limited in practice. Seriously, you're making such gross universal statements, then say that you're actually interested in a niche use-case, and then go back to making universal assertions. The reality is that in theory, sometimes JITs have the advantage and sometimes AOTs, and reality shows the same.<combinatorylogic>: &gt; This optimization was disabled because it was found to be more harmful than useful overall While for a typical standalone compiler it would have been an instcombine-style optimisation, which is really hard to screw up. Simpler than what JVM is doing, but a tiny bit more expensive. &gt; They precisely enable those optimizations you mentioned in many more cases. My point is, the way it's done in JVM is different from, say, LLVM. &gt; I don't know what to tell you other than learn how C2 and Graal work. I know how Graal works (and I wrote a number of Turchin-style supercompilers over a decade before Graal appeared, none of the Graal ideas are new). &gt; Runtime data, however, can make solving the problem itself moot -- you no longer need to prove that the optimization is correct in all cases, just for the cases you've seen. And it seems like you don't understand the difference yet. No runtime data can help here. It gives you a bit more constraints to consider, but does not help in discovering the globally beneficial reductions. &gt; C2 and Graal do them all the time. And on a very local scale. Uninteresting. One thing is to know that a certain method argument was \"1\" ten times and \"2\" three times, you can do a couple of little optimisations with this knowledge. A totally different thing to know *statically* for sure that it can only ever be \"1\", \"2\" or \"3\", and that when you specialise for \"2\", you eliminate all of the virtual calls in the underlying call tree. &gt; E.g., here is an optimization Graal does for Ruby code. What exactly in this example that cannot be done statically? Any Turchin-style supercompiler must fold it into a constant, and it's not even expensive, there is no space for speculative specialisation. &gt; That's an empirical hypothesis (like the generational hypothesis for GC), No it is not. A static call is *always* cheaper than a virtual call, you do not need any fancy cost model to know it for sure. If a specialisation or a condition hoisting eliminates your virtual calls, it's a beneficial specialisation, period. Consider this example: You have a rectangular area, and a code processing it contains a condition detecting the edges (e.g., think of a typical convolution implementation). With a static optimiser you can *prove* (wasting tons of time) that the order does not really matter, so you can split processing in two parts, one kernel only processing the edges, and you do not care much about its performance, and another kernel running on the inside of the area only, which you can trivially vectorise. Then you run both kernels in any order. None of your fancy runtime-data-collecting JITs will ever be able to capture what's going on here and realise that you can do such a transform. &gt; Hard realtime is one of the cases where JITs are bad, Bad for the soft realtime too (which is, for example, any GUI - humans have an aversion to visible hiccups). It's hard to find a load profile it's good for, actually. &gt; AOT are very limited in theory, and even more limited in practice. Nope. As I said, the knowledge of runtime hot spots can give you a few percentage point optimisations - you'll hint your branch prediction accordingly, you'll slack on optimising less important things, and so on. While very expensive static optimisations can discover orders of magnitude worth transforms, such as one I described above. <pron98>: Look, your hypothetical speculations are predictable, but unfortunately many of them are empirically false, so I don't know what to tell you other than look at the data. The optimizations you imagine are theoretically possible in AOTs are not possible in practice because they take trillions and trillions of years. In reality, most of the work done by all kinds of compilers is heuristic, and so the question is just what happens in reality. In practice, JITs do more specializations than AOTs, they run programs in languages like JS and Ruby in speeds that AOTs don't even try (an AOT compiler cannot possibly do the constant folding I showed, because there is no way for it to know that the expression is constant without solving halting). I really so no point in arguing what you imagine could be done, would be appropriate etc., when there's so much data right in front of us: to put it very bluntly, JITs run typed languages at speeds comparable with the best AOT compilers, and they run untyped languages orders of magnitude faster. Those are just the plain facts.<combinatorylogic>: &gt; Look, your hypothetical speculations are predictable What speculations? I described a very real and practical optimisation case (one I'm closely familiar with). &gt; The optimizations you imagine are theoretically possible in AOTs are not possible in practice because they take trillions and trillions of years Lol wut?!? The one I described was implemented and worked perfectly, as I said, it does not even need too much time. &gt; I really so no point in arguing what you imagine could be done, I *implemented* that optimisation (in a GPGPU context - another place where none of that JIT stuff will ever be allowed). &gt; JITs run typed languages at speeds comparable with the best AOT compilers (small print: only after warming up a hot path for quite a while, but never mind those GUI hiccups) Again, I'm not talking about trillions of years - all such optimisations can take hours for a large project, but that's perfectly bearable for release builds. <pron98>: I honestly have no idea what you're talking about, nor what some optimization you once implemented for a GPU has anything to do with what I said. I said at the very beginning that JITs are bad for some niche software. But the reality is that in the places they are used -- which nowadays means much of the software in the world -- JITs perform better than AOTs for the wide use cases people use them. Obviously, JITs are not good for certain kinds of software, and they are neither used nor recommended for those cases. But making universal statements about their inadequacy requires a truly exceptional ability for reality-denial. But feel free to write a good AOT compiler for JS.<combinatorylogic>: &gt; nor what some optimization you once implemented for a GPU has anything to do with what I said. That's an example of a *relatively* cheap static optimisation which is very beneficial, and cannot be done by any JIT. And there is a lot of such cases, where a static compiler can spend few minutes - up to hours and do something that's orders of magnitude beneficial, while a JIT will never have enough time and enough runtime profiling data to do such a transform. &gt; I said at the very beginning that JITs are bad for some niche software. And they're bad for anything interactive. And for anything short-living. And for anything with too complex global control flow, lacking distinct hot paths. I.e., for pretty much everything. They're good for long running server applications which do not have even soft latency requirements. Sounds like a very niche thing to me. &gt; JITs perform better than AOTs for the wide use cases people use them And I'm struggling to imagine all those cases (besides the one I mentioned above). &gt; But feel free to write a good AOT compiler for JS. Such dynamic languages should not exist (unless they're just toys). They have no benefits whatsoever, only drawbacks. <pron98>: &gt; and cannot be done by any JIT. I don't understand what this has to do with this discussion, though. Did I say that there are no optimizations that could be done by an AOT and not a JIT? I said that there are some that can be done by AOTs and not JITs, and some that can be done by JITs and not AOTs, and that the reality is that in the huge portion of software where JITs are used, they perform really well. &gt; And they're bad for anything interactive. They are not. I think you're uninformed. &gt; I.e., for pretty much everything. Right, except that this is a clearly counterfactual statement, and I know you know that. Much of the world's software these days is written in Java and JavaScript. I challenge you to write AOT compilers that outperform the current JITs. And please don't say something like \"who cares about JS\" or \"people shouldn't write in JS\" or whatever. This is not what we're talking about. &gt; And I'm struggling to imagine all those cases (besides the one I mentioned above). No need to imagine, once you choose to speak about software by actual observation rather than speculation. &gt; Such dynamic languages should not exist (unless they're just toys). They have no benefits whatsoever, only drawbacks. Oh. You said that. OK, I think this discussion is over.<combinatorylogic>: &gt; They are not. I think you're uninformed. No, thank you very much, I use Java GUI applications frequently. They're very annoying. &gt; Much of the world's software these days is written in Java and JavaScript. And a lot of it just sucks. &gt; I challenge you to write AOT compilers that outperform the current JITs. No need to. Use gcc or clang/llvm. &gt; Oh. You said that. OK, I think this discussion is over. Mind providing *any* justification at all for existence of such languages? Just, any? Don't tell me they're somehow more \"productive\", that'd be a lie.<pron98>: The hiccups you experience in Java are not due to the JIT, and I am not interested in discussing language choices <combinatorylogic>: You're preaching superiority of JITs - so you already initiated a discussion about language choices. Also, if you want to keep language choice out - stick to Java, and try to explain, how exactly such a perfectly fit for static analysis language would benefit from a JIT, if a sufficiently smart (I know, done it deliberately) AOT is available. And it's not just hiccups - it's *inconsistent* performance and a huge memory overuse (which, in turn, lead to hiccups too). JIT is very much responsible for it.<pron98>: I have not preached, nor do I believe in, the superiority of JITs. I simply pointed out that in the areas they are used -- which constitute a huge portion of software these days -- JITs *generally* (not *always*) emit faster code than AOTs. There are situations when AOT is clearly preferred. I have also certainly not started a discussion on language choice. When I said I am not interested in discussing languages I didn't just mean that I don't want to get into that. I mostly meant that personally, I find debates over empirical claims without empirical data uninteresting, and the subject of programming languages is one that is of particular little interest to me. What I do find interesting is the tendency of some programmers to keep making all sorts of strong empirical claims, undeterred by either the lack of data or clear contrary evidence.<combinatorylogic>: &gt; I simply pointed out that in the areas they are used -- which constitute a huge portion of software these days -- JITs generally emit faster code than AOTs. Which includes an awful lot of Java. Not a very dynamic language, even if you try to really stretch the definition. That's why I'm puzzled with your references to JIT success with the dynamic languages in this context. <pron98>: And also an awful lot of JS. JITs emit *generally* faster code for both Java and dynamic languages, and people are happy with the tradeoffs in most domains Java is used (and can use AOT Java compilers when they want different tradeoffs).<combinatorylogic>: &gt; And also an awful lot of JS. I'd never claim that dynamic languages can be efficiently statically compiled. That's pretty much obvious. This is all about Java - because, as far as I'm concerned, there is absolutely nothing a JIT can do to make it run faster than a \"*sufficiently smart*\" static compiler. I'm very well aware that such a static compiler does not exist yet (not for Java, at least), so we're obviously out of empirical context.<pron98>: As you can move the definition of \"sufficiently smart\" around all you want, it is a vacuous claim, rather than an empirical one, which is even less interesting. However, it can easily be shown that a JIT performs extremely useful optimizations for Java (such as virtual call inlining, and then all other optimizations enabled by the inlining) that are reducible from halting for an AOT compiler. In other words, an AOT compiler that performs the same optimizations provably cannot exist. Would *other*, tractable optimizations balance the scales? It's impossible to answer as you have not defined what your compiler could or could not do, and are basically arguing for the superiority of something which definitely does not exist, and it is impossible for you to know how it would perform in comparison if it did. So you are basically vehemently arguing about make-believe things. This is something I find interesting. <combinatorylogic>: &gt; such as virtual call inlining, and then all other optimizations enabled by the inlining Static devirtualisation is more costly, and may need aggressive specialisation and condition hoisting to be enabled, but it's also more powerful (as it's not constrained by only your runtime profiling data). It's quite dishonest of you to accuse AOT compilers of not being able to cater for the edge cases, while JIT will only start being beneficial after some very high hot path threshold is reached. JITs cover a much smaller number of paths, while AOT knows most of them, excluding only the edge cases that are very unlikely to ever affect performance anyway. More so, you can easily build the very same kind of speculative optimisation that JITs do into your statically compiled code - e.g., speculate that some condition is true, specialise a call tree for it (if your cost model shows it's beneficial), and fall back to the original version (still optimised, no C1 / interpreter crap) if the condition (cheap to test on entry) is false.<pron98>: &gt; Static devirtualisation is more costly, and may need aggressive specialisation and condition hoisting to be enabled, but it's also more powerful (as it's not constrained by only your runtime profiling data). JIT devirtualization is less costly, and it's also more powerful (as it's not constrained only by what you can tractably prove). Now, to test which of those two statements is right, all we need to do is compare the cost and performance of existing JIT compilers to your imaginary AOT ones. &gt; It's quite dishonest of you to accuse AOT compilers of not being able to cater for the edge cases, while JIT will only start being beneficial after some very high hot path threshold is reached. I am not accusing AOTs of anything. I *love* AOT compilers, and I also love JITs. They both make different tradeoffs, though. What I am arguing against is the practice of making such confident empirical claims about nonexistent things (which, I admit, is an interesting take on making empirical claims without empirical evidence). The rest of your comment includes further speculations that are impossible to empirically test (on cost v. benefit, which is what you're trying to argue), as they're made about nonexistent things. E.g.: &gt; speculate that some condition is true, specialise a call tree for it No, you cannot, because you cannot know in advance what the hot paths would be, and call-paths can grow super-exponentially (this is why JITs have a code cache, and throw out unused compilations). But maybe you can find some working heuristics. Who knows? &gt; no C1 / interpreter crap Many Java applications rely on dynamic code loading. So I understand you believe that your imaginary compiler would easily beat those that have been running much of the world's software in both elegance and performance, but sheer performance is not the only reason for the interpreter/C1/C2 \"crap\". I mean Java AOT compilers already emit code that's sufficiently fast for many Java applications, even without beating C2, but don't support some useful and widely used Java features. <combinatorylogic>: &gt; as it's not constrained only by what you can tractably prove That's your conjecture, not backed by evidence or a theory. You just expect that in certain cases that you deem important, runtime information (along with the cost of accumulating it) will yield useful optimisations. No way you can know for sure. &gt; all we need to do is compare the cost and performance of existing JIT compilers to your imaginary AOT ones. Or, compare the performance of the existing AOT compilers (say, for C++) vs. your imaginary omnipotent JITs. The fact that nobody cared enough to build a good AOT for Java only means that for most of the use cases the current performance is sufficient. After all, there are people out there who are totally fine with Python performance. &gt; I am not accusing AOTs of anything. I love AOT compilers, and I also love JITs. You're obviously biased, since you're working on JITs. I'm biased too, specialising on hardware-assisted compute accelerators (i.e., as static as it gets, no place for anything dynamic here at all), so we have to rely on evidence. All the evidence I cared to look at suggests that there is little chance a hypothetical C++ JIT will outperform even the not so smart existing stock C++ compilers, outside of some very carefully constructed microbenchmarks. No such thing was ever demonstrated. &gt; No, you cannot, because you cannot know in advance what the hot paths would be Again... You do not care in the AOT if it's hot or not. You optimise everything. If there is a performance benefit (from your cost model), you exploit it, without caring too much that this path will only take up 0.0001% of the total run time. You can afford it in an AOT. &gt; and call-paths can grow super-exponentially Paths that are significantly beneficial (to deserve a specialisation) don't just pop up out of nowhere, there is a very limited number of them in any typical code base. &gt; But maybe you can find some working heuristics. You don't need to, you have tons of time to try many different paths, as long as you know there is a potential for a beneficial specialisation. For the kind of languages I'm dealing with, devirtualisation is not interesting (not even possible, luckily, for the complete absence of virtual calls and high order functions), but in this regard it's not any different from a potential for vectorisation, for example, so I can safely extrapolate the supercompiler behaviour observable on such cases onto the OO languages. &gt; Many Java applications rely on dynamic code loading. Which, from the user perspective, is not much different from the usual DLLs. Dynamic *code generation* is rarely used, so all the dynamic code you can possibly load is already available statically. <pron98>: &gt; That's your conjecture, not backed by evidence or a theory. Well, that compilers can only optimize based on what they can prove, and that many proofs are intractable is pretty much basic theory, but yes, my general statement on who \"wins\" in a completely hypothetical fight was a satire of yours. &gt; Or, compare the performance of the existing AOT compilers (say, for C++) vs. your imaginary omnipotent JITs. I'm not talking about imaginary JITs but about C2 (or Graal), and I linked to a talk that makes the comparisons. &gt; The fact that nobody cared enough to build a good AOT for Java only means that for most of the use cases the current performance is sufficient. There are good AOTs for Java. They're just not as good as your imaginary one, that no one has bothered to build such a compiler (e.g. that can inline virtual calls as well as C2/Graal) for any language. &gt; All the evidence I cared to look at suggests that there is little chance a hypothetical C++ JIT will outperform even the not so smart existing stock C++ compilers, outside of some very carefully constructed microbenchmarks. No such thing was ever demonstrated. I don't think a JIT for C++ is worth it, either. But the reason is that the language, like Rust, is built around explicit control. The programmer has to tell the compiler whether she wants a virtual method or not. JITs are good for languages that don't give you this level of control. &gt; You can afford it in an AOT. No, you really, really can't. The reason is that the number of paths you'll need to consider would result in generated code that cannot fit in the entire universe. JITs don't even specialize for all the paths *they actually encounter in a particular execution* because of code size considerations (although they may throw out some old compilations to make room for new ones), let alone specializing for all possible paths. &gt; there is a very limited number of them in any typical code base. This is an interesting conjecture, but until you show me an AOT compiler that can correctly guess the common types encountered at a virtual callsite, whether or not it is possible to build such an AOT that generally beats a JIT remains unknown. &gt; You don't need to, you have tons of time to try many different paths, as long as you know there is a potential for a beneficial specialisation. You don't have the time to try 2^10000 paths. When someone manages to find heuristics that are good enough, you'll see efficient AOT compilers for JS. &gt; Which, from the user perspective, is not much different from the usual DLLs. Ah, but Java calls are *much* faster than DLL calls. They are inlined and optimized together with application code. In fact, when you run TruffleRuby with C extensions on the JVM, they optimize and inline Ruby and C code. &gt; Dynamic code generation is rarely used I wonder where you're getting this data. First, Java reflection heavily uses code generation, as do lambdas (although they can do without it, at the cost of performance). Second, many popular frameworks rely on dynamic code generation. In fact, only recently a [new framework/spec](https://microprofile.io/) was introduced which tries to *not* rely on dynamic code generation.", "num_messages": 26, "avg_score": 2.5384615385}
{"start_date": "1544609698", "end_date": "1544758550", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 39, "text": "<sh_pa_ic_rk_ie_tr>: How Reddit ranking algorithms work \u2013 Hacking and Gonzo \u2013 Medium <13steinj>: To note this is outdated. Reddit stopped being open source 2 years ago. Even while open source, * The \"hot\" algorithm went through a variety of secret changes * so did the front page algorithm * now we have \"best\" for posts, which is not \"best\" for comments * it's extremely possible the comment sorts have changed, I ran the math on a variety of comments recently and found that the best sort on site did not match the open source version, but I'm not sure if my findings are statistically significant, and I'm explicitly biased in finding comments that specifically break the rule.<NoMoreNicksLeft>: It's difficult to believe that reddit was, long ago, not Facebook.<13steinj>: It has nothing to do with the redesign so I have no idea why you're commenting this as it is unrelated.<Glader_BoomaNation>: How is what he said related to the redesign??<13steinj>: When most people say what he did they are referring to the redesign because of its aesthetics and because of the new user profiles (also redesign). It's definitely completely unrelated to my comment either way.", "num_messages": 6, "avg_score": 6.5}
{"start_date": "1543589699", "end_date": "1543636721", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 19, "text": "<speckz>: A Collection of Well-Known Software Failures <pardoman>: Now we have JS and can avoid most of those errors. /s<ThirdEncounter>: To be fair, JS has been a thing for 20+ years now. Try ~~Python~~ or MongoDB.<Axxhelairon>: python is 4 years older than javascript and even java<ThirdEncounter>: Had to look it up. You were not kidding. I stand corrected.", "num_messages": 5, "avg_score": 3.8}
{"start_date": "1544698020", "end_date": "1544744172", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 2554, "text": "<miguelmotah>: How Unix programmers at restaurants search menus for their favorite plate <heptadecagram>: No Unix programmer would invoke a [useless use of cat](http://porkmail.org/era/unix/award.html)<barubary>: Similarly, no awk programmer would invoke a useless use of grep: cat menu.txt | grep shrimp | awk '{print $NF}' can be simplified to awk '/shrimp/ { print $NF }' menu.txt The use of `foo &amp;&amp; bar || baz` instead of a proper `if` / `else` is a [known bash pitfall](http://mywiki.wooledge.org/BashPitfalls#cmd1_.26.26_cmd2_.7C.7C_cmd3) (i.e. it's broken in the general case).<barubary>: In fact, the whole thing: $ cat menu.txt | grep shrimp | awk '{print $NF}' | sed -E 's/\\$([0-9]+)\\..*/\\1/g' | xargs -I {} test {} -lt 10 &amp;&amp; echo \"Available!\" || echo \":(\" can be reduced to the following: perl -nle 'if (/shrimp.*\\$(\\d+)/) { print $1 &lt; 10 ? \"Available!\" : \":(\"; exit }' menu.txt ... which I'd argue is - shorter - easier to read - more correct - faster (and probably uses less memory to boot)<tritratrulala>: Not always faster (multiple processes vs one single process).<emn13>: You're downvoted, but you are, of course, entirely, 100%, completely correct. But in general, for small datasets and simple problems: fewer processes usually wins.<PC__LOAD__LETTER>: For small datasets and simple problems it usually doesn\u2019t matter, is a more accurate assessment IMO. <MrDick47>: No he is technically more accurate and this is a sweeping statement.<PC__LOAD__LETTER>: No if you care about performance for small problems and datasets, don\u2019t use shell one-liners.<MrDick47>: No one said anything about language used. Its point is that processes create overhead, and small datasets/problems will create enough overhead where the execution is slower than if it was a single process. The point is valid across all languages including shell. Stop arguing for the sake of arguing, it's annoying.<PC__LOAD__LETTER>: The shell is a place where you glue a bunch of different tools/processes together. That\u2019s the Unix way. If you\u2019re nervous about performance, write an actual program in an actual programming language where you have tighter control over exactly what\u2019s happening. There\u2019s a big difference between a shell one-liner and a custom program. Who\u2019s arguing for the sake of arguing again?<emn13>: &gt; Who\u2019s arguing for the sake of arguing again? I'm pretty sure I'd argue that we all are. Though I still have a lively internal debate on the matter.<PageFault>: [Oh, have I got the package for you!](https://sourceforge.net/projects/suicide-linux/files/) It will helpfully resolve any mis-typed commands to `rm -rf /`", "num_messages": 13, "avg_score": 196.4615384615}
{"start_date": "1543583755", "end_date": "1543690548", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 107, "text": "<sheokand>: Not all CPU operations are created equal <o11c>: ~~Note that the info on RAM is from 2008, it's faster than 60 ns now.~~<mewloz>: Not much, and maybe even not at all if you do random accesses on a gigantic area. As a rule of thumb consider that after a completely out of cache random access, transferring on the order of 1kB will take approx the same time as the initial access latency.<o11c>: A typical DDR4 stick is something like: 3000MHz, 15-17-17-35. At the same speed, cheap/performance only changes those numbers by \u00b11 usually. If the RAM speed changes, the clock-based timings scale proportionally, keeping the absolute timings in nanoseconds the same. In nanoseconds, those are 5ns-5.7ns-5.7ns-11.6ns. Now, there's certainly some CPU bookkeeping overhead, but not 50ns worth.<AndyBainbridge>: &gt; In nanoseconds, those are 5ns-5.7ns-5.7ns-11.6ns. Now, there's certainly some CPU bookkeeping overhead, but not 50ns worth. I agree, it is hard to see what causes the difference between the SDRAM manufacturers latency figures and the observed 60-100ns of latency people say \"RAM access\" has. First up, if I understand Wikipedia correctly, the latencies are more like 13ns, not 5ns or 5.7ns like you said: https://en.wikipedia.org/wiki/DDR4_SDRAM#JEDEC_standard_DDR4_module[57] Next, we have to consider what we mean by a RAM access. Lets say we've got DDR4-2666 and we write a C program that creates a 2 GByte array and reads 32-bit ints from that array, from random offsets, as quickly as possible and calculates their sum. The table is too big to fit in cache, so the CPU will have to read from RAM. Here's what I think happens: CPU core fetches and decodes a read-from-memory instruction. Virtual address translated to physical address via TLB. Since our address is random, we will almost certainly get a TLB miss, which means the memory controller has to get the page table entry for the virtual address we requested. The funny part here is that the page table entries are stored in RAM. If the one we want is not already in the cache, then we have to read it from RAM. The even funnier part is the page tables are in a tree - we need to walk the tree from the root node that represents all of memory, through many layers until we get to the leaf node that represents the page we are interested in. If the cache is empty, each hop on the tree traversal causes a read from RAM. This gets boring quickly, so I will assume we have enabled huge pages and that the page table entry is in cache. As a result, we get the physical address in a few clock cycles. Now the CPU looks for the data in each level of cache: L1 checked for hit. Fail. L2 checked for hit. Fail. L3 checked for hit. Fail. By now on 4 GHz Skylake, 42 cycles or 10ns have gone by since the read instruction started to execute - https://www.7-cpu.com/cpu/Skylake.html. So now the memory controller has to actually start talking to the DDR4 DIMM over a memory channel. Let's assume that the part of RAM we want to read isn't already busy (refreshing, being written to etc). Let's also assume that somebody else hasn't already read from the part we want, because if they have, the \"row buffer\" might already contain the row we want, which would save us half the work. Let's assume nothing else in the CPU is busy using the memory channel we need. Given the C program I described, and an otherwise unloaded system, there's &gt;90% chance these assumptions are true. Now the memory controller issues an \"active\" command, which selects the bank and row. (https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory#SDRAM_construction_and_operation). It waits some time for that to happen (this is the row-to-column delay and is about 10-15ns). Then the memory controller issues a \"read\" command, which selects the column. Then it waits a bit more (this is the CAS latency, another 10-15 ns). Then data starts to be transmitted back to the memory controller. Then somehow the data gets back to the CPU and the read instruction can complete. There are various clock domain crossings on the way to and from the SDRAM - the CPU, memory controller, memory channel and memory internal clocks are all running at different rates. To transfer data from one clock domain to the other, I guess, costs something like half a clock cycle of the slower clock, on average. Then there are overheads like switching the memory channel from read to write takes some cycles. I think I can make all this add up to about 40ns. I wrote the C program and timed it (I had to take special measures to prevent the CPU from speculatively issuing lots of RAM reads in parallel). The result was 60ns per read. So there's about 20ns of overhead remaining that I don't understand.<o11c>: &gt; the latencies are more like 13ns, not 5ns or 5.7ns like you said Ooh, right. Because it's *double* data rate and uses both edges of the clock, those all need to be doubled. So 10ns or 11.4ns for the small numbers, and 23.2 for the big one. Wikipedia's clock-relative CAS numbers *are* a little higher than sticks on the market now, explaining why they said 13 instead of 10. &gt; The funny part here is that the page table entries are stored in RAM Fortunately in physical RAM not virtual RAM, or we'd never get anywhere ... &gt; add up to about 40ns Hmm ... we can add 1ns for the time it takes electricity to move 8 inches in copper. Maybe L3 cache eviction hurts that much? Assuming 2 sticks (and 2 channels) like most end-user systems use, there's a 50% chance of requiring a write to the same DIMM (not sure how the \"4 sticks but still only 2 memory channels\" case works). I'm not sure how overlappable that is, since RAM commands are pipelined these days.", "num_messages": 6, "avg_score": 17.8333333333}
{"start_date": "1544650205", "end_date": "1544700321", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1494, "text": "<Niomar>: Visual Studio Code (Version 1.30) Released <Ermaghert>: At this point VSC has pretty much everything that I personally need. So while this update adds features I'll not use anytime soon, I want to give a shout out to the dev team for this amazing piece of software, the constant and frequent influx of updates, superb changelogs and all the great customizability options! <ivquatch>: Semantic syntax highlighting would be the coup de gras<LaM3a>: Lol it's spelled coup de gr\u00e2ce, what you wrote basically means \"hit of fat\".<Miranox>: C*\u00e2lisse* *de tabarouette*<404_GravitasNotFound>: Bone apple tea....<korynt>: Omelette du fromage<yogobot>: http://i.imgur.com/tNJD6oY.gifv This is a kind reminder that in French we say \"omelette *au* fromage\" and not \"omelette _du_ fromage\". [Sorry Dexter](https://www.youtube.com/watch?v=8nW3-9gdjYA) [Steve Martin](https://youtu.be/DOJDNChwgBw?t=2m49s) doesn't appear to be the most accurate French professor. --- ^(The movie from the gif is \"OSS 117: le Cairo, Nest of Spies\" https://www.imdb.com/title/tt0464913/ )<vonforum>: Bad bot<bleuge>: Good bot<WhyNotCollegeBoard>: Are you sure about that? Because I am 99.52053% sure that vonforum is not a bot. --- ^(I am a neural network being trained to detect spammers | Summon me with !isbot &lt;username&gt; |) ^(/r/spambotdetector |) [^(Optout)](https://www.reddit.com/message/compose?to=whynotcollegeboard&amp;subject=!optout&amp;message=!optout) ^(|) [^(Original Github)](https://github.com/SM-Wistful/BotDetection-Algorithm)<moonsun1987>: Good bot", "num_messages": 12, "avg_score": 124.5}
{"start_date": "1543619552", "end_date": "1543627992", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 38, "text": "<playaspec>: The Arcane Algorithm Archive <claytonkb>: I'm not knocking your link, but if you think that's incredible then [Rosetta Code](http://www.rosettacode.org/wiki/Category:Programming_Tasks) is going to blow your mind.<dwhite21787>: Ooh that\u2019s cool", "num_messages": 3, "avg_score": 12.6666666667}
{"start_date": "1543494710", "end_date": "1543537293", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 7, "text": "<atreadw>: How to measure DNA similarity with Python and Dynamic Programming <arogozhnikov>: Why not just using edit distance (which is what is proposed, as I can see)?<atreadw>: The Needleman-Wunsch algorithm, which is implemented here, is formulated in terms of maximizing a score because its goal is to optimize how two sequences (DNA, RNA, amino acids etc.) are aligned - so as to maximize that score. It can be viewed as effectively using a (potentially) weighted edit distance, with different scores for insertions / deletions / matches. So effectively maximizing the similarity between the two sequences is equivalent to minimizing the edit distance.", "num_messages": 3, "avg_score": 2.3333333333}
{"start_date": "1544738010", "end_date": "1544833811", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 503, "text": "<davidk01>: Why bad software architecture is easy to monetize <papertowelroll17>: Meh, so pay this guy 2x what the other companies are asking and he'll think of all the things and get them done? I'm certainly skeptical of contract software for various other reasons, but *do the simplest thing that can possibly work* is good design. You don't make software projects cheaper or better by being more thorough in the planning phase, you achieve that by not building shit that you don't need and doing your best to avoid making simple problems complicated.<s73v3r>: It's good design until it isn't. These things almost always change, and building a system that isn't flexible to change is much simpler than one that is. <papertowelroll17>: I've yet to see code that was intentionally designed from the beginning for \"flexibility\" actually be flexible in a way that was useful. (And to be clear, I have personally made this mistake). The easiest code to change is the code that is the shortest, simplest, easiest to comprehend, etc. Extra bells and whistles that don't deliver business value make change more difficult. I think _actual_ flexible code is typically the result of factoring out common elements of working systems, not from engineer brainstorming sessions in the planning phase.<throwawaysonlyalways>: So true. This has become so *obvious* to me over time that I can't understand why it's not common knowledge between senior devs and still there's experts out there pitching flexibility, genericness, reusability, with not a word on simplicity, and they seem convinced that layers and layers of abstraction and indirection is \"good design\".<Snowtype>: I have to deal with an \"architect\" 9-to-5 programmer on my team who insists on having wrapper layers between every piece of the application (including duplicate data structure wrappers), who's now blaming me for delaying the project after pushing onto me the whole work of connecting the poorly matching wrappers with the actual working code. I'm just going to delete the whole pile of junk instead.<Remarkable_Button>: &gt;9-to-5 programmer Why specify that this person likes reasonable work hours? And don't you have any input? You're a team, aren't you. That sounds like more of a problem than this specific programmer. <Snowtype>: It's more of a term to describe him as someone who's just a programmer as a means to get billable hours. But yeah, the team dynamic is terrible.<s73v3r>: It's a pretty poor term. It derides people who dare to have a life outside of work, and contributes to the idea that programmers have to work insane amounts of hours if they want to be in this industry. <saltybandana>: He explained what he meant, and it's the meaning I gleaned from it without his clarification, so I think you need to drop it. At some point we all need to grow up, put on our big boy pants, and realize that we should be taking someone for their intent. And yes, I'm sure that's also offensive in some way.<s73v3r>: No, at some point we need to grow up, put on our big boy pants, and realize that how we communicate, and the words we choose are important. <saltybandana>: eh, you want to die early from being offended at everything, I guess that's your prerogative. myself personally, I think life is much more enjoyable when you treat others' as creatures to understand rather than enemies. but you do you.", "num_messages": 12, "avg_score": 41.9166666667}
{"start_date": "1543623450", "end_date": "1543645098", "thread_id": "t3_a1qyws", "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 9, "text": "<j16180339887>: Thanks for the feedback, actually I am looking for a new maintainer.<Kamek_pf>: You're looking for a new maintainer for a tool you wrote less than 24 hours ago ?<j16180339887>: \\&gt; less than 24 hours ago I committed 24 hours ago doesn't mean I only take a day to get it done. I wrote this only to get the info I want, now it's done. If no one wants to taker over this, I'll update it slowly.", "num_messages": 3, "avg_score": 3.0}
{"start_date": "1543577760", "end_date": "1543621793", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 98, "text": "<aka-rider>: How to optimize C and C++ code in 2018 <ariasaurus>: I no longer considered them the same language after smart pointers and exceptions were usable. By this I mean that I don't really consider them similar, even though some subset of C sometimes compiles in C++ and might actually run correctly if you're lucky. Editing to add: No, I didn't get that the wrong way around ... a number of times I've seen people try to compile C with a C++ compiler and it ended in tears.<aka-rider>: They never were the same language. Although, C and C++ compilers/optimizers share some code. In the context of this article, approach to C and C++ code optimization is the same - watch for an optimal data flow, let a compiler to do low-level stuff.<ariasaurus>: I agree that much of the article is relevant to both. Once, a lot time ago, some of us used C++ as \"C with classes\" and the languages were considered very similar, with a few caveats like 'a' being either a char or an int. Since then they have diverged so much, not just in the complexity &amp; compatibility of the codebases, but in the design patterns, and the new syntax, and features, and there's a lot of pressure being applied to the \"C with classes\" people to change their ways. <aka-rider>: I can imagine. I have been thinking of C as a something similar to C++ until I saw \"Deep C\" slides. https://www.slideshare.net/olvemaudal/deep-c/<Syrrim>: Is there a PDF anywhere?<aka-rider>: I've never met a PDF", "num_messages": 7, "avg_score": 14.0}
{"start_date": "1543623296", "end_date": "1543721169", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 31, "text": "<Crypto_To_The_Core>: Al Lowe reveals his Sierra source code collection\u2014then puts all of it on eBay <Poddster>: I imagine it's as shit as his games. <Kittoes>: Dunno why you're getting downvoted; there might be one gem in that library but the large majority were utter garbage. Games like Reader Rabbit and Outnumbered were not only on the scene sooner but more entertaining AND actually taught me how to read/do math.<shevegen>: He is being downvoted because his statement is incorrect. <https>://en.wikipedia.org/wiki/Leisure_Suit_Larry#Series Leisure Suit Larry was a success back then, so the statement \"as shit as his games\" is simply incorrect. So of course he is being downvoted.<Kittoes>: Sure it was a success but, as a game, humanity should be able to agree that it is (and was) objectively bad. The series is remembered fondly largely because of the campy humor that resonated with a generation and not at all because it was seriously fun or groundbreaking in any way.<jephthai>: I've introduced my kids to the kings quest, space quest, and quest for glory games. The campy humor works just fine, and they've enjoyed them immensely. It's an issue of taste, and I'd say the are many who like them for each who doesn't.", "num_messages": 6, "avg_score": 5.1666666667}
{"start_date": "1544681821", "end_date": "1544760495", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 6, "text": "<TwiiKuu>: Get Free for Aoao Photo Watermark 8.7 Full Version <altmanyip>: niche<TwiiKuu>: thanks this is full version enjoy...", "num_messages": 3, "avg_score": 2.0}
{"start_date": "1543583384", "end_date": "1543615072", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 42, "text": "<DevFRus>: Unreasonable gatekeeping of basic coding skills is why women in psychology \"can't program\". <alienus>: EDIT: /u/DevFRus seems to focus mostly on non-programming and equality stuff, please keep SJW stuff out of here, its part of the reason I unsubbed from /r/programming, and I dare say, most people are not appreciative of this topic. I would have expected more from the [author of the article](https://avatars3.githubusercontent.com/u/5082092?s=400&amp;v=4) who [clearly did program beyond simple things](https://github.com/oliviaguest?utf8=%E2%9C%93&amp;tab=repositories&amp;q=&amp;type=source&amp;language=) and did manage to manipulate the same kind of complexity that I routinely deal with. this should mean that she has the same experience that I detail further below, but somehow manages to construct this kind of disingenuous article that casts unjust blame at people who are barely involved when it comes to programming! &gt;Why can\u2019t they code? Because they aren\u2019t taught how to code. **STOP** This is not the problem. I learned to code, and anyone worth his salt as a programmer, learned programming *while programming*. A recruiter once asked me: Alienus, how do you think I should go about learning to program a bit so that I can better understand my clientele? I told him, find a problem you want to solve, and then solve it. Programming is an iterative process where you keep refining instructions. I had to stop reading the article at the quote, too many red flags going off. I'd rather read fifty shades of grey. Okay, I did pick over it a bit more. Most of it is just the author quoting places where other people, who are not programmers themselves, stated that 'X or Y cannot program'. But here is a quote that is not seen in the right context, and fits in with my narrative: &gt; Not everybody can learn to code. False, everyone can learn to code, but not everyone will be a good programmer. Just like some people will not be good musicians; you can only be a good musician if you enjoy making music, and are self-aware and critical enough to keep improving where you make mistakes. Ask any good musician if they were happy with their performance, or if something could be improved... Ask any professional developer if they are happy with any code they wrote 2 months ago, or if they would improve something. Right; the problem is not with the programming, the problem is with people **LETTING OTHER PEOPLE TELL THEM WHAT THEY CAN OR CAN NOT DO\" <TL>:DR; entire article is some kid whining about not having been stimulated and encouraged properly. <warchestorc>: I find your attitude interesting because that's exactly how I learnt to code.. By just doing it. Solving a problem. I googled my problem in plain English and by adding ' in c#' to it. But every time I suggest that people just say they don't see how you can do it without formal classes <alienus>: That is because these people **cannot become programmers by themselves**. They need external motivation; this means you can never become good at anything. True motivation comes from within, is self-sustained. If it is not, you will never be 'talented' or 'good' at it. In short, not only women cannot be taught programming, men also cannot be taught programming.<warchestorc>: I'm not going to disagree with you but I will point out how much shit you will receive for expressing rhatm<alienus>: And yet if you talk to recruiters, as a young programmer, one question always asked is: do you like to program in your free time?<warchestorc>: I've never been asked that. (UK FYI). I thibk its very important very very early on and once in a while but I thibk as a habit its not healthy. <alienus>: I'm not doing to disagree with you, but I will point out how much shit the UK is in; it is a sinking ship. Saying anything on the internet or in real life that can be construed as offensive can get you sued and convicted for hate speech. With how things are in the UK, I take as much credence to your \" I will point out how much shit you will receive for expressing rhatm\" as I would a Chinese person telling me that.<warchestorc>: It's so odd you jump to talking about governments when we're clearly talking about intra personal relationships <alienus>: &gt; intra personal relationships It's so odd your government/police/legislation cares about **inter** personal relationships. Because, yes, that is what they do. Your laws are terrifying, as a Dutchman they make me wonder what happened to 'Great' Britain. But more to the point -and not at all odd!- the UK is setting very wrong precedents when it comes to anything that looks like discrimination; racial, gender, ageism, or even ' disablist' Take a look at this... [sad example](https://www.youtube.com/watch?v=wcr7hg8SJik), or the story of that lady that got arrested over wishing someone a 'gay day', or many similar incidents, such as a woman getting arrested over racist slurs, which... well, she quoted a rap song, in memory of a passed friend :( Your country makes me very, very sad. Sad in the same way this programming post made me sad. Sad in the way that you seem to feel we should not be a society that values meritocracy above 'equality'. Inherently.. we are not equal, the value of individuals for the roles they seek to fulfill in society differs, and it differs on ability, and there is a big correlation between interest- even in your free time- and at work. As a rather extreme example, would a politician be any good if they absolutely hated parties, hated philosophy and did not like watching the news in their free time? So to can you expect a programmer who does not sharpen his skills because he likes it, but only programs what must be done to not be any good at his job. That is, not compared to those who do care. By the way, did I mention I unsubbed from /r/programming because of posts like this? Because I did, and I keep being reminded why, every time I speak out about being passionate about programming and being severely not amused at people who seem to care more about token equality.", "num_messages": 10, "avg_score": 4.2}
{"start_date": "1544740950", "end_date": "1544815148", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 197, "text": "<Fabien_C>: Should Have Used Ada #1 - How some famous vulnerabilities wouldn't have survived Ada <telionn>: This makes a strong case for always using braces with conditional (if) blocks. The risks go beyond ordinary programmer error. <annexi-strayline>: As mentioned in the article, it is easy to make a rule like this, but how do you enforce it? The argument being is that Ada doesn't give a choice, it simply requires it. This is a consistent pattern throughout Ada. Another example is that case (switch) statements must always cover every possible outcome - and that is enforced by the compiler.<Jataman606>: How do you enforce good names for variables and functions? Theres no tooling for that.<annexi-strayline>: Sounds like you're just looking for arbitrary holes to poke through.<Jataman606>: My point is that it takes very little effort from a programmer to not make this particular mistake (always wrapping blocks in braces). There are multiple classes of mistakes in C that are actually worth addressing in newer languages, but this one is just trivial.<annexi-strayline>: Baby steps! Remember that the blog specifically intends to focus on well-known, actual vulnerabilities. The theme is to say - if Ada was the project language, this disaster would probably never have happened. Goto fail is both (in)famous, and easy to understand. It is a great introductory example. It is not, nor does it try to be, the be-all to end-all example.<devraj7>: &gt; if Ada was the project language, this disaster would probably never have happened. This specific disaster, maybe. But maybe other disasters would have happened because of the choice of Ada. You don't seem to understand the limits of your analysis. <annexi-strayline>: Give me one single example where Ada caused a disaster. Or even easier - give me one example where using Ada could cause a disaster due to the choice itself. And don't tell me \"Ariene 5\" - any competent review of that event will tell you Ada was not even remotely to blame.<devraj7>: &gt; Give me one single example where Ada caused a disaster. Brainfuck never caused a disaster. Does that mean that Brainfuck is a good choice of a language? You are shifting the burden of proof, that's not how logic works. Given your posting history, I understand you are heavily invested in Ada. May I suggest maybe you try to diversify your interests a bit? This would go a long way toward making your views on this topic less radical. <annexi-strayline>: &gt; Brainfuck never caused a disaster. Because it isn't used for anything IRL, unlike Ada. &gt; You are shifting the burden of proof, that's not how logic works. You made the statement, you should bring the proof. Your statement was not based in logic, it was based in subjective conjecture. &gt; Given your posting history, I understand you are heavily invested in Ada. May I suggest maybe you try to diversify your interests a bit? This would go a long way toward making your views on this topic less radical. No. I believe in Ada. If that bothers you, I am not sorry. <devraj7>: &gt; No. I believe in Ada. If that bothers you, I am not sorry. There's nothing wrong with beliefs, except when you only have one. There are plenty of modern languages that not only match Ada in safety on a lot of fronts, but also offer additional features that don't exist in Ada. Which is why Ada is largely irrelevant today. <annexi-strayline>: &gt; There are plenty of modern languages that not only match Ada in safety on a lot of fronts, but also offer additional features that don't exist in Ada. Name one that has more features (including all the same), is compiled, and has been used on actual real-world projects successfully. I'd love to know. &gt;Which is why Ada is largely irrelevant today. This statement is predicated on other languages having the same + more features. So I'll wait on your answer to the above. <devraj7>: You seem to think that the more features, the better language? This is an extremely na\u00efve view of language design, and Ada is often cited as an example where more is less because of that. But if you want an example, there is another language that's often mentioned in the category of having too many features: Scala. And it's viewed as negatively as Ada for that reason. C#, Swift, and Kotlin have gained their popularity and success because of the way they successfully turned down features that didn't carry their own weight, a lesson that Ada never learned and which led it to irrelevance. <annexi-strayline>: &gt; You seem to think that the more features, the better language? I thought you were implying that. Sorry if I misunderstood. No I don't think so. But why else use a different language from Ada, which is already highly orthogonal and stable? I can't only think that it is because some other language has some killer feature that Ada doesn't have, while also not requiring one to give-up on all of Ada's features. Ergo, it should have Ada's features and then some. &gt; a lesson that Ada never learned and which led it to irrelevance. You didn't do your homework here. Ada has resisted bloat far more assertively than the languages you mentioned. The ARG spends almost every day discussing the merits and details of proposed changes. So Ada not only \"learned this lesson\", it is the first language to even try.", "num_messages": 15, "avg_score": 13.1333333333}
{"start_date": "1543544538", "end_date": "1543655867", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 236, "text": "<xtreak>: Maybe Not - Rich Hickey <sisyphus>: Upvoted because I already know I will agree with everything Rich Hickey says and marvel at how much smarter and what better hair than me he has and still not use Clojure.<deep_fried_eyeballs>: You dont use Clojure because you're stil smart enough to know that using a type system has advantages when coding in teams of people of different mindsets and competence level.<yen223>: Type systems have *tradeoffs*. It's important to recognize that. There's nothing more annoying than having to contort your program in weird ways just to satisfy the type checker's idea of correctness.<idobai>: Then you're using your typechecker wrong. You're supposed to make strongly typed modules and they should fit together like legos - if the compilation fails, then you'll know something is wrong. A good typesystem can be used to design APIs where it's hard for the user to incorrectly use it. A typesystem can easily and efficiently detect high-level errors - look at what Idris, Nim and Rust can do - but you can get most of the benefits with other statically+strongly typed languages too.<yen223>: In Haskell, you have `Data.List.NonEmpty`, which represents Lists which cannot be empty. Now, Lists which are not empty are obviously a subset of all Lists, which means that any function that works on Lists should also work on NonEmpty, right? Alas, that's not the case according to Haskell's type system. Haskell lacks structural typing, and thus has to treat NonEmpty different type from List. You'd have to add a bunch of useless `toList`s just to satisfy the type checker. Type systems have *tradeoffs*. This is a relatively benign example - there are plenty of cases of Haskell making the wrong call, and Haskell's type checker is one of the better ones out there.<idobai>: &gt; Now, Lists which are not empty are obviously a subset of all Lists, which means that any function that works on Lists should also work on NonEmpty, right? Nope, you just reversed the theory. NEL and List have a large intersection of operators but they're not the same. With a list it's either empty or has at least 1 element - but you can never act like it has an element. While NEL means that you've at least one element - the contract is different. &gt; Haskell lacks structural typing, and thus has to treat NonEmpty different type from List. Scala has structural types and yet scala users don't use it for this \"issue\" because structural typing is a hack. Nim has a structural abstraction too but I've never seen them abused like that. &gt; You'd have to add a bunch of useless toLists just to satisfy the type checker. Why would you do that? Btw, if this really bothers you, you can create an implicit conversion in haskell, scala, nim etc. to convert your NEL to a regular list - and this will always work while it wouldn't work backwards(which is good). &gt; Type systems have tradeoffs. Dynamic typing is a typesystem too and it *really* has a lot of tradeoffs. &gt; This is a relatively benign example - there are plenty of cases of Haskell making the wrong call, and Haskell's type checker is one of the better ones out there. Dependent types can solve that(I mentioned Idris) too. But if you're trying to argue that this problem would be better solved in dynamically typed languages then I need to disagree because you might spare a bit of boilerplate there(if you don't have implicit conversions) but you'd also take a lot more risk at runtime. A bit of boilerplate is fine but runtime errors aren't.<yen223>: &gt; Nope, you just reversed the theory. NEL and List have a large intersection of operators but they're not the same. With a list it's either empty or has at least 1 element - but you can never act like it has an element. While NEL means that you've at least one element - the contract is different. If I have a function f that takes List argument: f [] = ... f (x:xs) = ... Then why should it fail if I'm passing in an argument that's guaranteed to be of the form `(x:xs)`, which are what NonEmpty lists conceptually are? &gt; Dependent types can solve that(I mentioned Idris) too. But if you're trying to argue that this problem would be better solved in dynamically typed languages then I need to disagree because you might spare a bit of boilerplate there(if you don't have implicit conversions) but you'd also take a lot more risk at runtime. A bit of boilerplate is fine but runtime errors aren't. There are plenty of functions which can only be expressed in a way that will result in runtime errors in Haskell (due to its lack of dependent types), but going all in with e.g. Idris will incur a severe cost in terms of compilation time. Tradeoffs! Now I'm not saying that dynamic type systems are strictly better than static type systems, which is why I keep emphasizing the word *tradeoff*. I do like IDE autocompletes and not having runtime NPEs. But I just don't think static type systems are the straight win that a lot of people here seem to think they are.<idobai>: &gt; Then why should it fail if I'm passing in an argument that's guaranteed to be of the form (x:xs), which are what NonEmpty lists conceptually are? `(x:xs)` is not a NEL nominally, but you can use implicit conversions if you think that's how it should work(in a certain scope, at least). Nominal typesystems have the benefit of explicit code: you can decide how things work without the implicit magic. Also, conceptual equality != structural equality. With static+strong typesystems you don't need to *guess* and *hope* that things will work because there's a little proof system in your hand with a lot of extra benefits. &gt; There are plenty of functions which can only be expressed in a way that will result in runtime errors in Haskell (due to its lack of dependent types) That doesn't make any sense. Programming languages have limitations so it's not like you can express anything with a super-language. Dynamic typing won't solve this issue. If anything, it'll make it worse by hiding the issues from your sight. With dynamic typing you've the initial comfort of not caring but later it'll just get harder for those who want to read or refactor your code. &gt; but going all in with e.g. Idris will incur a severe cost in terms of compilation time. Tradeoffs! You pay a little to get a lot of safety - sounds like a good deal. &gt; Now I'm not saying that dynamic type systems are strictly better than static type systems, which is why I keep emphasizing the word tradeoff. I do like IDE autocompletes and not having runtime NPEs. Modern statically and strongly typed languages have far more benefits than that(just from the top of my mind): - the basic ones can help you with refactoring(showing incorrect function calls/nonexistent functions), significantly improve your code's performance, catch typos and give you early feedback when your idea about the data's shape is incorrect(so: static typesystems) - on top of the previous benefits, the more modern ones(like Rust, Nim, Pony etc.) can prevent various errors at compile-times(like correct *and* efficient resource and memory management, the possibility to design safer APIs etc.), don't force you to fall back to inefficient immutability-based concurrency(if you care about safety) and can even infer possible errors( ex. [effects](https://nim-lang.org/docs/manual.html#effect-system-effects-pragma) ) Yes, there are tradeoffs. But you'll need to make those tradeoffs because the truth is that dynamic typing doesn't really solve any serious issue - it just makes it easier for beginners to not care about correct code. Why would you take the risk when you can choose the safe path which also comes with benefits you'll never get from the unsafe one? &gt; But I just don't think static type systems are the straight win that a lot of people here seem to think they are. Then explain why do you think that. The group of \"issues\" you've mentioned is far are easier to deal with than the issues dynamic typing introduces.<jephthai>: I think the discussion would be a lot better if the pro-type-system people would just occasionally agree that the type system can be annoying. You don't have to concede your point that they're valuable, just admit that they're not perfect, and occasionally a dynamic language lets you get somewhere fast, even if it's risky. It's the fundamentalism that's the problem, if you will.<idobai>: &gt; I think the discussion would be a lot better if the pro-type-system people would just occasionally agree that the type system can be annoying. We agree on that. But we think that annoyance is nothing compared to the safety and performance issues introduced by dynamic typing. &gt; just admit that they're not perfect We know that too, nothing new. &gt;, and occasionally a dynamic language lets you get somewhere fast, even if it's risky. Now, I don't agree with that. With a modern statically+strongly typed language I can move *really* fast without worrying about everything - if I screw a few things the compiler will catch it. For the rest I might write a few tests. With dynamic typing you need to sit in the REPL, write more tests and carefully read the docs so that your program will work. With a static language I just write code and then press the compile/run shortcut. While with dynamic typing even exploring the API of a library is a chore because your IDE might not be able to tell when you made a type mistake - the typesystem doesn't matter because your code works on data with a specific \"shape\". &gt; It's the fundamentalism that's the problem, if you will. Computer science is about facts, we don't need to agree - we just need to accept the data or show a better way.<jephthai>: &gt; Computer science is about facts, we don't need to agree - we just need to accept the data or show a better way. It's not about agreement, it's about realizing that others genuinely feel differently. You may feel like you can move really fast in a highly type-constrained system. But maybe I don't. I've spent a lot of time in very typish languages -- I get the value that type safety gets me in something like Haskell. I blew the same trumpet for a long time. But I eventually realized that when I'm playing, doing exploratory programming. Sketching something, tearing it down, trying it another way, etc., I am far more comfortable doing it in a dynamic language. At work, we have a codebase that's split between Python and C#. I hate the grind of rerunning, printing exceptions, and figuring out the edge cases on the Python side. I really like the \"once it compiles at least I didn't make a stupid mistake\" experience on the C# side. It doesn't help as much as more powerful type systems, but I spend less time frustrated. But when I go to play with an idea to build something new, disconnected from that codebase, I am likely to sketch it in Ruby or Lisp because I can run around for awhile without the handcuffs. That unrestricted feeling and ability to setup and tear down quickly is a real thing, and not properly accounted for in these discussions. It's the categorical statements about types being better, without apparent reservation, that make the pundits' arguments into lead balloons. <idobai>: &gt; It's not about agreement, it's about realizing that others genuinely **feel** differently. Yep. &gt; But I eventually realized that when I'm playing, doing exploratory programming. Sketching something, tearing it down, trying it another way, etc., I am far more comfortable doing it in a dynamic language. It sounds like you used static typing as a toy instead of realizing and using its tools. Going back to python/c# from haskell also sounds unbelievable. &gt; That unrestricted feeling and ability to setup and tear down quickly is a real thing, and not properly accounted for in these discussions. Prototyping - you can do it in statically typed languages too, especially if the language has a REPL. &gt; It's the categorical statements about types being better, without apparent reservation, that make the pundits' arguments into lead balloons. You still have types in dynamically typed languages - you just need to keep everything *in your head*. You're literally the typechecker.<jephthai>: &gt; Going back to python/c# from haskell also sounds unbelievable. It made sense to me, whether you choose to believe it or not. I regularly write code in several dozen languages. I'm sort of a language hobbyist. I've been programming for hours a day for 30 years now. I definitely associate what I'm doing with art and creativity, possibly because programming has been formative in my thinking habits, and is involved in most of my work and hobby time. One thing I've definitely learned is that over-commitment to a given paradigm or language construct as a panacea is very common. I chased the blub paradox for years, only to arrive at a place where each language or paradigm has its own place and usefulness. Strangely, I've been spending more time in assembly and forth because of a few research projects I've been working on. It's very limiting to burrow into a single way of doing things, and then to proclaim that others are doing it wrong :-).<idobai>: What's unbelievable is how you can feel comfortable in lesser languages. &gt; I regularly write code in several dozen languages. I'm sort of a language hobbyist. Just like me. &gt; One thing I've definitely learned is that over-commitment to a given paradigm or language construct as a panacea is very common. Yes. &gt; I chased the blub paradox for years, only to arrive at a place where each language or paradigm has its own place and usefulness. You should have arrived at a place where \"paradigms\" don't exist - just languages which can provide better solutions for certain problems. I don't have a favorite language and I'm pro-static typing because 1. I have the evidence and the experience to argue for them and 2. dynamically typed languages don't really innovate because they're too limited(give up on a lot of info) to solve certain issues.<jephthai>: Where did I say they don't exist? I said each has its place and usefulness. <idobai>: I was the one saying that they don't exist.", "num_messages": 17, "avg_score": 13.8823529412}
{"start_date": "1544713597", "end_date": "1544760718", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 10, "text": "<sensored>: How to write useful comments <Deranged40>: I think this is pretty accurate, however I feel like the example of Bad Clarification Comments, while an excellent example of what I would call bad comments, is exactly the type of comments that most professors are going to expect to see on turned in work. Meaning that lots of people are taught to explain each line of code via a comment as if the person reading has never seen a variable declaration before.<draculamilktoast>: First from the text: &gt; Getting clarification comments right is hard, since there is often no objectively right answer. and as you say, &gt; ... as if the person reading has never seen a variable declaration before. In a learning environment one can reasonably expect the reader not to know everything and it might even be appropriate to add such a comment. \"Variable declaration\" might be a pretty scary term if you're new to programming so writing it down might help you remember. As an example: //Assign values to array let arr = [1, 2, 3, 4, 5]; While this may seem like a redundant comment its not obvious to all students that we are \"assigning\" values to an \"array\", so while this comment can be seen as useless from a more experienced programmers point of view, if you're new and especially haven't been paying much attention in class, this comment might have some utility. Then on the other hand in this particular case I might add that we're declaring these values inside the function because it's an example and that you would usually get these values as function parameters, but if we're at a stage where variable declaration is a new thing, it is probably not yet appropriate to be discussing that slightly more advanced concept. But if you were to add that in the comment it wouldn't be a bad thing. So the comments, when it comes to code in education, can be used as a platform for you to repeat obvious stuff just to show off to the teacher that you know what you're doing. In fact the best comment in that particular function may be that you don't need it as you can use [1,2,3,4,5].reduce(function(a, b) { return a + b; }); instead. This example might even in some cases teach the teacher about the joys of functional programming but in the very least it shows that while you're capable of solving the problem, you are also a \"team player\", which may be an even more important thing than your raw ability to solve problems for some companies or organizations.", "num_messages": 3, "avg_score": 3.3333333333}
{"start_date": "1543614881", "end_date": "1543668615", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 22, "text": "<craig081785>: Why the RDBMS is the future of distributed databases <coffeewithalex>: Citus does a lot of cool stuff, but it's rarely updated. For instance, columnar stores are very useful for large denormalized data sets, and they do offer an extension that enables columnar store for Postgresql, but it does not compete with the fastest performing databases out there, like Vertica or ClickHouse. Postgresql 11 has added more parallelization in queries, so you can partition the data as you like, and do parallel scans on partitions, but Citus's columnar store extension is still not marked as parallel safe, which is just 4 lines of code, that would make it's performance at least approach that of ClickHouse.<elcric_krej>: I've recently benchmarked it against redshifts, mariadb CS and clickhouse. Calling citrus CS a column store is...stretching it, it's more of a school experiment performance wise. Partially due to the complete lack of parallelism.", "num_messages": 3, "avg_score": 7.3333333333}
{"start_date": "1543522612", "end_date": "1543623987", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 2, "text": "<stronghup>: How to Compile Node.js Code Using Bytenode? \u2013 Hacker Noon <birdbrainswagtrain>: &gt; This allows you to hide or protect your source code in a better way than obfuscation or other not-very-efficient tricks (like encrypting your code using a secret key, which will be embedded in your app binaries, that\u2019s why I said \u201ctruly\u201d above). Imagine the implications for npm malware! I hate to participate in a circlejerk but I really can't resist with this one.<slykethephoxenix>: Many NPM Modules already use binaries from other languages that are compiled, or downloaded in the npm package (This is one of the reasons you don't commit node_modules).", "num_messages": 3, "avg_score": 0.6666666667}
{"start_date": "1543597008", "end_date": "1543685174", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 103, "text": "<re_anon>: Bug: The latest NodeJS LTS can make permanent changes to Windows Update and UAC <Lt_Tuck_Pendleton>: Not permanent. <Edit>: well, it isn't. No matter how much you downvote me it won't make what I said untrue. Pretty disappointed with this sub now, thought you were better than this. <shevegen>: Shooting yourself into the foot is also not ... well, permanent.<Lt_Tuck_Pendleton>: The scar most certainly would be. That's what this post is claiming, that the change is permanent. It isn't. <MintPaw>: Yes obviously reinstalling an OS will undo all configuration changes so it's not technically \"permanent\". So what's your point? We should never use the word \"permanent\" in software development? <Lt_Tuck_Pendleton>: It's not even necessary to reinstall the OS. You shouldn't use absolute statements when they're absolutely untrue. It's hyperbolic nonsense, which is ironically the same sort of shit programmers complain about end users doing.", "num_messages": 6, "avg_score": 17.1666666667}
{"start_date": "1543591166", "end_date": "1543698138", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1288, "text": "<magenta_placenta>: Company (Google) Tried to Patent My Work After a Job Interview <LunaLoonLooney>: \u0336D\u0336o\u0336n\u0336\u2019\u0336t\u0336 \u0336b\u0336e\u0336 \u0336e\u0336v\u0336i\u0336l\u0336<plonkisok>: When they dropped that, they were telling the world \"We're evil\".<queenkid1>: The reason google removed this isn't because \"they're evil\" but because being \"evil\" is arbitrarily defined. Anyone can argue anything is evil, and thus saying \"don't be evil\" is a statement that could mean ANYTHING from a legal standpoint. You're literally handing anyone who has an issue with your company a \"free pass to win a lawsuit\" as long as you can convince the judge any little tiny this is \"evil\", which goes against the entire point of a objective court of law.<snowe2010>: They didn't drop it. I don't know why everyone thinks that. <https>://www.searchenginejournal.com/google-dont-be-evil/254019/ <https>://abc.xyz/investor/other/google-code-of-conduct.html &gt; And remember\u2026 don\u2019t be evil, and if you see something that you think isn\u2019t right \u2013 speak up! <queenkid1>: Alright, maybe I should be more specific then... ALPHABET no longer says \"don't be evil\", but Google does. Google is just the search engine subsidiary of Alphabet. I *do* remember a very specific thing related to Google Code, where the statement had to be removed. Google was removing certain things from Google Code, due to issues with the legal ambiguity of \"don't be evil\". <snowe2010>: &gt;ALPHABET no longer says \"don't be evil\", but Google does. Google is just the search engine subsidiary of Alphabet. What? First off, Alphabet never ever claimed \"don't be evil\". Second off, Google is not just the search engine subsidiary of Alphabet. I don't know where you heard either of those things. &gt;I do remember a very specific thing related to Google Code, where the statement had to be removed. Google was removing certain things from Google Code, due to issues with the legal ambiguity of \"don't be evil\". Please provide a source for this. <edit>: Also note that Alphabet doesn't claim anything about not being evil, because it's a holding company. They don't do anything except wrap other companies up. You can see this from their very small CoC... https://abc.xyz/investor/other/code-of-conduct/<queenkid1>: &gt;Alphabet never ever claimed \"don't be evil\". They did, before the corporate restructuring, when \"Google's\" CoC became \"Alphabet's\" CoC. &gt;Alphabet doesn't claim anything about not being evil, because it's a holding company. They don't do anything except wrap other companies up. You can see this from their very small CoC... That's literally my point. Google went from being the *whole* company, including subsidiares, to being called \"Alphabet\", where Google was a subsidary of Alphabet. Google mainly handles the search engine, called GOOGLE for god's sake. The CoC that used to cover all of Google, and thus it's subsidiaries, is no longer accurate. Now, they follow the very short CoC that *you* linked to.<snowe2010>: No they didn't. Alphabet is not Google. That's the whole point. &gt;Google went from being the whole company, including subsidiares, to being called \"Alphabet\", where Google was a subsidary of Alphabet. No. Google is Google. It never 'became' Alphabet. <https>://en.wikipedia.org/wiki/Alphabet_Inc. &gt;On August 10, 2015, Google Inc. announced plans to ***create a new public holding company, Alphabet Inc.*** Google CEO Larry Page made this announcement in a blog post on Google's official blog. Alphabet would be created to restructure Google by moving subsidiaries from Google to Alphabet, narrowing Google's scope. The company would consist of Google as well as other businesses including X, CapitalG, and GV. Sundar Pichai, Product Chief, became the new CEO of Google, replacing Larry Page. --- &gt;Google mainly handles the search engine Google was restructured to focus on it's core mission. The internet. From the founding letter: &gt;What is Alphabet? Alphabet is mostly a collection of companies. The largest of which, of course, is Google. This newer Google is a bit slimmed down, with the companies that **are pretty far afield of our main internet products contained in Alphabet instead. What do we mean by far afield? Good examples are our health efforts**: Life Sciences (that works on the glucose-sensing contact lens), and Calico (focused on longevity). Fundamentally, we believe this allows us more management scale, as we can run things independently that aren\u2019t very related. &gt;&gt; https://abc.xyz/ --- &gt; The CoC that used to cover all of Google, and thus it's subsidiaries, is no longer accurate. Now, they follow the very short CoC that you linked to. I think I've thoroughly refuted this in the above sources. <queenkid1>: &gt; Alphabet would be created to restructure Google by moving subsidiaries from Google to Alphabet This should make it pretty clear that companies covered under Google's CoC are no longer covered under it. They're now covered by Alphabet's CoC, which is exactly what I was saying. <snowe2010>: &gt;This should make it pretty clear that companies covered under Google's CoC are no longer covered under it. They're now covered by Alphabet's CoC, No, they're covered by their own CoC. &gt;which is exactly what I was saying. No it isn't.. You said this: &gt;Alright, maybe I should be more specific then... ALPHABET no longer says \"don't be evil\", but Google does. Google is just the search engine subsidiary of Alphabet. Which mentions no other subsidiaries. Speaking of which you still haven't provided a source for this: &gt;I do remember a very specific thing related to Google Code, where the statement had to be removed. Google was removing certain things from Google Code, due to issues with the legal ambiguity of \"don't be evil\". <the_mighty_skeetadon>: You're completely right, but one other point: Google is NOT just a folder for the search engine. It includes almost every Product Area that you have heard of: Chrome, Photos, Maps, Fi, Android, Ads, Search, Brain, Cloud, Hardware (pixel, home, etc), YouTube... And the list goes on and on.<snowe2010>: Yeah I mentioned that two comments up: &gt;&gt;Google mainly handles the search engine &gt;Google was restructured to focus on it's core mission. The internet. &gt;From the founding letter: &gt;&gt;What is Alphabet? Alphabet is mostly a collection of companies. The largest of which, of course, is Google. This newer Google is a bit slimmed down, with the companies that **are pretty far afield of our main internet products contained in Alphabet instead. What do we mean by far afield? Good examples are our health efforts**: Life Sciences (that works on the glucose-sensing contact lens), and Calico (focused on longevity). Fundamentally, we believe this allows us more management scale, as we can run things independently that aren\u2019t very related. &gt;&gt;&gt; https://abc.xyz/", "num_messages": 13, "avg_score": 99.0769230769}
{"start_date": "1543583298", "end_date": "1659474030", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 45, "text": "<jeekl>: uno: a uniq like CLI tool for log data. A tiny tool that can learn normal data and single out anomalies <prawnsalad>: This looks cool, is it open source anywhere? I'd rather not run a mystery binary from some blog somewhere.<psykhi>: Hey, uno author here. It's not open sourced yet, we're working on it and hope to be able to share a github link soon. Totally understand that you don't want to run a random binary from the internet.<snowe2010>: This looks super useful. I can't wait. <psykhi>: It was only a 4 year wait... :) https://github.com/psykhi/uno<snowe2010>: Oh nice!!! I don\u2019t even remember making that comment lol. I\u2019ll definitely try it out!!!", "num_messages": 6, "avg_score": 7.5}
{"start_date": "1544701613", "end_date": "1544802749", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 52, "text": "<emididam>: Razor Components for a JavaScript-Free Frontend in 2019 <deceased_parrot>: &gt; Razor Components treats the browser as a \u201cthin-client\u201d where the web browser handles just the user interface (UI) and the server receives events and responds with UI updates remotely. Are they really suggesting to run parts of the UI on the server?<Zakman-->: Yes, it's almost like you're streaming the entire UI. Having said that though, only the DOM diffs are sent to the server and back so everything would fit under 1KB. The problem is latency. The Blazor devs found that if you're hitting a server on the other side of the world then you'll be looking at half a second response times for UI events but if you're within something like a thousand miles then there's no lag whatsoever. I could see this being really popular with small businesses in the B2B world. Definitely wouldn't recommend it for any public-facing sites.<deceased_parrot>: And then there's GraphQL whose entire point is to minimize the size and number of requests to the API server. \u00af\\_(\u30c4)_/\u00af<Zakman-->: I guess the advantage with Razor Components is that if everything exists on the same server then you can do away with HTTP endpoints... but then is it really an advantage if it's masking how web applications really work?<deceased_parrot>: To be honest, I see no advantage in it what-so-ever. I can't even imagine the though process that went into creating something like Razor Components - I mean, what's the use case? Appeasing developers who hate JavaScript?<Zakman-->: If you've kept up with Blazor then you'll know about the client-side version which runs on WASM and how they've built on top of Razor to create a full UI framework, so the work to create this was pretty much done since they found it was trivial to run the UI on the server (which is what this is - Razor Components) instead of the browser. The massive benefit to this is they can continue to build on the UI framework while not having to wait for WASM to mature or for Mono-to-WASM compilation to mature. Once the two have matured then they can use the work they've done to the UI framework via Razor Components and apply that to the client-side version since Razor Components will be built to work exactly the same as Blazor. It all makes a lot more sense if you've been following the project from the beginning and understand which parts are independent. If you were to create an application using Razor Components as if it were running in the browser then all you need to do to have it actually run on the browser is change a couple of lines of code. There are benefits to running all this on the server. You get full .NET Core functionality (including full debugging), initial page load is extremely quick since you're not shipping Mono to the browser and there are a couple of other benefits as well. /u/eldamir88 and /u/chucker23n have explained the benefits in full.<deceased_parrot>: I don't doubt there are benefits. I just don't see how any of these benefits overcome the latency of running back and forth between the server and the browser.<Zakman-->: If you're physically not far from the server then it feels like a SPA. [Listen to this.](https://youtu.be/CWuIz9khK-o?t=3710) They explain the pros and cons of the server-side model, including latency.<deceased_parrot>: TLDR: Multiple datacenters around the world, clients with guaranteed good connectivity (no trains, shitty data plan connections, etc) ?<Zakman-->: Aye, which is why I said it's a perfectly viable model for small businesses doing B2B. Or even internal applications. It's not a model right for public-facing sites.<deceased_parrot>: So basically a solution applicable to a pretty limited niche for which there are already a bunch of viable and mature solutions?<Zakman-->: Wouldn't say niche mate considering how many small businesses there are and how many of them create simple CRUD applications. The difference between this and other solutions is that changing 3-5 lines of code would turn Razor Components into Blazor (from server-side to client-side).<deceased_parrot>: &gt; Wouldn't say niche mate considering how many small businesses there are and how many of them create simple CRUD applications. When I said \"niche\" I meant a niche for which this particular technology was superior to the already existing ones. And then asked for an explanation as to _how_ this is superior to the already existing solutions. I don't mean that internal or B2B are niche, I meant the niche within that segment where Razor/Blazor is superior to already established technologies. &gt; The difference between this and other solutions is that changing 3-5 lines of code would turn Razor Components into Blazor (from server-side to client-side). And this solves which problem exactly?<Zakman-->: &gt;When I said \"niche\" I meant a niche for which this particular technology was superior to the already existing ones. And then asked for an explanation as to how this is superior to the already existing solutions. &gt;I don't mean that internal or B2B are niche, I meant the niche within that segment where Razor/Blazor is superior to already established technologies. There's no existing solution for this for those that want to work with .NET and the other established technologies that I've seen don't have as good as a component model as Razor Components. But again, this is mainly for existing .NET developers. &gt;And this solves which problem exactly? Feels like you're taking the piss now... it solves the issues that come with the server-side model. The same issues that you brought up. Just say a customer suddenly wants offline support for a certain application then the client-side model would do the trick.<deceased_parrot>: &gt; There's no existing solution for this for those that want to work with .NET Seriously? I would expect at least .NET, the poster child of enterprise intranet applications would have solved that problem already multiple times over. &gt; Feels like you're taking the piss now... it solves the issues that come with the server-side model. Yes, but that's my whole point - it's solving a problem _it_ created! This is not a problem the user has, this is not even a problem other software has, this \"problem\" is specific to Razor/Blazor. Somebody please correct me here.<Zakman-->: &gt;Seriously? I would expect at least .NET, the poster child of enterprise intranet applications would have solved that problem already multiple times over. Web Forms would be the closest model but Razor Components is far superior due to Razor syntax, the component model, no ViewState and how only the DOM diffs are sent back and forth. There's no support for Web Forms on .NET Core as well so this would be a superior replacement. &gt;Yes, but that's my whole point - it's solving a problem it created! This is not a problem the user has, this is not even a problem other software has, this \"problem\" is specific to Razor/Blazor. Read this entire comment tree from start to finish.", "num_messages": 17, "avg_score": 3.0588235294}
{"start_date": "1544728311", "end_date": "1544819215", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 12, "text": "<nicolaspeixoto>: How we built Globoplay\u2019s API Gateway using GraphQL <vitorgrs>: My dream would be Globosat Play, Telecine Play and Globo Play unified (and all on Globo Play end). <andreeberhardt>: https://m.oglobo.globo.com/economia/globoplay-tera-pacote-com-telecine-premiere-23281411<vitorgrs>: Yeah, but I mean, unified on a single interface :D", "num_messages": 4, "avg_score": 3.0}
{"start_date": "1543611440", "end_date": "1543743105", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 671, "text": "<shadowh511>: I Put Words on this Webpage so You Have to Listen to Me Now <BubuX>: That's basically how a colleague got roasted for suggesting to our CTO that we should rewrite all microservices in rust.<BoxMonster44>: I'm a huge Rust evangelist... and that's the dumbest idea I've ever heard lol<defenastrator>: As a someone with a small interest in rust that doesn't sound like what rust is for. <steveklabnik1>: It\u2019s a real production use case, though. Every README on npm\u2019s website is rendered via a service written in Rust, dedicated to that. They made that call due to performance, stability, and low memory usage.<shadowh511>: I don't mean to be rude, but this is exactly what my blogpost is satirizing.<steveklabnik1>: What is? I don\u2019t see the connection. I did love the post, by the way.<shadowh511>: My post is meta-linguistic satire about people choosing things over other things for the wrong reasons. Performance is nice, but what about long-term maintainability? Etc. Don't get caught up in the exact details because ultimately they don't matter. This is why I used the words `flopnax`, `ropnar`, and `rilkef` (This might break part of the satire, but they were really just \"iterate over the list\" and \"async await\" when I wrote this out initially at the lowest level of abstraction). They don't matter. They're just things that you can do with a tool. You can do the same basic things with other tools with different properties. The fact that they have different properties means that they have different behavior, and this is fine. Sometimes you really just need to buckle down and flopnax the ropnar and get on with life, even though the experimental rilkef is five times faster...(in a single micro-benchmark based on a synthetic load while the CPU is not in a state representative of what the real world actually is). The overall patterns of how to design the design philosophy of things is more important in my opinion. Nobody cares about a service that renders responses in microseconds when nobody is able to reliably understand it. Introduction of new tools or methods of solving problems should be done carefully, not via installing a package from the package manager and using it to replace the entire method of current operation just because of some words someone said on the internet via a webpage. &gt; They made that call due to performance, stability and low memory usage This tells me about as much as the graph I made in that post does. Performance compared to what? Stability compared to what? Low memory usage compared to what? What kernel? What architecture? What micro-architecture? What manufacturer of dram? What phase of the moon? What was the relative alignment of the planets? [What was the poison arrow that hit you made out of?](https://en.wikipedia.org/wiki/Parable_of_the_Poisoned_Arrow). More importantly, how does this help you to live your life as a better person? It's great that the NPM guys have found a tool they can use. I'm not saying it's a bad thing that they have this tool. Please just stop selling methods of encoding grocery lists to five year olds as the nectar of the gods. If anything, it's just a different encoding format for the aforementioned grocery lists that (metaphorically) only five people (including the metaphorical you) know.<shadowh511>: [https://christine.website/blog/that-which-is-for-kings-12-02-2018](https://christine.website/blog/that-which-is-for-kings-12-02-2018) I put this in more words in more context here.", "num_messages": 9, "avg_score": 74.5555555556}
{"start_date": "1543615116", "end_date": "1543616165", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 10, "text": "[deleted]: My friend just found a bug in google images, can anyone confirm? [deleted]<nilamo>: &gt; Just because it has a computer in it doesn't make it programming. If there is no code in your link, it probably doesn't belong here. /r/programming isn't the internet's bug tracker.<diguifi>: I imagined that I would get this kind of comment, just though it might interest to anyone on this sub, since it's not a common thing to find bugs on Google (specially when it's so easy for anyone to reproduce). Apologies for bothering.", "num_messages": 3, "avg_score": 3.3333333333}
{"start_date": "1544747433", "end_date": "1544801952", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 153, "text": "<dikiaap>: Bootstrap 3.4.0 released <nikoschalk>: Isn't this old news? There is already Bootstrap 4.1.3 out there <l_o_l_o_l>: You might want to get the article<fuckin_ziggurats>: I think he didn't understand that a major version release doesn't equal stopping maintenance on older, still widely used versions.<MMPride>: To be perfectly fair, they have pretty much said that they stopped supporting 3: https://github.com/twbs/bootstrap/issues/22343 For example: \"We stop supporting old versions as soon as the new one comes out honestly.\"<fuckin_ziggurats>: Nice find. Seems like they didn't intend to pay attention to older versions but they realized v3 is going to continue to be a lot more used than v4 for a some time to come.<kevindqc>: Hmm that's not how I read it, here's my interpretation: They don't support minor versions (ie: 3.3.5) when newer minor versions come out (ie: 3.3.7). Makes sense - it should be fully backward compatible, so why make a [3.3.5.1](https://3.3.5.1) or something like that. They say they aren't actively developing bootstrap 3 because of bootstrap 4 - to me that means no new features, mostly just bug/security fixes if needed. Which is what they did for 3.4.0 (they also improved their tooling and documentation). The blog even says: &gt;While we\u2019d planned for ages to do a fresh v3.x update So I don't think they ever had in mind to completely disregard v3 after v4?<fuckin_ziggurats>: They're very vague about it. In the GitHub thread the creator said: &gt;We stop supporting old versions as soon as the new one comes out honestly. So 3.3.5 is done because 3.3.7 is out. I'm taking the word \"support\" as maintenance and bug-fixing. He didn't really specifically say that it's about patches and not minor releases. And this part: &gt;We're not actively developing v3.3.7 any further though because we're focused on v4. Is even more confusing as it implies that once a major release is out all previous major versions are no longer developed. Personally, I don't think they have an official stance on any of these things. They're probably being pragmatic about it and change their policy based on user feedback.", "num_messages": 8, "avg_score": 19.125}
{"start_date": "1544715987", "end_date": "1544793760", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 82, "text": "<chrismccord>: Phoenix.LiveView: Interactive, Real-Time Apps. No Need to Write JavaScript. <audioen>: The innovation since PHP-based web apps of the 2000s has been to create the notion of persistent page state, which is updated by events. When Ajax was invented, the page state lived on the web client, and I think that was a revelation to all those people who tried to create component frameworks server side, but got bogged down by having to carry so much state in form hiddens from request to request because every little detail had to be recreated. With Ajax, you no longer had to do that. The innovation given by React seems to be to use some diffing mechanism to rapidly discover the parts of page that are different, and focus any expensive operations only to the changed parts. While, say, Wicket, had the concept of server-side state in the 2000s, the notion of directly diffing the page state on server side was beyond the framework, and so the programmer had to tag the components that changed due to an event, and wicket would render those and send back the entire HTML for them. I really appreciate the straightforward nature of the framework. While I personally am not excited about switching back to server-side HTML rendering, and probably won't ever go back to doing things that way, at least these days it's not that bad compared to doing things client side. The higher latency of anything server side pushes the desire to do more work on the client side, but on the other hand, the client side needs to keep the download size in control, which is challenging as apps grow ever more features.<0xB7BA>: Just out of curiosity; How do you handle SEO when doing client-sided rendering? And also the latency thing; Fetching data and rendering a somewhat basic article takes 20-30ms, add a 5-10ms network latency do that and your looking a 25-40ms for the markup do be rendered. Looking at React; it still takes 500ms to run the scripts the first time.<audioen>: Search engines, in order to be useful, basically must be able to deal with SPA style applications, because programmers keep writing them. The benefits are still pretty huge, and sometimes SEO is not that important. It is definitely an evolving aspect, but as far as I am aware, e.g. Google Search is based on some forked version of Chrome that renders the page for analysis, so even SPAs should be searchable with e.g. GS as long as they are written well enough. The initial render is relatively terrible in JavaScript. I can only hope that people have the patience to wait that 1 second or something like that before they give up and go away. Some tricks help, like marking all the web fonts as being swappable so that they don't block initial content rendering, and maybe you'd want to http/2 push to send all data before client manages to ask for it, and so on. On the other hand, once the initial download and render gets done, many page views can potentially occur without any further network traffic, and those that do need more data can be quite compact in terms of response size, typically much smaller than any comparable HTML would be. Still, given the use of gzip compression, I think most of the time the difference between JSON and HTML is not that great on the wire. Both formats would end up having highly regular structure which should compress very well. Of course, the technology in the article is based on transferring HTML initially, running a piece of JavaScript to upgrade connection to WebSockets, and then using JSON on the wire (and quite well optimized JSON at that). As an example for why I think SPAs have their place: I recently wrote a simple SVG-based editor for drawing floor plans of client sites in an application. You can create walls (curved and straight lines), drag object library entries like tables, and move any anchor point of an object, and resize and scale them). The page makes no network traffic until you press save, and at that time, a JSON structure of objects, points, and relationships between objects and points get transmitted. I would kind of hate doing this with server being pinged a hundred times every time you do some drag and drop to move, rotate or scale the object.", "num_messages": 4, "avg_score": 20.5}
{"start_date": "1544762432", "end_date": "1544891793", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 19, "text": "<Mynameis__--__>: We Need an FDA For Algorithms <no_condoments>: 1) this seems like a call for more government to solve a problem that doesn't exist yet. It's mostly a scare tactic based on hypotheticals and grandiose futuristic claims 2) Hannah Fry is a brilliant mathematician and is remarkably attractive. [deleted]: &gt;In terms of wide-reaching impact, the stuff that\u2019s happened with Facebook\u2019s Newsfeed is really, really concerning. Fifteen years ago, let\u2019s say, all of us were watching all the same TV programs, were reading the same newspapers. The places we would get our news, and especially our politics, tended to be universal. And what that meant was that when you had a national conversation about an issue, everyone was coming to that conversation with the same information. But as soon as Facebook decided that they wanted to become purveyors of news, suddenly you have these highly personalized newsfeeds where everything is based on what your friends like, what you like, things that you\u2019ve read in the past. And that\u2019s become so infinitesimally cut up into tiny little chunks, that suddenly when you try and have a national conversation, people are missing each other. They\u2019re talking about different things, even though they think they\u2019re talking about the same thing. Even before all of this Cambridge Analytica stuff, which is a whole other level, I think there is a really serious implication on democracy and on politics. But it\u2019s something that can happen without anybody ever being malicious or having ill intent. It\u2019s just a totally unintended consequence of barging in somewhere without thinking through what the long-term implications of being in that space was. It's not about hypothetical and grandiose futuristic claims. It's about what's happening right now as a result of today's algorithms. \\------ Here's the justification &gt;**Why do we need an FDA for algorithms?** &gt; &gt;It used to be the case that you could just put any old colored liquid in a glass bottle and sell it as medicine and make an absolute fortune. And then not worry about whether or not it\u2019s poisonous. We stopped that from happening because, well, for starters it\u2019s kind of morally repugnant. But also, it harms people. We\u2019re in that position right now with data and algorithms. You can harvest any data that you want, on anybody. You can infer any data that you like, and you can use it to manipulate them in any way that you choose. And you can roll out an algorithm that genuinely makes massive differences to people\u2019s lives, both good and bad, without any checks and balances. To me that seems completely bonkers. So I think we need something like the FDA for algorithms. A regulatory body that can protect the intellectual property of algorithms, but at the same time ensure that the benefits to society outweigh the harms. &gt; &gt;**Why is the regulation of medicine an appropriate comparison?** &gt; &gt;If you swallow a bottle of colored liquid and then you keel over the next day, then you know for sure it was poisonous. But there are much more subtle things in pharmaceuticals that require expert analysis to be able to weigh up the benefits and the harms. To study the chemical profile of these drugs that are being sold and make sure that they actually are doing what they say they\u2019re doing. With algorithms it\u2019s the same thing. You can\u2019t expect the average person in the street to study Bayesian inference or be totally well read in random forests, and have the kind of computing prowess to look up a code and analyze whether it\u2019s doing something fairly. That\u2019s not realistic. Simultaneously, you can\u2019t have some code of conduct that every data science person signs up to, and agrees that they won\u2019t tread over some lines. It has to be a government, really, that does this. It has to be government that analyzes this stuff on our behalf and makes sure that it is doing what it says it does, and in a way that doesn\u2019t end up harming people. &amp;#x200B;<no_condoments>: Do you think Facebooks response to it has been bad? Certainly they've responded to the claims above. More generally, how much control over sources of news do you think the government should control? Controlling Facebook news: good? Trump kicking Jim Acosta out of the press room: bad? I generally dont like the idea of government controlling the press or news distribution in anyway. [deleted]: Do you think private corporations should be able to manipulate people in order to suit their own private interests without any oversight? <no_condoments>: Yes. That's basically guaranteed by the first amendment. What about yourself? Do you think the Trump administration should be given more oversight and control of CNN, MSNBC and the Huffington Post? [deleted]: &gt;Yes. That's basically guaranteed by the first amendment. What about yourself? I don't believe corporations are people and thus the first amendment shouldn't apply to them. I also wouldn't consider a product to be speech. &gt;Do you think the Trump administration should be given more oversight and control of CNN, MSNBC and the Huffington Post? I think the FCC should reinstate it's fairness doctrine that \"required the holders of broadcast licenses both to present controversial issues of public importance and to do so in a manner that was honest, equitable, and balanced\". The FCC eliminated the policy in 1987 and broadcast news has certainly become much worse since then.<no_condoments>: &gt;I don't believe corporations are people and thus the first amendment shouldn't apply to them. The first amendment isnt just about speech of individuals. It's also about freedom of press, which was even done by corporations at the writing of the constitution. Requiring a balanced approach could easily backfire. If someone runs a piece on climate change, are they required to give time to climate change deniers? If so, /r/enlightenedcentrism would like to have a word. [deleted]: &gt;The first amendment isnt just about speech of individuals. It's also about freedom of press, which was even done by corporations at the writing of the constitution. There's a difference between saying a group of the press that is organized as a corporation should have free speech because they're part of the press, and saying that because some of the press is organized as corporations, therefore, all corporations should have free speech. I can understand the former but not the latter. &gt;Requiring a balanced approach could easily backfire. If someone runs a piece on climate change, are they required to give time to climate change deniers? If so, [/r/enlightenedcentrism](https://www.reddit.com/r/enlightenedcentrism) would like to have a word. No solution to a real-world problem is perfect, with no downsides. I would rather allow those who are clearly wrong to have a voice in a debate than to allow for unaccountable private corporations, with their own private interests, dictate what is said about a particular topic.<no_condoments>: &gt; There's a difference between saying a group of the press that is organized as a corporation should have free speech because they're part of the press, and saying that because some of the press is organized as corporations, therefore, all corporations should have free speech. I never said all corporations should have unlimited speech or anything of the sort. With regards to the press, Facebook is the single largest distributor of news in the US. Here's a source showing more than 50% of people get news from Facebook. <https>://pauldughi.com/2017/02/14/even-people-that-get-their-news-from-facebook-dont-believe-it/ [deleted]: &gt; I never said all corporations should have unlimited speech or anything of the sort. Could you clarify what your argument is then? We're in a thread talking about a policy that would apply to all corporation and your argument against it seems to be that: 1. This policy affects free speech 2. The press shouldn't have their free speech restrained in any way 3. Facebook should be considered part of the press 4. Therefore, this policy shouldn't be applied to Facebook 5. Facebook is a corporation 6. Therefore, this policy shouldn't be applied to any corporation As I said, I can understand 1-4, but 5-6 doesn't make sense. <no_condoments>: My argument is two fold. With regards to Facebook specifically, parts 1 through 4 accurately summarize it. It's weird to think of Facebook as part of the press, but as I said, more people get news there than from newspapers and cable TV. As for other corporations, I'm OK with limiting their speech in the future if needed. As of right now though, I think an FDA for algorithms is premature and an expensive solution to a problem that doesn't exist. You then mentioned Facebook (as does the article), but I don't think that counts as a non-press corporation that currently is a problem.", "num_messages": 12, "avg_score": 1.5833333333}
{"start_date": "1543617450", "end_date": "1543669137", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 3, "text": "<fagnerbrack>: What I learned about cryptography in 3 weeks <BigFatMonads>: This is a disappointingly little amount of content from 3 weeks of learning<that_which_is_lain>: I believe that\u2019s the point.", "num_messages": 3, "avg_score": 1.0}
{"start_date": "1543617243", "end_date": "1543626142", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 4, "text": "<Zzzxxxccc123>: is there a windows program/website like this that can ping flight prices in real time and send email/txt notification? <frunze9219>: Why not to use Google flights tracker, it can send you email if price changes<Zzzxxxccc123>: only send once a day. need something that checks every 15-30 mins and send notification<Annh1234>: You either get this data from mainly cached sources like the Skyscanner API ( with limitations), illegally scrape some OTAs like Expedia ( and probably get blocked), or get your data from a GDS ( very expensive ). Inventory changes in real time, and beside a monthly fee, the GDS usually charges per scan/search. ( Google fights used to have an API, at 3 cents per scan, but they closed) So whatever site you find, will eventually block you if you don't actually book/buy something. For a monthly fee, I can provide you the data ( have a system that does this, mainly used by b2b travel agencies), but for free, it's unlikely you will find something useful for more than a flight or two.", "num_messages": 4, "avg_score": 1.0}
{"start_date": "1544581478", "end_date": "1544790204", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 2, "text": "<CurtLiom>: I dunno how many of you are memers but I coded this Discord bot... <dataf3l>: what tutorials did you use for this?<CurtLiom>: I coded my bot in Python. To get started I used this [website](https://www.devdungeon.com/content/make-discord-bot-python). I also used the Discord.py documentation which can be found at [this site](https://discordpy.readthedocs.io/en/latest/api.html).", "num_messages": 3, "avg_score": 0.6666666667}
{"start_date": "1543265054", "end_date": "1543484203", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 73, "text": "<akalenuk>: Fortran is still a thing <m1el>: I've read megabytes of FORTRAN code and the article is just full of shit. &gt; However, the modern Fortran already has modules, objects, generics and built-in support for parallel computing. Foot shooting area grows steadily to meet the fiats of the modern world. This is so painful to read. FORTRAN still doesn't have adequate abstractions. Nearly every modern language allows you to build on top of everything available to you and wrap it in a nice way. Be it C#, Rust, Java... hell, even JavaScript. But FORTRAN doesn't have *anything* that makes abstractions safe, ergonomic and reusable. I'd rather use C than FORTRAN because how shitty that is. &gt; It stll excels in the good old structured programming. It has features that mainstream C-like languages lack. For instance, it can exit or continue a loop from a nested loop. OH FOR FUCK SAKE. C#, Java, Rust, JavaScript have labeled loops. And even then, I only had to use them *twice* in my entire programming career. Also, this piece of shit code shows how *terrible* the abstractions are. You're using zero values as terminators in matrix, instead of having matrix size. Fuck. <rows>: do i = 1, 10 <columns>: do j = 1, 10 if (a(i, j) = 0.) exit rows ... enddo columns enddo rows &gt; So Fortran is not complex and it\u2019s not outdated. Now how come it went underground in the modern world? If you think that labeled loops (which FORTRAN had for &gt;40 years), structs (*please*), ranged switch-case and modules make FORTRAN \"modern\", then you're deluded.<The3bodyproblem>: Im kind of new to programming, and I like structs. Is there something wrong with them or are they just old?<btingle>: No, they\u2019re just old. Most people use classes instead of structs now, but if all you want is a custom data container structs are fine.<tcbrindle>: &gt; Most people use classes instead of structs now Please, do educate me on the difference between a class and a struct.<btingle>: I actually have no idea. Is there a difference? I think Structs support constructors and methods and such, so I'm not really sure what distinguishes it from a class.<Astrokiwi>: In C++, the only difference is that a struct has all contents public by default, while an object only has public content if specifically labelled as such. Generally, the convention is to use structs to contain data that a lot of other things will work on, while you use objects for more complex objects with more complex behaviour where you don't want everybody to see the innards. But you can declare everything public in a class and it'll behave like a struct if you really want, and you can put methods and private variables in a struct if you like as well.", "num_messages": 7, "avg_score": 10.4285714286}
{"start_date": "1544713621", "end_date": "1544763781", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 16, "text": "<yesnoornext>: CS interviews and how they can become un-broken <gauauuau>: &gt;And maybe the problem of pretending to be a coder is less pervasive in 2018 than it was 10 years ago, so this whole current design of coding interviews is outdated? &amp;#x200B; Maybe he has a better system for weeding out resumes, or a better candidate pool, but in my experience, you DO get loads of people that just have no ability to code whatsoever. I'm not sure what the best method of evaluating them is (I understand the complaints about all the various interview techniques, and I have my own theories about what works well and doesn't, but that's a tangent I won't get into here). My point is that just gauging soft skills isn't sufficient. You still need some way to weed out the people who are not technically able to do the job.<BadLuckLottery>: &gt;You still need some way to weed out the people who are not technically able to do the job. Yup, if there's money to be made, fakers will exist. You'll get candidates with BS + MS/PhD or 5+ years in industry who still can't write a simple recursive function or even sometimes a working for loop. I'm sure their soft skills are great (because how else did they get this far) but you also need to be able to write at least *basic* code on occasion without resorting to copy/pasting from Stack Overflow.<WonderfulNinja>: Fakes with great soft skills are very dangerous fools. They believe they know something and they will convince top management they know, but they only can bring down a project and burn it into ashes. <rememberthesunwell>: you just triggered my imposter syndrome", "num_messages": 5, "avg_score": 3.2}
{"start_date": "1543605813", "end_date": "1543621168", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 29, "text": "<amaxen>: Baker's Law: You\u2019ll never know how evil a technology can be until the engineers deploying it fear for their jobs <bigmell>: Everyone fearing for their job? This was the point of unions right? There are no more unions so you can be fired on the spot for any of your superiors whims. Your only recourse is very costly litigation. Only you probably cant afford this because you no longer have a job. Also the company will say \"oh thats illegal? Well then we fired you for a different reason that is more ambiguously illegal.\" <ex_nihilo>: Reason is a right wing rag.", "num_messages": 3, "avg_score": 9.6666666667}
{"start_date": "1543415743", "end_date": "1543609750", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 895, "text": "<freebit>: Exploiting JavaScript developer infrastructure is insanely easy <GYN-k4H-Q3z-75B>: This isn't just a problem of the ecosystem, it is a problem of the community and how they develop. npm packages are free, as in free toilet. While there are the occasional gems, on average it's just a random shithole. And the developers don't care or even understand what they include and how it is updated. node_modules isn't a meme just for the hell of it. It's funny because it is true. As a engineer in various midsized enterprise projects, the decision to include a library in a specific version is a serious discussion with paper trails. Who maintains the library, what does it cost, what are the alternatives, why do we need it in the first place. There are committees who deal with such questions. Of course this is also possible because we don't need a ton of tiny libs to patch up holes in the ecosystem left and right all the time. But also because there are architects who are actually concerned and responsible in their role. We dabble in extensive JS apps as well, work with node and npm. The potential is limitless it seems. But it is easy to get burned if you're not careful.<zappini>: aka Tyranny of Structurelessness [https://www.jofreeman.com/joreen/tyranny.htm](https://www.jofreeman.com/joreen/tyranny.htm) Curation is useful. Anyone chafing at Apple's velvet handcuffs is willfully, negligently ignoring that Freedom Markets\u2122 (npm, Google Play) inevitably leads to lawless cesspools. Functional, efficient markets require trust. Meaning protections, options for recourse. Duh.<swordglowsblue>: &gt; Apple's velvet handcuffs Apple's handcuffs may look velvety, but they're about as sandpaper as it gets. $100 USD is a ridiculous price just to publish an app, and that's just the developer side of things, not even getting into the ridiculous \"white lies\" and marketing speak their entire consumer-side business is built on.<qomu>: &gt; that's just the developer side of things, not even getting int Plenty of people seem perfectly willing to pay $100 to publish an app, so how is it ridiculous? If there wasn't a fee there would be way, way more garbage to filter through.<swordglowsblue>: A fee is one thing. A fee as expensive as some entire devices (admittedly low-end ones, especially with recent price bloat) is another. Yes, it raises the bar for quality somewhat, but it also hamstrings access to that ecosystem for smaller developers or hobbyists who just want to make something cool. I'd rather have more junk to wade through than a relative lack of anything at all.<ulyssesphilemon>: If any app developer can't spring for $100 publishing fee, the community is better off without their crap.<filleduchaos>: I agree even as someone from a very, *very* low income area. If you can't pony up $100 to have your app listed you really cannot afford the time and effort to build any half-decent app, much less maintain it for months or years on end with possibly no revenue.<NoMoreNicksLeft>: What kind of stupid logic is this? Only the well-off have free time or can expend effort? The cleverest programmer I know plays bass doing bar gigs to pay his rent. Of course, he wouldn't be caught dead writing swift code, but that's not the point. Do you walk around thinking that some people are better than others, and that the test to determine whether they are or aren't is a glance at their bank statement?<s73v3r>: &gt;Only the well-off Having $100 is not \"well-off\" &gt;The cleverest programmer I know plays bass doing bar gigs to pay his rent. And if they can't find $100, or find development work that would get them at least $100, then they're clearly not that clever. <NoMoreNicksLeft>: &gt; Having $100 is not \"well-off\" Being able to spare $100 on an artificial obstacle to being able to write software isn't the same as \"having $100\". <s73v3r>: Any half-competent developer will have no issue with that \"obstacle\". <NoMoreNicksLeft>: That's circular. Certainly anyone interviewing for $250,000 jobs at Google will have no problem with it. That doesn't make them competent, just well-off. It's not a meritocracy.<s73v3r>: &gt;That's circular. No, it's not. &gt;Certainly anyone interviewing for $250,000 jobs at Google will have no problem with it. Those are not the only software engineering jobs out there, and just about all of them will pay enough for the Apple fee to not be an issue. &gt;It's not a meritocracy. I'm not saying it is. I'm saying that if you're even half decent at software engineering, you'll either have or be able to get a job that will easily allow you to afford that.", "num_messages": 14, "avg_score": 63.9285714286}
{"start_date": "1543512117", "end_date": "1543526181", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 179, "text": "<Senekay>: Go 2, here we come! <ibroheem>: [Go 2 considered harmful.](https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf) That being said, &gt;A major difference between Go 1 and Go 2 is **who is going to influence the design and how decisions are made. Go 1 was a small team effort with modest outside influence**; Go 2 will be much more community-driven. After almost 10 years of exposure, we have learned a lot about the language and libraries that we didn\u2019t know in the beginning, and that was only possible through feedback from the Go community. The tyranny had to end. &amp;#x200B;<jyper>: They really should skip to Go3 to avoid that joke<driusan>: Why would you want to avoid that joke?<jyper>: I imagine after hearing it for the millionth time it will get old<driusan>: Gotta replace the generics complaints with something.", "num_messages": 6, "avg_score": 29.8333333333}
{"start_date": "1544716004", "end_date": "1544739856", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 78, "text": "<ahsansaeed067>: 10 new features in Java 11 <zeroone>: That looks rather lousy.<duhace>: The new java releases are done every 6 months, so they don't have a lot of features each release. Here's the java 12 features coming out early next year: https://openjdk.java.net/projects/jdk/12/ Lots of nice garbage collector improvements, plus the addition of the shenandoah gc as an experimental GC alongside ZGC which came out with 11. The reason for this change in release cadence is to help keep the jvm releases on time instead of letting them be held up potentially for years like happened with java 7, 8, and 9. Instead, features are released when they are ready, and so we get stuff like switch expressions early instead of waiting 3 years for project valhalla to be finished in order to get them.<Matrix_V>: &gt; Shenandoah gc as an experimental GC alongside ZGC Would you provide further reading for Shenandoah vs ZGC vs G1?<duhace>: There isn't a ton of comparisons between shendandoah and ZGC yet. I would like to make one, but Shenandoah isn't included by default in any jdk except fedora's build of openjdk, and zgc just became available for use in openjdk 11. However, the ZGC creators have made comparisons between it and G1. [This video](https://youtu.be/kF_r3GE3zOo?t=903) goes into detail about ZGC, and the time I linked you at shows some performance comparisons between it and G1 and parallel GC. Right now, in synthetic benchmarks at least, ZGC seems to have significant throughput advantages over both G1 and parallel GC, and I think that's because of how its garbage collection is run mostly concurrently with the program that's generating garbage. I think Amdahl's law gives it some wiggle room to improve performance despite the on-paper perf losses (up to 15% throughput reduction) incurs. I'm pretty sure that G1 can improve beyond what it is right now and that it can be a better GC that balances throughput and latency, but it still has issues. I'm hoping some of the improvements in java 12 make it better (and java 11 already had some improvements to it)<funkinaround>: &gt;Right now, in synthetic benchmarks at least, ZGC seems to have significant throughput advantages over both G1 and parallel GC It seems to me that this is not the case. The idea with low pause collectors is to sacrifice a bit of throughput to make sure pause times are small and predictable. In the video you link to, the regular max jOps is tied between the collectors. The difference that shows ZGC performing better is when latency is considered. That said, I will be a user of ZGC.<duhace>: &gt; It seems to me that this is not the case. The idea with low pause collectors is to sacrifice a bit of throughput to make sure pause times are small and predictable. Yes, this is the sacrifice made on paper. It appears that in their benchmarks, this sacrifice is not an issue, and like you said, they are tied on max jops while zgc is the clear winner whenever latency is a consideration", "num_messages": 7, "avg_score": 11.1428571429}
{"start_date": "1537995690", "end_date": "1543662587", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 2763, "text": "<mareek>: How Microsoft rewrote its C# compiler in C# and made it open source <Jeflol>: I know I\u2019m late to this but how do you write a compiler in the same language that it is compiling? Wouldn\u2019t you need a compiler to compile the compiler? Rust is similar in the fact that the compiler is written in Rust but how would that even work? I don\u2019t know much about compilers so don\u2019t hate too much. <Voltasalt>: You use an older version of the compiler, or a different compiler or even interpreter. Then you can compile the compiler with itself. <https>://en.wikipedia.org/wiki/Bootstrapping_(compilers)<fourdebt>: But why would they do that... wouldn't that be significantly slower?<coder543>: significantly slower than what? also, if you're writing a language compiler, it's usually because you think that language is great, so why wouldn't you want to write the compiler in that great language? bootstrapped / self hosted compilers are a traditional sign of language maturity. If your language isn't powerful enough for the compiler to be written in that language, is it really ready for the industry to use it to develop even more complicated applications?<yawkat>: The only problem with bootstrapped compilers is that it becomes infeasible to compile the language completely from source. This is often fine but can be a problem if you want end-to-end source verification.<coder543>: it's not uncommon for languages to have alternative implementations of the standard, which can help alleviate this issue. mrustc is a Rust compiler written in C++ that has successfully compiled the Rust compiler written in Rust.<yawkat>: Yea, but this isn't always reliable - for example, now that gcj is dead, there is no real java bootstrap.<sindisil>: What about OpenJ9 and Excelsior Jet? And graal will be another option in the near future.<yawkat>: OpenJ9 and JET are not compilers. I believe graal also operates on bytecode.<sindisil>: Not quite sure how you could be more wrong, really. [OpenJ9](https://www.eclipse.org/openj9/) is IBM's implementation of the JVM, somewhat recently donated to the Eclipse foundation. It includes a JIT compiler and an AOT compiler. [Excelsior JET](https://www.excelsiorjet.com/) is an AOT Java compiler. [GraalVM](https://www.graalvm.org/docs/why-graal/) includes a JIT compiler, as well as a mechanism for creating native packages via an AOT compiler.<yawkat>: A jit compiler is useless for bootstrapping :) all these operate on bytecode, which you have to get to somehow first. You need javac or ecj for that, both of which are implemented in Java.<sindisil>: All have AOT options. JET is perhaps a bad example, since it isn't open source, so you can't add your own back end. Still, it could be used to bootstrap a new compiler written in Java. The bytecode translation step doesn't make bootstrapping significantly harder. Not sure why you think it does. Anyway, I assume I'm not understanding some assumption you're making about the definition of bootstrapping here, so I think there's little point to my continuing to argue.<yawkat>: But *not from java*. The whole point is that the java-&gt;bytecode step is, at this time, only implemented in java (javac and ecj). That makes it impossible to bootstrap java without either using old Java versions, or implementing a compiler in a non-java language. Bytecode aot or jit is completely irrelevant to this, you don't even need an aot option for bootstrapping, a jit would be enough. \"Bootstrapping\" means compiling a full jdk from sources without already having a compiled jdk on hand, using only, say, a c++ compiler. But at the moment, to compile a jdk, you also need a java compiler, for which you need... A jdk.<sindisil>: Bootstrapping *doesn't* preclude using a previously existing implementation for the bootstrapped language (or a subset thereof). In fact, that's often how it's done. Java is only special in that it's typically implemented as a JIT or interpreter, and even when AOT it requires a significant runtime (at least as compared to languages like C, C++, or Rust). That doesn't change the basic nature of bootstrapping. Besides, most existing JVM implementations are written in C++, so I don't see the point of your argument, even if I agreed with your constrained definition of bootstrapping.<yawkat>: Bootstrapping does very often forbid a pre-made *binary* of the product you're building. The purpose for this is ensuring binary trustworthiness. You can't do that if you use a binary blob compiler. What the JVM does and how it works is irrelevant. <http>://bootstrappable.org/<sindisil>: I give up. You have a very particular definition of bootstrapping a compiler, as opposed to the usual way of using that term (see https://en.wikipedia.org/wiki/Bootstrapping_(compilers) or http://foldoc.org/bootstrap for example). <yawkat>: Well, from your second link: &gt; The usual process is to write an interpreter for a language, L, in some **other** existing language. There is no interpreter that can run Java source code, that is not also implemented partially in Java. They all rely on javac/ecj.", "num_messages": 18, "avg_score": 153.5}
{"start_date": "1544756315", "end_date": "1545018595", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 10, "text": "<itswilson8>: A Deep Look at the Different Skill Requirements for Blockchain Developers <aullik>: I still don't understand why the F we need special blockchain devs. How much can you scam a company that they decide to use blockchain for everything it wasnt designed to do.<pixelrevision>: This is the same phenomenon as \u201ccloud\u201d 6-7 years ago. Sure some of the tech related to the term will end up relevant (secure transactions and distributed applications are useful) but in a few years it\u2019ll just be a tool/concept you can add to your stack. Everyone will be able to breath a sigh of relief until a new buzzword emerges.<aullik>: Well in a way, but blockchain is a VERY niche technology. I'm pretty sure the majority of distributed applications will shy away from it. I'm not even sure it will be used for transactions. Variations of this are clearly useful... like they are in GIT today (and 3 years before bitcoin was released). However those are not the variation blockchain enthusiasts are talking about.", "num_messages": 4, "avg_score": 2.5}
{"start_date": "1544695637", "end_date": "1544980156", "thread_id": "t3_a5mumu", "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 18, "text": "<Gotebe>: What you (and hordes of other people) call waterfall is not what the guy who coined the term imagined. I suggest you read what he wrote **past page 3** (it's 10 pages or so, it's short). If you do it with attention, you'll see that, well, Agile didn't change all that much. Vocabulary, mostly.<igouy>: Please provide a link to show \"what the guy who coined the term [waterfall] imagined.\"<Gotebe>: [Original afaik, pdf](http://www-scf.usc.edu/~csci201/lectures/Lecture11/royce1970.pdf) If you read with intent, you will find that **even then** people knew about iterations, feedback loop, testing, customer involvement... What clueless management put in place was the very beginning, for which paper says \"risky and invite failure\". <Edit>: I did not come up with what I am telling you. If you google further, you will see people basically defending Waterfall as the \"original Agile\". If you think about it, it's obvious: making software has innate nature and practitioners understood it early on. But management? Not to this day (which is why today we see moaning about how Agile is awful).<igouy>: Where does Winston Royce's 1970 paper say waterfall ?<Gotebe>: It doesn't. I don't know who put that name up - but it's for this very paper.<igouy>: Given that you don't know \"who put that name up\" how do you know \"it's for this very paper\" ?<Gotebe>: I just do. Ask somebody else where \"Waterfall \" came from, don't believe me. Or, you know, google? [Winston Walker Royce (August 15, 1929 \u2013 June 7, 1995) was an American computer scientist, director at Lockheed Software Technology Center in Austin, Texas. He was a pioneer in the field of software development, known for his 1970 paper from which the Waterfall model for software development was mistakenly drawn.](https://en.m.wikipedia.org/wiki/Winston_W._Royce)<igouy>: Look at the source provided for your quoted \"from which the Waterfall model for software development was mistakenly drawn\" in that Wikipedia article. It's the same 1970 paper that you've already agreed *does not mention* \"waterfall\". You really don't know.<Gotebe>: Erm... that changes anything, you think? I do not. What useful point are you trying to make? That I don't know the minutiae? Sure, you've won. Happy? <igouy>: You made a claim which seems to be untrue.<Gotebe>: I did and I don't care, because it is irrelevant to the bigger point I am making. I believe that by now, you see that my bigger point is correct, but are pissed off because it bursts some bubble of yours, so you are hanging onto what I said wrong to get sime petty revenge.<igouy>: &gt; irrelevant to the bigger point I am making State whatever your \"bigger point\" is up at [the head of the thread](https://old.reddit.com/r/programming/comments/a5mumu/agile_estimates_versus_noestimates_bridging_the/ebnz9pd/?st=jppttu6b&amp;sh=7d04431e) where everyone will be able to see it.<Gotebe>: Nope. I already did it high enough.<igouy>: Well as-long-as you keep your \"bigger point\" hidden, no one will check whether or not it seems to be true.<Gotebe>: It's not hidden, read above.<igouy>: You said all kinds of things. <Gotebe>: Just go up, what's important is there on top. And I know you're pretending that you don't see it, because it rattles your cage.<igouy>: &gt; And I know you're pretending that you don't see it\u2026 Once again, that is something you really don't know.<Gotebe>: Once again, I don't care if I really don't. And it's not important to me. Or anyone, really. Since some time here, I am just saying things for the heck of it. But here's the deal: I can stop anytime. You can't. You will continue saying something, anything, because it gives you a misguided impression that you have won something.", "num_messages": 19, "avg_score": 0.9473684211}
{"start_date": "1544616566", "end_date": "1544709314", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1801, "text": "<RunasSudo>: Investigating an early-2010s gaming DRM system (or: turning 41MB of DRM into 20 lines of C) <mallardtheduck>: The whole \"skip every 3 bytes\" thing sounds like a pointer arithmetic error in C... i.e. someone changed a parameter from `char *` to `int *` and left more-or-less everything else the same.<reyqn>: I did exactly the opposite of this 5 minutes before my first C project presentation in school (don't ask why). Needless to say, the presentation did not go as planned...<Tormund_HARsBane>: That's why you use version control<reyqn>: Yeah I think the only kind of version control I knew at the time was \"project - copy (2) - final version - copy - this time final version for real\"<redrumze>: That\u2019s not far off how team foundation server did their version control. <Thaurin>: I'm not sure if you're trying to be funny or dead serious, but that's *exactly* how TFVC did it and I can't believe we were using it in the past!!<redrumze>: I was exposed to it in a 6 month contract gig. I found it easier to just manage my own folders and ideas. Branches were a no go. <Thaurin>: We tried using branches (just a dev branch and a main branch), but even that was just too much of a mess to manage. When we wanted to publish a feature or fix, but not include another feature or fix, we'd probably comment the stuff out. Horrible. I can't imagine how Microsoft used to do it. A Microsoft guy at a git talk said they used to have a full-time, dedicated person responsible for merging. <redrumze>: Yeah, I would have folders of folders and they\u2019d be named with what i was primarily working on. I also did mass comments for when I got lost. I\u2019m glad to be using Git and I\u2019ve stated the fit flow process of developing and It\u2019s a godsend from where I came from. <Thaurin>: Even ~~git flow~~ Git Flow is too complicated for me. As every commit represents a full snapshot of the code base, I will just tag my important commits and don't want to worry about more than one permanent branch. But that's just me. Yeah, git allows for so much freedom and gives such a sense of security, it's definitely the best thing to happen to my work flow since a long time! But some people just don't seem to ever get a good grasp on git, unfortunately... <Muzer0>: There is no one standard git workflow. One of the great things about git is you can pretty much use or ignore its concepts however you like; however best suits the project you're doing. Seems like you're doing just that! <Edit>: sorry, just realised I'm an idiot and you're referring to an actual workflow called Gitflow. I'm leaving this up though regardless!<Thaurin>: Yeah, I meant Git Flow. I should have capitalized it! I figured that by \"stated the fit flow process\" /u/redrumze meant \"started the Git Flow process.\" <redrumze>: Yeah. I started to use a develop/feature branches and it\u2019s kept me organized to the moon... sometimes I find my self forgetting I\u2019m on a feature branch but the noob in me saves those files some where else and cleans the branch im on and start a new one and commit my changes back to it. I\u2019m aware there is a git command for everything but sometimes the stupid method is all I need. <Thaurin>: Just learn the power of the interactive rebase and you should be fine for most things.", "num_messages": 15, "avg_score": 120.0666666667}
{"start_date": "1543591289", "end_date": "1543747886", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 23, "text": "<izackp>: This explains why maintaining reactive code makes me want to eat my insides <minionslave>: In what environment do you use this? I was exposed to it through Angular 6 + development using RxJs. Honestly, I read the docs and put code in but it's really hard to grasp. I just have faith it'll work. I am glad i'm not the only one. <izackp>: I recently started working on an Android project which pretty much uses Reactive programming for everything. DB changes, API, updating the UI. I'm so used to understanding the cause and effect of everything I do. I try to follow each chain of listeners but it's very difficult and once I have an idea of what it does then that's just one chain of many which I'll soon forget. God forbid this chain of listeners intersects with another. I spent a half a day on a simple bug where an alert didn't pop up under specific circumstances. Because a value was set to null to trigger another behavior which caused the value being observed to change state before the final observer gets the event. Either way, there's a very good chance that this was just poorly written Android code. Though I've been a professional iOS developer for a good 7 years and pretty any code written with key value observers was guaranteed to have bugs and be a pain to deal with. Mostly because it hides cause and effect. The fact that the whole pattern is based on side effects bothers me. Set a value.. oh wait that updates the UI. Oh you weren't ready? Oh you have several other values to set first? Too bad each will update the UI. Maybe you should create an intermediate object to update when you're ready. The code being ran is never done synchronously so the callstack is virtually useless when debugging. I've only found this pattern useful when a third party API didn't give me an event I needed. So I created it myself by observing certain values which worked and felt like it was a good solution. I don't know why I don't hear more hate for the abuse of this pattern. Maybe this stuff is easy to create projects with as long as you don't have to look at other people's code or maintain it? Perhaps the people who agree with me are just not vocal about their opinion in this 'everyone is sort of right' society? Or maybe we're just to dumb to work with reactive code? Which honestly is stupid. Clean code is code that is easy to read and understand. Is reactive programming easy to read and understand? I'd have to vote no. <matthieum>: &gt; Because a value was set to null to trigger another behavior which caused the value being observed to change state before the final observer gets the event. Ouch; that's painful. It's really a case where you'd like the type-system to enforce the immutability of messages, if a listener wants a modified version, it's welcome to make a copy.<oldsecondhand>: It's sounds like a half-assed forward chaining expert system.", "num_messages": 5, "avg_score": 4.6}
{"start_date": "1544737093", "end_date": "1544794650", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 9, "text": "<stargazer83>: Dockerize your development environment <hasen-judy>: &gt; Back when I first started with Docker, one of my main struggles was how to make any real use of it. I couldn\u2019t really see how it would fit any of my use cases. Huge red flag. Are you just trying to jump on the hype wagon?<ruinercollector>: It's pretty justified hype. Being able to run your app immediately in a clean, reproducable environment that mirrors a fresh production system and is isolated from other developers is extremely useful. There are other ways to get there, but most of them are a lot slower to develop and run then docker.<hasen-judy>: All hype can be justified by some sound bite. That's what makes the hype. The key is OP did not see any use for it, but he still forced himself to learn it ...... just because?<ruinercollector>: Sometimes the use for something is not immediately obvious, but it is worth looking into further. If credible sources in the industry seem to be finding something extremely useful, it is sometimes worth looking further than just introductory material before assuming that it's of no use to you. There are plenty of things that are hype, but disregarding everything popular because \"it's probably just hype\" is just as short-sighted as blindly following the hype and implementing everything that's cool right now.", "num_messages": 5, "avg_score": 1.8}
{"start_date": "1543603457", "end_date": "1543616578", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 4, "text": "<RickyWanga>: Laptop for Programming <nilamo>: &gt; Just because it has a computer in it doesn't make it programming. If there is no code in your link, it probably doesn't belong here. /r/programming is not a support forum.<RickyWanga>: Sorry for that, will keep in mind for the future.", "num_messages": 3, "avg_score": 1.3333333333}
{"start_date": "1544744852", "end_date": "1544827198", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 5, "text": "<jeanlaf>: 'Full Stack Software Developer' Named Fastest-Growing Job of 2018 <scurtie>: Sometimes, I feel like I could write a book about this and exploited business practices. Business use the word developer vs engineer because they\u2019re cheaper to hire. Monster companies like Facebook and google releasing micro managed \u201cOSS\u201d frameworks so developers are more abundant and cheaper. \u201cFull stack\u201d because a medium sized company literally has no idea the breadth of the fields and why hire more than one when it\u2019s \u201ccheaper\u201d. <Caraes_Naur>: Also, HR and recruiting toss around many terms they don't understand and impose ridiculous requirements such as \"10 years of progressive web app experience\".", "num_messages": 3, "avg_score": 1.6666666667}
{"start_date": "1544627638", "end_date": "1544765850", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 20, "text": "<Andrew_Sunshine>: A cross-editor plugin to improve any text editor or IDE on Windows OS. <polymorphiced>: How do I get it? Is there any documentation? It looks cool (if a bit cryptic), but that website is awfully light on details!<Donald_Zhou>: You can get it on the website download page, with ConyEdit running in background, you can use its command in any text editor or IDE. The document is on The Tutorial Page of the website and on the software, and every command has a help message on details, you can get the help message with the command 'cc.&lt;command&gt; -h'. The website is on improving.<polymorphiced>: There is no download page.<Andrew_Sunshine>: There is no download page on the mobile website, it is on PC website.", "num_messages": 5, "avg_score": 4.0}
{"start_date": "1539266127", "end_date": "1539364114", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 426, "text": "<alexeyr>: Protobuffers Are Wrong <evmar>: The HN thread has a few responses from some of the people involved in designing protobuffers: <https>://news.ycombinator.com/item?id=18188519 In particular read the comments by `kentonv`. I thought this in particular was pretty insightful: &gt; The main thing that the author of this article does not seem to understand -- and, indeed, many PL theorists seem to miss -- is that the main challenge in real-world software engineering is not writing code but changing code once it is written and deployed. In general, type systems can be both helpful and harmful when it comes to changing code -- type systems are invaluable for detecting problems introduced by a change, but an overly-rigid type system can be a hindrance if it means common types of changes are difficult to make.<tsec-jmc>: &gt; is that the main challenge in real-world software engineering This is a common \"pragmatist\" approach to shit that I find absolutely garbage. \"X concept doesn't work in the _real world_ because in the _real world_ we don't have room for sane practices\". How can anyone speak about overly rigid type systems being a hindrance with a straight face? Yes, type changes aren't a 2-step change unless they're in parametrically polymorphic functions _because they inherently change the behavior and semantics of your program_. Change an int field you were using in some operations with addition to a string, and now your operation is concatenation. Does this change matter? A good chance is: it actually does, or if it doesn't, you're treating addition like a monoidal combine (in which case the code could be polymorphic). If you need something such as \"general behavior\" for certain constructs, that's what polymorphism is for, and stronger abstractions (depending on your language, like typeclasses or interfaces in the generic type) within it. \"Pragmatists\" like this are absurdly anti-intellectual and it angers me. **EDIT**: I hope, going forward for anyone reading this, that the above comment isn't interpreted as \"protobuf never solved any problems and is wrong going forward for all time\". My general, wider point is that protobuf is the wrong tool _in hindsight_, when you're not already stuck with years of legacy protobuf systems.<a_marklar>: &gt; \"In theory there is no difference between theory and practice. In practice there is.\"<arbitrarycivilian>: I hate this quote. Either 1. The theory is wrong or incomplete 2. The theory is being misapplied There's not some magical dividing line between theory and practice<a_marklar>: There actually is a magic dividing line, thats the part where you actually do it.<arbitrarycivilian>: Yes you do have to \u201cactually do it\u201d. That\u2019s how you validate a theory. If you don\u2019t get the results you expect you have to either rework the theory or the assumptions it makes. That\u2019s how science works <a_marklar>: Right... so what do you do to validate theories on something like serializing structured data? Release a framework for doing so in multiple languages to hundreds of thousands if not millions of developers? Iterate on it over 17 years and 3 major versions? No, of course not. And that's where the difference is between theory and practice.<loup-vaillant>: \"Release then iterate\" is not going to work. Y' know, compatibility\u2026 But you *can* validate your ideas by testing them in private, then releasing some beta version, or even doing experiments in the lab (see how fast people pick up the tool, how many mistake the make\u2026). I have written a [crypto library](https://monocypher.org/). The real deal, intended for production, high stakes and all. But with your kind of \"practice\", this would be impossible. A \"real\" crypto library has to implement TLS, be used by billions of users on the web, and is inevitably a crumblingly complex pile of error prone (and error _ridden_) pile of underfunded garbage. (I'm not blaming the authors, they probably did better than I would have.) Well, no. We can do much simpler, and I did it (so did other people, by the way, I'm not even the first). Serialising structured data is likely similar: protobuf were written under certain circumstances that made them the garbage they currently are. Could it have been better? I don't know. Could we make something better _now_? Absolutely. And at a fraction of the cost. Just look around for other serialisation frameworks. <a_marklar>: I agree that we could definitely make something better, my point was not about protobuf itself but that the differences between theory and practice are immense. It is obviously an extreme form of 'practice' and I don't think that it's a good benchmark. I'm a little shocked that so many people seem to think that there is little difference between theory and practice, especially in this community. We're exposed to the differences constantly in the form of bugs.<loup-vaillant>: Thing is, the whole *point* of theory is to give you insights about the practice, so you can plan ahead, explain why stuff doesn't go the way you want to etc\u2026 Thus, any difference between theory and practice that isn't a mere approximation is a _failing_ of theory. Such failings should be corrected, not accepted as a fact of life. For instance, my crypto library: when I got bug reports, I didn't just corrected the bug. I wrote a regression tests, I looked for the buggy pattern *everywhere* so I could eliminate it for good, and often expanded the test suite so I could eliminate a whole class of bugs (even if they were absent to begin with). I _amended_ my theory so I could not only have less bugs, I could be more and more _confident_ that there were no bugs left. <a_marklar>: &gt; Thus, any difference between theory and practice that isn't a mere approximation is a failing of theory. Can you point me to a non-failing theory? It sounds like we are saying the same thing from different perspectives. Your last paragraph seems to be exactly what the saying I quoted initially is getting at.<loup-vaillant>: &gt; Can you point me to a non-failing theory? Programs that just work. Planes that just fly. Stuff like that. Also, lots of theories have margins for error. Like, \"planes _almost_ never crash (one time in a million)\". A theory doesn't have to have infinite precision to be correct and useful. (Of course, the more accurate the theory, the more useful it is likely to be.) &gt; It sounds like we are saying the same thing from different perspectives Then you probably went over the top. You said that _\"the differences between theory and practice are immense\"_. No they're not. If the difference between a theory and its application is immense, that theory is just crap, and it doesn't get to represent \"theory\" in general. When you hold yourself to higher standards, things get much better. There are software projects for instance that took over a year to design, but only a couple months to actually code and tests, yielding a maintainable code base that does the job. When you idea of \"theory\" is just what you happen to think at the start of the project (the point where you know *least* about the project), well, sure, the difference between your strawman of a theory and subsequent practice will be immense. One can only do so much in 5 seconds of thinking. <a_marklar>: &gt; Programs that just work. Planes that just fly Those are practice though, right? <Edit>: The more I think about it, planes are the perfect example. The science and theories of flight were pretty well established for centuries before practice caught up.<loup-vaillant>: &gt; &gt; Programs that just work. Planes that just fly &gt; &gt; Those are practice though, right? Those are things that in practice, match the expectations of theory. We have very good reasons to believe a given airliner will fly safely, and what do you know, 99.999% of the time, it does. <a_marklar>: Hey I can tell you're not willing to be convinced, and thats alright I'm not either. We obviously have different definitions for theory and practice, among other things. I'll leave you with the thought that if there was no difference it would just be considered the same thing. From there it is a question of how big is the difference, and I think you brought up a perfect point with the history of flight. Take care.", "num_messages": 16, "avg_score": 26.625}
{"start_date": "1543513620", "end_date": "1543679243", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 329, "text": "<steveklabnik1>: A new look for rust-lang.org <DC-3>: I think this is a real step backwards, to be honest. When I see the webpage of a new language, I want to be presented from the outset with the features it has that differentiate it from any other language that I might care to learn and use. It's very nice\\* to say *The programming language that empowers everyone to become a systems programmer*, but it doesn't tell me anything about the language at all. A similar problem happens with Linux distributions - every distro is tripping over its toes to tell you all about how it has 'modern design' and 'gets out of your way and lets you get your work done', but you have to scroll three pages before you can see what window or package manager it uses. I applaud the Rust team for trying to make Rust beginner friendly - but even a beginner's first question will probably be 'OK, but what does Rust code actually look like' - and a code sample like the old Rust calculator example (I think the more recent example is less useful in this regard) is a really nice way to demonstrate that. Have faith that beginners won't be scared off by seeing the phrase 'trait-based generics' - because any reasonable person trying something new expects to see things that they don't understand right from the outset. \\* albeit probably wrong - not everyone has it in them to be a footballer, not everyone has it in them to be an author, and not everyone has it in them to be a programmer.<steveklabnik1>: Thanks for the feedback! &gt;I want to be presented from the outset with the features it has that differentiate it from any other language that I might care to learn and use. We have heard, overwhelmingly, over the past few years, that most people \\*don't\\*. It's impossible to satisfy everyone... &gt;and a code sample like the old Rust calculator example (I think the more recent example is less useful in this regard) We liked the code too, but it's so hard to get a good snippet, as you notice.<DC-3>: Thanks for the response. &gt; We have heard, overwhelmingly, over the past few years, that most people *don't*. This is surprising to me. Without wishing to stereotype, I consider most programmers (especially systems programmers) to be pragmatic, engineer types, who simply want to get their project done well, not be the proverbial Fire Mario. To what extent are you sure that the feedback you have recieThanks for the response. &gt; We have heard, overwhelmingly, over the past few years, that most people *don't*. This is surprising to me. Without wishing to stereotype, I consider most programmers (especially systems programmers) to be pragmatic, engineer types, who simply want to get their project done well, not be the proverbial Fire Mario. Would you be able to reveal where this feedback has primarily come from and to what extent you are sure that the feedback you have received is representative of the programmer population at large?<steveklabnik1>: &gt;where this feedback has primarily come from and to what extent you are sure that the feedback you have received is representative of the programmer population at large? It's come from our annual surveys, as well as many, many, many in-person conversations with people all over the programming world. &amp;#x200B; I think this is the crux of it: &gt;I consider most programmers (especially systems programmers) to be pragmatic, engineer types, who simply \\*\\*want to get their project done well\\*\\*, not be the proverbial Fire Mario. Emphasis mine. I think this is very true! The problem is, \"trait-based generics\" does not say what kind of problem that they solve. Saying \"this is the problem Rust solves\" \\*is\\* \"being fire mario\". You can only say so much in a slogan, so that's why it's pretty vague. The next two sections, right below that, get into more of what you're talking about, which \\*is\\* what people do want: performance, reliability, productivity, and examples of what kinds of uses you might actually do things with. But it doesn't say \"zero cost abstractions\" or \"compiled into machine code\". Does that make sense?<DC-3>: I take your point, but I don't think this is how the website comes across. If you wanted to say that 'if you use Rust your programs will probably be better' (which, by the way, I agree with - I love the language!), you'd say something like what Go says: &gt; Go is an open source programming language that makes it easy to build simple, reliable, and efficient software. But instead, you focus on the idea that somehow Rust makes you a good programmer, which doesn't make sense to me - while good languages make good programs, good languages do not make good programmers! As I have mentioned elsewhere in this thread, there is no-one who can become a systems programmer with Rust that could not have done so with C. That's as nonsensical as saying that Black and Decker allows anyone to be a carpenter. Oh, and another point (sorry to pile on, please understand that my criticism comes from my desire for the language and its community to grow and improve) - I do think that the new website design is uglier than the old one. I really liked the old website design - it was simple, understated, and made good use of the page. I don't like the new gigantic text and strange splashes of colour. It actually is a little hard on my eyes to read text whose top half is on an off-green background and whose bottom half is on an aubergine one.<steveklabnik1>: &gt;I take your point, but I don't think this is how the website comes across. That's super fair! As we said in the post, we don't think it's perfect. Please put these ideas on the tracker! &gt; (sorry to pile on, please understand that my criticism comes from my desire for the language and its community to grow and improve) Don't worry about it; if I didn't want to hear it, I wouldn't be replying to these threads. It's really tough to please everyone with design. Thanks for going into detail.<dimitriye98>: You keep saying \"It's really tough to please everyone with design,\" but frankly, it seems the feedback in this thread has been pretty clear that this new site doesn't please *anyone*. I'd greatly encourage you to do some split testing and see which site leads to more conversions from people new to the language, because frankly, I'm fairly certain it'll be the old one.<steveklabnik1>: &gt; it seems the feedback in this thread There are more places than just this thread.<dimitriye98>: Certainly, and I've seen the subset of those places which I frequent, the /r/rust thread, and the [HackerNews](https://news.ycombinator.com/item?id=18562049) thread. Both of which are negative, even if HN is less overwhelmingly so. If you have a venue where there's been mostly positive feedback, I'd love to be linked it, because as is, most feedback I've seen overall is predominantly negative.<DC-3>: Twitter is relatively positive. I imagine the Rust forums might be, though I haven't checked. To be frank, those sites are a little sycophantic at times, so it's not that much of a surprise. They are also a bit more tech-left, so are probably more inclined to approve of the empowerment slogan.<Benjamin-FL>: [This](https://users.rust-lang.org/t/the-beta-rust-lang-org-pr-debacle/22778/26) is the main feedback-related thread I've seen on the rust forum. It is *not* generally positive. I'm honestly really curious where I can find any significant positive feedback on the new design.<DC-3>: Look at Steve's twitter replies. That's it, as far as I can ascertain.", "num_messages": 13, "avg_score": 25.3076923077}
{"start_date": "1544712750", "end_date": "1544767348", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 29, "text": "<DetachedObserver>: Helping blind people learn to code [deleted]: this is cool but surely they would be better at something else idk why we want everyone to code. or why they would want to code<send_codes>: Imma give you the snarky answer and let you decide if you want a better question answered: Because people like you don't get to tell people like me what we're passionate about. Thank fuck too, or we'd still be stuck making brooms. [deleted]: as a passion sure. that's great. but I don't think blind folks would be able to get nearly as much done just on account of how screen focused computers are.<send_codes>: Front end development is one tiny overblown section of the industry. Accessibility tools have been around for literal decades. Go watch Saqib Shaikh's demonstration of using VS2017 with a screen reader from Build 2017. We're capable, competent, adaptive, and how dare you even begin to suggest that anyone you don't know can or can't do something based solely on your own miscomceptions and ignorance. [deleted]: it's cool that tools are improving and I'm sure that's great and all and maybe if I lost my sight I'd try them out. altho tbh I'd change jobs most likely. and yeah I'm sure blind people are pretty capable and certainly adaptive to their circumstances but there's no getting around the fact that unfortunately we have a limited number of senses and computers as they are right now rely heavily on vision. as you've pointed out you can try to adapt to that, but it's not even close to what the average person with all their senses can do.<SuperNintendoChlmrz>: I guess your account is deleted for obvious reasons, so I'm yelling at the clouds probably, but man this is a fucking thoughtless and ignorant statement. For the most part, computer code is text based and not \"visual\". The tools that YOU may use are visual, but there are other means of manipulating text and getting feedback that can be just as efficient, and perhaps moreso given that visually impaired folks won't be as distracted by the sheer volume of eye-fuckery driven by the ad tech industry when interacting with the online world.", "num_messages": 7, "avg_score": 4.1428571429}
{"start_date": "1542990432", "end_date": "1543230939", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 296, "text": "<cryptoz>: Flutter: the good, the bad and the ugly <thosakwe>: Dart is *really* unpopular in this sub.<BIGSTANKDICKDADDY>: Dart is being developed by Google because Google has a business interest in migrating off the JVM. If you don't have those same business interests, what is Dart offering? It's lacking in features (No multi-threading, poor support for higher order functions, no null safety, no union types, no extension functions, no data classes, syntax quirks like the cascade operator, etc.), it has less robust tooling, a less mature ecosystem, etc. Unfortunately Google is still playing catch-up after pivoting Dart from dynamic to static typing, and with the technical debt of existing Dart code in the wild, modern features (Like null safety) may never be added. I'm excited for where Dart might be in 5 years, but it's really difficult to sell Dart on its own merits today. <brogam>: Dart is still the best thing on the frontend until true fast and small webassembly languages with DOM access happen. Even then I would say that Dart can compete just fine. The entire debate about null safety or type safety in general is ridiculous, anything is better than javascript. I too miss some features from typescript but whatever, at least I have 'this' which is way more important.<jl2352>: I would prefer JavaScript because then I can use TypeScript, and at least then I have null safety. Plus everything else it does.<bartturner>: JS is not efficient enough for memory management in creating and destroying objects that you need for Flutter. <jl2352>: Then implement the same GC for V8, Go, or their Java implementation on Android. This isn\u2019t a Dart thing, this is them choosing not to bother with anything else.<bartturner>: Ha! The issue is how Flutter needs to create and destroy objects at a much more intense rate then normal. Here maybe this would help. \"Why Flutter Uses Dart\" <https>://hackernoon.com/why-flutter-uses-dart-dd635a054ebf You do realize Google created V8? It all goes together. Flutter and Dart. Would not work with JS. Well would work but NOT get the UX.<jl2352>: Most of the ideas in the development stack are actually taken from modern web development.<bartturner>: Yes they are borrowing a lot from the web side. The core piece being Skia. But what is so awesome is that the UX is more like web but without the major negative of web and that is the DOM. It is like all the plus with web without the major negative. I am old and done a lot of development and Flutter is just an incredible developers UX. But what is very different is how Flutter creates and destroys objects and we do NOT have the same with web development.<jl2352>: You are awfully zealous about Flutter with only handwavey statements that it\u2019s better. In practice there is nothing here they couldn\u2019t have done using a different language.<bartturner>: &gt; You are awfully zealous about Flutter I am. I am older and done a TON of development. I have done pretty much every type of GUI development there has been. I just can't get over how awesome the developers UX is with Flutter. &gt; In practice there is nothing here they couldn\u2019t have done using a different language. Well that most certainty NOT true. Flutter creates and destroys objects at a more intense pace then I am aware of anything else? Why Dart was a critical component. But lets try another way. Why do you think Google chose to use Dart? Google process the vast majority of JS in the world. Both front-end and back end. Maybe 10x more than the next? Nobody would know JS better then Google. Yet they chose Dart. Why? <jl2352>: Dart had been an utter failure for it\u2019s original purpose. So they had a lot of investment in a language that had gone no where. Meanwhile they want to move off Java due to all of the hassles with Oracle and a lack of control. Third they want people using a Google language. One developed and predominantly owned by Google. So they picked up the failed Dart language and repurposed it for this.<bartturner>: Thanks! Did not see. You realize Java script has nothing to do with Java? I get similar branding but the two have ZERO to do with the other?<jl2352>: &gt; You realize Java script has nothing to do with Java? I never said they were.<bartturner>: You wrote reason #2 was because of Java and Oracle?<jl2352>: Partly. I also said because of *'a lack of control'*. Google can't just add something to Java. They have to go at the pace of the OpenJDK (i.e. slow and safe).<bartturner>: This why I asked if understood Java has nothing to do with Java script. So reason does NOT make sense. Google used because they needed to for Flutter. Exactly what they explained why use. Which is what we should want as move things forward and keep innovating. Same with Zircon. JS is old.<jl2352>: Android development is heavily based on Java. It is the defacto language for Android development. There was even a court case over it between Google and Oracle.<bartturner>: Yes. What does that have to do with using Dart with Flutter? I do not see the connection?<jl2352>: I believe in the long run Google want to move Android development away from Java, and onto their own language. Like Apple with Objective-C and now Swift, Microsoft with C#, and Facebook with Hack, Flow, and Reason.<bartturner>: I do think it is time to get off of Java for a variety of reasons and really technical reasons. Think Google is fine legally to continue to use. But that does NOT answer the question? Javascript has NOTHING to do with Java. So Google could have used JS. I have told you why they used Dart instead. Google has indicated why they did and others have also explained why they had to use Dart. You do not believe that is why. Which find right away weird. Why would Google lie? But put that aside. I was curious why you think? Now you have brought Java into the discussion. But we are talking Flutter and using Dart. So the Java argument would NOT be a reason for them using Dart with Flutter. I mean you do get they could never have used Java for technical reasons? OMG! You are not suggesting they would use Java are you? I am getting a bit of a headache trying to follow your logic. It is a bit all over the place. They had to use Flutter for technical reasons and they did. Pretty simple. Personally I am glad as like to see things more forward. JS is ancient and was designed in just a couple of days. I have used a ton and tend to like. But I can see would NOT have fit with Flutter and it is time to move forward.", "num_messages": 22, "avg_score": 13.4545454545}
{"start_date": "1544621399", "end_date": "1544728044", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 426, "text": "<rogerwcpt>: The Rise of Microsoft Visual Studio Code <ImNotRedditingAtWork>: I'm interested to know if the reason the Go developers did better on the interview was because A) People who write go tend to actually be better developers or B) The interviewers who interviewed them have a bias for Go developers. I had a colleague be told in an interview to never write code in C# for the interview unless the job was specifically for C#, as interviewers are biased against C#. I have no idea if that's true or not, but it's an interesting thing to think about.<supercyberlurker>: After enough interviews, you realize half of it is just gambling. That is, you're not *really* dealing with people who are completely objectively evaluating your skills based on rational criteria garnered from the coding questions. You're much more likely dealing with people just confirming their pre-existing biases and prejudices. That's almost even fair, since they are really testing to see if they could stand being around you. The gamble is on culture-fit.<vim_all_day>: You know, I didn't want to believe this early on in my career, but I'm starting to think a good part of \"nailing\" an interview is truly a gamble. Sometimes, the programming puzzle they give you just clicks and you look impressive in solving it quickly. Sometimes you just, blank, and you look dumb. Honestly, it feels like all the job offers I've received were based more on good luck in an interview rather than my *actual* skills. I don't know if that's good or bad, but here we are.<ivquatch>: Success in an interview is really defined by the criteria of the organization doing the hiring. You can \"hack\" the process by figuring out what it is they want to hear. Acing the interview, however, doesn't guarantee that it'll be a good fit for both parties.<shevegen>: Which is why these interviews are so broken and a waste of time. I guess the only \"use case\" they have is to weed out some people, just to lower the number of people to screen for.<ivquatch>: They ultimately want a repeatable process for hiring quality talent. The outcomes, as we know, are usually mixed. \"Screening\" is probably the most reliable part of the process (and that's being generous). Once you've got that candidate in the door, however, there's no proven guideline for assessing the potential he/shee has for on-the-job success. The issue with hiring is that you never really know what it's like to work with someone until you actually work with them. That's true on both the employee and the employer's side. I dunno what more you can do than give someone who's passed screening a probationary employment status. It's not really fair to the employee to do this, though. Companies should commit to the people they choose to hire as a show of good faith.<pts_>: It's not up to the companies. Since companies treat employees as commodities, employees should take bulk salary and give warranty of a year, like commodities. &amp;#x200B; Right now employees are open to being bought at a fraction of their cost, placed on premises, maybe used, dirtied and possibly thrown away. Tell me which retailer will let you do it with a commodity?<ivquatch>: Commodities don't have bills to pay :)<pts_>: Retailers do.", "num_messages": 10, "avg_score": 42.6}
{"start_date": "1544117142", "end_date": "1544172273", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 3916, "text": "<tomzorzhu>: It's official, Chromium is coming to Microsoft Edge <kwiwksh>: I am really wary of Chromium being \"the\" de-facto rendering engine. Almost every major desktop and mobile browser uses it with the exception of Safari and Firefox. Vivaldi, Brave, Opera, DuckDuckGo Mobile all use Chromium. How much influence does Google have over Chromium? <EDIT>: Related: https://twitter.com/_zouhir/status/1070728681834786816 EDIT 2: Seeing it this way makes a lot of sense https://twitter.com/SwiftOnSecurity/status/1070822156731322368<matthieum>: &gt; How much influence does Google have over Chromium? Well, in this case, I suppose Microsoft would gain a large influence themselves, so maybe it would help balance Google's agenda (if any). I am more worried about the impact of a virtual monopoly of Chromium with regard to standard compliance and security risks.<AyrA_ch>: &gt; I am more worried about the impact of a virtual monopoly of Chromium with regard to standard compliance and security risks. That would mean that Microsoft and Google had to agree to non-compliant behavior. I'm not sure if the likelihood of that happening going up or down with MS joining Chromium development. Comparing with Google and Apple, Microsoft is probably the least evil of them by now.<Eirenarch>: But this is not how it works. They agree on a behavior, put it in Chromium and it becomes the de facto standard. Basically Google gets to write the standard and everyone else can fuck off.<AyrA_ch>: &gt; Basically Google gets to write the standard and everyone else can fuck off. But now they have to agree with MS too.<Eirenarch>: No, they don't. Google controls the Chromium project they can add whatever code they like and reject whatever pull request MS sends them.<AyrA_ch>: Yes but MS can fork the project at any time and continue doing their own thing, keeping the codebase in sync except for the disagreement. Now there are two chromium versions that are fully binary compatible which makes them easy to switch at any time for people that use them in their projects. I would say google doesn't wants that to happen because it could potentially massively reduce their browser market share if the MS version of chromium engine is as easy to use as the Google version.<Eirenarch>: This is true but in theory they could have done it now. If you don't control enough market share or have your browser be the favorite of the devs then you can fork off as much as you want, the feature in the most popular browser will become the de facto standard and websites would stop working on your browser. Even if ALL Chromium projects switch to the MS version Chrome is still much bigger and then you have the Google web properties. Have you tried using YouTube with Edge? Take this super slow Web Components polyfill Edge! Who cares that it is not standard yet or maybe ever :)<AyrA_ch>: &gt; This is true but in theory they could have done it now. Yes but the idea is that you contribute first and then fork if needed. Otherwise you'll hear people complain that it's a dick move. If they fork after a disagreement and announce to the public that the reason was a dispute over google trying to push their own web standards it looks like google is the bad guy and not MS. &gt; the feature in the most popular browser will become the de facto standard and websites would stop working on your browser. Considering how big the market share of Windows is, the share of Edge users could massively increase if the behavior is almost identical to chrome. You get a chrome that is installed by default but will not sell your data to google. So for most users there is likely no longer a reason to use chrome.<Eirenarch>: &gt; google is the bad guy and not MS. Google is the bad guy with the most popular web browser and the most popular web properties. As if users will care or even hear about who is bad and good. The share of Edge users would not increase or it would increase now.<AyrA_ch>: &gt; Google is the bad guy with the most popular web browser and the most popular web properties. Your average user doesn't knows that. &gt; The share of Edge users would not increase or it would increase now. The share of Edge users will not increase before MS advertises it to the normal consumers. It's likely that they push it via an update that either on purpose or \"accidentally\" changes the default browser to edge. The closer Microsoft gets their browser to look and behave like that from google the less likely are users to download chrome in the first place. The look and feel of the user interface itself is part of the open source portion of chrome, which means they could literally make a browser that looks almost identical without fearing that google would take legal actions. As an added feature of the pushed update, it could automatically import bookmarks from chrome. Your average user is not going to spot the difference.<Eirenarch>: &gt; Your average user doesn't knows that. Of course he does. He uses Chrome and uses YouTube and Gmail. Microsoft can't get people to use Edge because Google chose to make YouTube slow on Edge. Good luck advertising!<AyrA_ch>: &gt; He uses Chrome and uses YouTube and Gmail. Which both work well with edge too. Youtube and gmail are both even decently fast on IE11, I just tried. Youtube became slower with the design change. Your average user is going to blame a slower experience on that, not on the browser, because it's only youtube that's got slower, not other sites they use on a regular basis. &gt; Microsoft can't get people to use Edge because Google chose to make YouTube slow on Edge That becomes more difficult if Edge starts using the same engine. If google continues this behavior, microsoft will just put their services on a compatibility list that makes the browser pretend to actually be Chrome.<Eirenarch>: &gt; Which both work well with edge too. This is simply not true. YouTube sucks bad on Edge for about a year now because of the web components polyfill. My girlfriend literally switched to Opera from Edge because of the YouTube slowness. &gt; That becomes more difficult if Edge starts using the same engine. True. But they still won't have the same features. If they ship rebranded Chrome instead of providing their own UI they can't push Windows features anymore. Of course if they do get market share they can fork, they can even fork the engine but I don't see any way of that happening. Simply being the default isn't enough anymore.<AyrA_ch>: &gt; But they still won't have the same features. The only feature that I could imagine a user would be missing would be extensions, but that API is part of chromium, not chrome. Which means that anyone who publishes extensions in the google web store, can publish the exact same extension in the microsoft store without the need for any modification. So it would be no surprise if they would allow publishers to upload their extension before the official launch of the new engine, ensuring that the most used extensions are already there to begin with. Let's look at the other features that make the difference between chrome and chromium **Automatic updates** Windows has that already and Microsoft is very likely continuing to use that for Browser updates. Updates are forcibly enabled on new versions of windows anyways unless you know the hidden settings. The benefit compared to chrome is that no additional service is required anymore for that. **Crash reporter** MS had that for years in Windows, no need for a separate implementation here either **Web store** As explained, this is trivial to port/integrate **Codecs** Chromium lacks closed source decoders but Edge and Windows have these already with the proper licenses so no problem here either to integrate them. In the end, there is nothing the user really has to miss if he switches back. Most companies would probably also prefer to use Edge rather than an additional 3rd party software when staging devices<Eirenarch>: &gt; The only feature that I could imagine a user would be missing would be extensions, but that API is part of chromium, not chrome. Until Google add features directly to Chrome and not to Chromium. Also the extensions are in the Google store not in the MS store. For the record Edge as it is today has Chrome-compatible extensions API. Still very few bother to publish. Again if MS ship Chrome without changing it they might as well ship Google's Chrome. MS needs a browser to develop Windows. They need something to showcase the pen input for example. IE for years was used as a vehicle to showcase Windows features for example the Window previews were first present in IE. If they are not doing a better job than Google they will continue to be ignored by users and they are not doing better job than Google because they suck.<AyrA_ch>: &gt; Until Google add features directly to Chrome and not to Chromium. Which can be reimplemented without looking at the source code if needed. &gt; Also the extensions are in the Google store not in the MS store. For the record Edge as it is today has Chrome-compatible extensions API. Still very few bother to publish. Because it's not the same thing. You have to publish for both browsers separately which you no longer have to do if edge switches to the same engine, you just have to upload the extension twice but not assemble it twice. &gt; IE for years was used as a vehicle to showcase Windows features for example the Window previews were first present in IE. These Windows features are operating system specific UI/Feature and have nothing really to do with the underlying rendering engine of the browser. &gt; If they are not doing a better job than Google they will continue to be ignored by users Right now they are not being ignored but actively avoided. Providing a browser that has the exact same behavior as chrome takes away any reason to switch to chrome in the first place.<Eirenarch>: &gt; you just have to upload the extension twice but not assemble it twice. I am not sure how much differences there are now but I am sure people won't do that either. &gt; These Windows features are operating system specific UI/Feature and have nothing really to do with the underlying rendering engine of the browser. Sure but they do make the browser different and will continue to be actively avoided.<AyrA_ch>: &gt; I am not sure how much differences there are now but I am sure people won't do that either. A number of extensions available on firefox and chrome would probably disagree with you. &gt; Sure but they do make the browser different and will continue to be actively avoided. I doubt it. The browser itself behaves the same by using the same engine. Since Chromium draws its own window borders it looks similar on all operating systems anyway.", "num_messages": 20, "avg_score": 195.8}
{"start_date": "1543629260", "end_date": "1543690526", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1755, "text": "<RobertVandenberg>: Hacker hijacks 50,000 printers with PRET to tell people to subscribe to PewDiePie <Fer2409>: Really?<midairfistfight>: Yes! I miss fun hacks. These days hackers are just annoyingly condescending about bad security and spend their days selling bugs or wrangling botnets.<zagginllaykcuf>: It's not a hack and hackers have always been like that. Nice try ultra normie<sarcasticpool>: Hackers haven't been \"always\" like that. Hacking was more of a \"just for fun\" thing when the Internet was in its initial phases. Hackers used to \"joyride\" other systems taking advantage of Dial-Up connections and boast.<zagginllaykcuf>: Lmao you sound like you're 12 and just saw a free 90's hacking movie on basic cable. \"Joyriding dialup\" Jesus christ dude lol<sarcasticpool>: A 19 year old student who has Cyber Security as a subject in his college. <zagginllaykcuf>: Oh yeah? \"Cyber security\" as a \"subject\" in college? You really sound 12 or like a high school drop out, no freshman majoring in CS especially security talks like that mate lol <sarcasticpool>: I'm not Majoring in Cyber Security. I took it as an Elective Subject. And one does not have to Major in Cyber Security to know the history of Internet. :)<zagginllaykcuf>: Yeah sounds like you don't know either or have ever been to college. You take college by choice every class is an elected subject lmao<Compiled_>: Looking at your comment history all I can see is that you're one angsty entitled individual...<zagginllaykcuf>: All you proved is that you're a creep ass and not even good at it. Read my /u/ backwards and you'll figure out it's a throwaway. Good job faggot lol", "num_messages": 12, "avg_score": 146.25}
{"start_date": "1544667115", "end_date": "1544723578", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 49, "text": "<CallMeMalice>: Extending a language with reader macros: a subset of C syntax as a native code in Common Lisp. <CallMeMalice>: I am not the creator. This library implements a subset of C language and makes it a part of the Common Lisp language - you can write code in C, compile it with lisp that loaded this library and it will work just fine! Moreover, you can call Common Lisp functions from within the C code too! It is made possible by a Common lisp feature called reader macros. Those macros allow you to \"reprogram\" your compiler by adding new parts to the parsing - you translate the code of C into lisp code and compiler compiles the modified code. <combinatorylogic>: &gt; It is made possible by a Common lisp feature called reader macros And in a Lisp without reader macros, you can easily add them by using the regular Lisp macros and replacing the parser altogether. It's mind-blowing what you can do with *any* language that's only got proper compile-time macros - you can literally turn any such language into any other language. Or mix any number of languages together. I've been playing with this sort of things a bit. E.g., a [C compiler](https://github.com/combinatorylogic/clike) (with a native back-end) embedded into Lisp, with a fall-back to Lisp available for defining C macros. Those macros on top of Lisp can be used to extend C in some crazy ways, like, for example, [embedding Verilog](https://github.com/combinatorylogic/soc) (in a way quite similar to the `asm` statement) into C. Mixing languages is not a trinket, it's among the most practicat things you can do - it opens a way to do a full scale language-oriented programming, by embedding and mixing eDSLs.<sinedpick>: &gt;And in a Lisp without reader macros, you can easily add them by using the regular Lisp macros and replacing the parser altogether. I don't buy it. Can you show a simple example? <combinatorylogic>: You can take a look at the links provided - everything is built on top of a very dumb Lisp without any fancy reader macros, the parser was replaced altogether with a PEG-based one instead (i.e., you have to change your REPL top level, and all those `load` implementations). Another example - Racket. No explicit reader macros, but languages are still easy to switch.<sinedpick>: I'd already pored through both those links and couldn't find an example of reader macros being implemented without reader macros. Can't you give me a link to a file, or even something to look up where I can see it clearly? I'm just not sure how you can claim that reader macros can be implemented with non-reader macros because reader macros run before any non-reader macro has a chance to run.<combinatorylogic>: &gt; I'm just not sure how you can claim that reader macros can be implemented with non-reader macros because reader macros run before any non-reader macro has a chance to run. Easy - if you [replace](https://github.com/combinatorylogic/mbase/tree/master/src/l/lib/pfront) your REPL or compiler toplevel and your `load` (or whatever else you use to include other source files) macro, you can insert your own parser, which, in turn, can support reader macros, or an extensible syntax in general. The only downside of this approach is that you have to [re-implement](https://github.com/combinatorylogic/mbase/blob/master/src/l/lib/pfront/sexp.hl) the original parser as well.", "num_messages": 7, "avg_score": 7.0}
{"start_date": "1544695891", "end_date": "1544771310", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 6, "text": "<TrepidEd0601>: Planning to start a stream for people who want to code together! (I hope this is appropriate for this page) <Kyrolike>: Probably not going to be able to catch the stream due to being at work during your proposed stream time but if I happen to have some time and the stream is online I'll drop by :)<TrepidEd0601>: Thanks! :) I'm looking forward to meeting(?) you!!<Kyrolike>: Looking forward as well. Btw as someone who is really interested in Japan myself, were you born there or did you move there for work/other reasons?<TrepidEd0601>: I actually did grow up in Japan, but I spent some time abroad when I was in elementary school, so I was lucky enough to pick up English :)<Kyrolike>: Yeah I was wondering why your english is so good. That's usually not the case for japanese people :D", "num_messages": 6, "avg_score": 1.0}
{"start_date": "1543488613", "end_date": "1543526377", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 9420, "text": "<RobertVandenberg>: eBay Japan source leak as .git folder deployed to production <davorzdralo>: &gt; allowed me to download the entire source code of www.ebay.co.jp including database passwords These motherfuckers committed DB passwords into git. I don't do this even for joke projects.<Skarsnik101>: I release an azure password in github, someone sent me an email letting me know i fucked up. It was for my home project using blobs but anyway.... He was a legend. <johnyma22>: Microsoft committed their sauce labs password to github and when I reported to their security team they told me it wasn't a concern or problem. I shrugged and vowed never to bother helping Microsoft again. Ebay have a very small window to compensate this guy before the infosec community shun them.<brtt3000>: If the database isn't accessible from the internet then leaking the password is sloppy but not disastrous.<robertcrowther>: Depends on whether the password or style of password is reused elsewhere.<Vakz>: Password reuse for production use should be a \"pack up your shit and go home. Don't call us for a reference\" offense. It shouldn't even be possible to reuse the style, because the style should be a very long random string.<jesuscrysis>: Too much software enforces a style as part of password requirements. Even my bank requires a password that has a minimum and maximum length and types of characters. Such a goddamn annoyance. Worst part is that they only allow using an on-screen keyboard and block paste operations.<snarfy>: And those requirements do absolutely nothing except make you have a harder time remembering it. We don't use 7 bit DES based crypt from the 70's anymore. They don't even [know why](https://security.stackexchange.com/questions/33470/what-technical-reasons-are-there-to-have-low-maximum-password-lengths) those requirements ever came about. <Irregular_Person>: The link you posted (at least the top answer) doesn't support what you're trying to say. That would be an example of why some passwords can't be *more* than 8 characters. Password rules *force* your password to be harder to brute force, at the expense of making it slightly easier to break that same password without the requirements (the requirements actually reduce the possible \"guesses\" to those that match the requirements)<axonxorz>: But if the passwords do not require extra complexity (eg: uppercase+lowercase+numbers+symbols), it doesn't matter. An attacker doesn't _know_ that your password only contains \"simple\" characters, they have to try all combinations. It is monumentally harder to break a longer password, than a shorter, more \"complex\" password. Also, it's been my experience that users will do the absolute bare minimum to comply with password update requirements. Your password is **J4mesFrank0//** today? Next update it will be **J4mesFrank01//**, then **J4mesFrank02//**, etc <edit>: spelling<Lafreakshow>: I used to go for ten random character to avoid common passwords but have since learned that longer is better. So now I use A couple short words with random numbers, special character and letters added between them while still avoiding common passwords. It's way longer AND easier to remember.<AnorakJimi>: I just use a 10 word sentence. Passwords that long are basically impossible to crack with current computer power and it's easier to remember a sentence than a lot of random characters. <Irregular_Person>: I wouldn't go quite that far... You could imagine (functionally) treating words as characters, and then limiting the combinations by some min/max combined length - that reduces the dataset from an assumed brute force approach significantly. the number of word combinations that add up to 8-20 characters is a big dataset, but much smaller than the possible combinations of 20 random characters. Add in common number patterns and names as 'words' and that's closer to how some password crackers work. Some even do common variations like 0 instead of o, l33tspeak etc<meekydeaky>: If you use randomly generated passwords, passphrases aren't better. But if you want easier-to-remember as a feature, passphrases are much, much better, and because there are so many words, even short phrases of four or five words are hard to crack, even if the attacker knows you're using dictionary words. A simple four word phrase with a pool of 10,000 words gets you 10 quadrillion possible combinations. To get a similar number of combinations with characters (I'm assuming a pool of 70) you need 8-9 characters. I'd say remembering four random words is much easier than remembering 8 random characters. And once you start using words that are not found in a dictionary, the attacker has pretty much no chance. Use made-up, misspelled, l33tsp34k'd or foreign words (even just one) and the attacker has to brute force through every combination of characters. A short passphrase of 30 characters now has over 870e40 combinations. The only downside of passphrases is typing them takes longer.<Irregular_Person>: I don't disagree at all, I was pointing out that \"basically impossible\" was a stretch with a *simple* sentence. Note that I did mention common variations like l33t, i've used crackers in the past that were capable of applying those variations to the supplied dictionary. My personal favorite for reasonable passwords I need to remember is to pick a memorized phrase and pull the first letters. Sprinkle in caps for beginnings of sentences or phrases if you're feeling saucy. &gt; Two roads diverged in a wood, and I\u2014 &gt;I took the one less traveled by, &gt;And that has made all the difference. **TrdiawaiIttoltbAthmatd** slower to type than a phrase, but if it gets used enough that it would be an issue, you will have memorized the letters anyway out of habit. **S123tmhacwmBylsfairwmym** &gt;So 1, 2, 3, take my hand and come with me &gt; Because you look so fine &gt;And I really wanna make you mine", "num_messages": 16, "avg_score": 588.75}
{"start_date": "1541215046", "end_date": "1541276550", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 4657, "text": "<monica_b1998>: Python is becoming the world\u2019s most popular coding language <Foodei>: IMO python is the best and most elegant language to learn/teach cs concepts. It is not the end of all you need to learn if you want to be effective in the field.<combinatorylogic>: &gt; IMO python is the best and most elegant language to learn/teach cs concepts. No way. The very design of the language won't teach you anything useful, quite the opposite, it'll just leave you full of mythical thinking. A language with totally broken, illogical scoping rules is a poor choice for teaching. Also, how can anyone call this crippled language \"elegant\"?!?<Foodei>: Ok then I recommend Scratch for you...enjoy.<combinatorylogic>: And another incompetent one. Behold, people, this is exactly what Python does to the education. Idiots like this one who somehow dare to believe that they know anything at all about programming.<TyrSniper>: Jesus Christ dude I see you all throughout this thread being a rude asshole. How can't you see you are an elitist dick? I created a notification system this week with a co-worker for our CICD system in Python. It took us an hour, we have multiple data processer and rest APIs, fully autogenerated documentation and swagger servers that's running in production. Please lecture me in what I'd ought to have done. <combinatorylogic>: So, you're praising the existing libraries and boilerplate, not the language. You just happened to have a problem that hundreds of people solved already. It's an entirely uninteresting use case.<TyrSniper>: You mean what the entire industry does all day every day? Is that where your hangups are actually at?<combinatorylogic>: &gt; You mean what the entire industry does all day every day? ROTFL, a puny web code monkey dares to speak for \"the entire industry\". Get out of your little stupid bubble. Web is the shittiest part of the industry. Also, only a complete idiot would waste time solving an already solved problem. Therefore the ecosystem argument is just retarded.<TyrSniper>: The fact that you won't actually answer any straightforward question elsewhere in the thread shows you might be deluded on some level. I will literally sing your praises if you can answer this question legitimately. You join a company, they need to track hours of employees and they want them to log time in a website. How would you consider implementing? (Make up the details, don't get stuck up on that). Tell them they are a shit company for not having solved this problem and quit? Just tell them they can buy a product that does it, avoiding my question? C'mon dude, are you seriously dense or just misunderstood by all of us in these subs. And the last time I asked this, someone avoided it by suggesting they were doing my work for me, which is hilarious and now old, so if you are going to avoid it, come up with something else. &amp;#x200B; &amp;#x200B;<combinatorylogic>: This is a *solved* problem. There are literally dozens of ready to use products that do this thing exactly. Why are you going to waste your time solving an already solved problem?!?<TyrSniper>: &gt; Just tell them they can buy a product that does it, avoiding my question? Cool, so I called that one at least. So let's make up an example, they ask you to build something that makes a decision based on the temperature. Hey /u/combinatorylogic, we want to raise a flag when the temperature gets too hot here. We already have the servos and a board with everything hooked up, here's an API library. Also we want to configure it through a website. Better example? Or do you want to miss the point even further and I'll come up with a new one?<combinatorylogic>: Now I really fail to see how you're going to fit your stupid Python into such a primitive task.<TyrSniper>: Holy shit, yeah you are definitely a troll. Alright, we had a good run and I'll admit you got me a little heated with the brick wall routine. Cheers friend, have a great life.<combinatorylogic>: &gt; Holy shit, yeah you are definitely a troll. This is exactly a symptom of your severe mental retardation. You're so dumb, so ignorant, so uneducated that you even failed to comprehend a single argument I provided here. Of course you prefer to think it's all trolling now, otherwise you'll have to accept that you're hopelessly stupid. <mtcoope>: I hope you are a troll or this is only how you act online. If this is you off the internet, you would be miserable to work with.<combinatorylogic>: Luckily, I never work with idiots. In fact, it's a very useful approach - if you do not tolerate idiots, they will stay away.<mtcoope>: Honestly I think you are a smart person so I wont question your intelligence. I just think you are arrogant and unfortunately almost all products, inventions, and innovations rely on teams and not just smart individuals. It will be a tough life for you if you are always like this. I might be an idiot but at least I am not hated by everyone I come in contact with. I'll take that life any day.<combinatorylogic>: It's not unrealistic to build even very large teams that are free from idiots, actually.<mtcoope>: Because most people are not idiots, most are average. That is how it works. A group of average individuals that know how to work with people are be a team will outperform a group of arrogant geniuses that cant agree on anything any day of the week. <combinatorylogic>: &gt; Because most people are not idiots, most are average. Exactly. So if you put your filter as \"at least average\", you won't see the idiots. Unfortunately, internet in general and this sub in particular does not implement such a filter.", "num_messages": 21, "avg_score": 221.7619047619}
{"start_date": "1543566818", "end_date": "1543587242", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 7, "text": "<mehdifarsi>: modules in Ruby: Part I <shevegen>: It is not really a \"namespace\" though; and a class can be used just as well instead (but module is better because you can use \"include Bla\"; and define \"#new\" on it if you want to anyway). I'd still like to see a variant of ruby without the module versus class distinction. Obviously this variant would need some way to provide what modules (and classes) do, too. That is not really a \"real\" complaint, though. My only real complaint is that I have to specify class or module, and ruby will complain if I use the wrong term. So: class Foo::Bar::Bla or more commonly for me: module Foo class Bar class Bar I have to specify each variant; and if I use the wrong one, ruby cries afoul. That sucks. That should not happen. I am not SPECIFYING something here IF another file already specified whether it is a module or class. I understand that this exists to remove confusion and ambiguity but still - if we would not have that distinction between module and class, we would not have that error-situation in the first place. The first variant of course is shorter, and you can argue that it is preferred, but it is not 100% the same. For example, I have errors when I subclass without specifying the parent class properly, in the first variant, since self is not known at that point. That is one reason why I prefer the more verbose second variant often. (I don't indent like that though; I indent \"incorrectly\" since I only want to have one indent for inner-classes and modules, no matter how many of them exist. I don't want to indent too many times to the right since it shifts the focus without giving me anything I really need - I operate within the \"middle\" of the .rb file, not within the pre-set indent level that would push me to the right side of a screen. Within a method, of course, I indent logically again, at all times, without exception. Only not at toplevel 100% of the time if there would be \"too much\" indent.)<myringotomy>: Modules and not classes and I don't see why they should be. There are many things in life which are \"like\" something else but not the same thing. Mandarins are like oranges but they are not the same thing. I am fine with having similar but slightly different concepts in a language as long as they both provide needed functionality.<sammymammy2>: Aiming for orthogonality is a worthy goal", "num_messages": 4, "avg_score": 1.75}
{"start_date": "1544542527", "end_date": "1544577347", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1349, "text": "<Shadowvines>: Australia's new encryption laws ensure companies can't hire AU developers or tech solutions. <coladict>: They're giving almost all their agencies the power to get your formerly private information except... &gt;However the government amendments removed the various anti-corruption bodies from this category. It's not clear why. Gee, I wonder why...<Shadowvines>: ya its pretty friggin nuts. Atlassian is an Australian company. The following scenario is possible. Country A want to get information on an individual who uses your platform. They contact the local police department in Sydney to go to the house of a developer for Atlassian to compromise your BitBucket account. They then alter the code to introduce logging to an AU server. You won't know it happened, Atlassian won't know it happened and now your platform is compromised and your not even the target. They don't even need a specific warrant for this. They can just have a warrant that broadly gives them the right to investigate the individual who uses your platform nothing else required any judicial oversight.<MakinThingsDoStuff>: What if the developer just keeps saying they don't know how?<Shadowvines>: may work. Actually, it would be great if someone more knowledgeable than me would pipe in on a script for what to do if you are approached.<alphaglosined>: You need lawyers for that. But I suspect it should include some way to verify that it is a legal request.<Glader_BoomaNation>: I think the law stated you can't tell anyone about the request. That means a company's legal team is not going to be in the picture.<shevegen>: I consider any law that forbids you from speaking about anything to be illegal.<414RequestURITooLong>: So... would you consider a law that forbids public officials from selling state secrets (or your private information, or...) to be \"illegal\"? Is attorney-client privilege \"illegal\"? What about HIPAA? The GDPR?<asbananasasyousay>: At least in those cases you can say things \"I can't tell you whether Jane Doe is here or not because that would violate HIPAA, assuming she exists\" Completely gagging someone from mentioning that they got a gag order is fucked up", "num_messages": 10, "avg_score": 134.9}
{"start_date": "1544218773", "end_date": "1544287251", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 53, "text": "<balazshoranyi>: PyTorch 1.0 released (stable) <13steinj>: I don't understand their reasoning about the C++ API. I mean, I want it. And it should be done. But to make a C++ API modeled after the Python one meant as an analogue is strange. It should be similar and as close as possible, so thag eventually the Python API will simply be just a thin wrapper around the C++ one.<nucLeaRStarcraft>: This is due fast prototyping nature of Python and PyTorch specifially. This is their main advantage over say Tensorflow. Writing experimental code, that is relatively fast (faster/similar to tensorflow most of the time), that is easy to debug is very straightforward. Then, the C++ component comes after one is done prototyping, and she can port the model to native code for production, using pre-trained weights for networks, without having to wonder about slight differences between them. I find it really good that they went this route, tbh the FB/PyTorch developers have had very good decisions so far, which is why they managed to get into this Keras/TF dominated area so fast and the adoption is only growing.", "num_messages": 3, "avg_score": 17.6666666667}
{"start_date": "1544379831", "end_date": "1546049124", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 6011, "text": "<chickensaresexy>: Why Software Developers Are Paid 5x More in The USA <khedoros>: If I were paid 20% of my most recent software development salary, it would be well into poverty-level pay in my area, and I'd make more money working at a McDonalds.<RonaldHarding>: One thing the video makes no mention of is cost of living. It's important to note that pretty much all of these high paying software developer positions in the United States exist in tech hub cities on the coast. Cost of living is exceedingly high in these locations. To the point that people who don't live in these locations balk when they find out what it actually takes to live there. It's actually economical to take a 20%-30% pay cut to work for a smaller company outside of these areas in many cases. Now I can't be totally sure, but I suspect software developers are a bit more spread out in the rest of the world. Without the centralizing tech giants to pull everyone into one place there's a lot more likelihood that jobs could be available across a region. Anyone outside the US want to help me determine if that's the case?<matthieum>: That's not been my experience. In Europe, there's a huge concentration of software developers around the Western capitals: London, Paris, Amsterdam. As a French, there *are* software development opportunities outside of Paris. I worked close to Nice for 9 years, for example. There were *1* big software company there. Hemorrhaging developers because... there's not much else to do apart from tourism/care for retirees in Nice, so their spouses could not find employment. I sometimes find it ironic that as one of the fields in which remote working is the easiest, it seems also to be one of the most concentrated. There *are* software jobs everywhere, but personally I'm not interesting in building an n-th mom'n'pop website, or \"internal\" CRUD applications. Those don't require an engineering degree to pull off (although it's often a pre-requisite), so they're not stimulating enough. And the stimulating jobs, cutting edge stuff? Well: London, Paris, Amsterdam that I know of.<8483>: &gt; \"internal\" CRUD applications. This is like 99% of things built.<oneonetwooneonetwo>: And they make the world go round. I always think it's like a welder thinking they're too good for pipes.<8483>: It's pretty funny because all of the \"attention\" is on the fancy technologies, when in reality, the most useful thing one could learn is SQL and database design.<MotorAdhesive4>: A good load balancer in hand is worth two blockchains in the bush.<8483>: The majority of web apps don't even come close to needing a load balancer.<MotorAdhesive4>: Even more web apps don't need a blockchain.<gintonicisntwater>: There are very few good use cases for an extremely slow distributed write only database.<Iamonreddit>: You can also get an actually fast write only database with Amazon now too<underStranix>: That pretty much defeats the whole purpose of block chain then doesn't it ? <Iamonreddit>: Unless you *really* want decentralised authority, yes.<underStranix>: But the point is somewhere along the line you need to trust some entity and that itself makes the whole thing pointless cuz you can just let them handle everything since you already trust them . Like for identity, how do you register when someone new is born on the block chain? Do you let individuals do it because then people can make fake identities, so you trust a source to do it (birth certificates) but you trust them so why not just let her handle everything and make it 1000x less expensive <Iamonreddit>: I think you've missed the part where we are very much in agreement", "num_messages": 16, "avg_score": 375.6875}
{"start_date": "1543525425", "end_date": "1543683368", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 19, "text": "<jmhummel>: \"Actually, callbacks are fine\" - Implementing Monads using Callbacks <clockdivide55>: I am completely lost on _why_ someone wants to write purely functional code like this. What does it express better than procedural-like code? const userVerification = await verifyUser(username, password); const rolesRetrieval = await getRoles(username); const logEntry = await logAccess(username); I think this post is in reference to a post that I haven't read that _does_ use promises, but I've seen code written in a function style in both C# and Javascript that is way, way clearer when written in a more top-down procedural style. Sell me on functional programming, there's a lot of stuff I like about it but this isn't it.<dinkandenza>: Hello there, author of the post here. IMHO the question is backwards; what does the language feature you've illustrated express better than just using functions? Is it really paying for itself, when we consider that it has to be specified in the language, implemented in browsers and transpilers, and requires duplication of the API surface of existing asynchronous libraries? For this cost we get an ill-typed and weak abstraction that is difficult to abstract over further. I'll give you an example. If I have an list of continuations of things, I can turn it inside out into a continuation of a list of things. In other words, I could have implemented `verifyUser` as: const verifyUser = username =&gt; password =&gt; Arr.sequence(Cont)([ dbVerifyUser(username)(password), dbGetRoles(username), dbLogAccess(username) ]); which, when fed a callback, will feed it an error or `[userInfo, role, undefined]`. Now you may be familiar with an analogous function called `Promise.all`, which given a list of promises of things, produces a promise of a list of things. What's the difference? `Promise.all` had to be implemented specifically for promises. The `Arr.sequence` function above actually knows nothing about asynchronicity or continuations, it is a general purpose tool that can take a list of some monadic values and give you a monadic list of values. It works just as well for taking a list of errors or values, and producing an error or a list of values, etc. Generalizing in the other direction, *all* traversable containers (such as trees, dictionaries, sets) will work perfectly with continuations, without having any awareness of them. For promises, once again, you must sit down and write: `Promise.allObj` or `Promise.allTree`, or go find a library on npm that's implemented this. It all works nicely together, and I don't need to think about it or implement common things over and over myself. If you're interested I can give you lots more examples, such as adding adding retry logic, managing state, error handling, etc. All of these cross-cutting concerns are easy to pull out into an abstraction when you're working with pure, referentially transparent code that isolates its side effects. Not so when side effects are happening willy nilly throughout your code. <PS>: Leaving this subjective question aside, it might be surprising to you to learn that the \"procedural\" code you posted is actually \\*also\\* monadic, except it's do-notation for the Promise (almost)-monad. In a language with general purpose do-notation for arbitrary monads, the continuation monad approach looks almost identical: verifyUser username password = do userInfo &lt;- dbVerifyUser username password roles &lt;- dbGetRoles username _ &lt;- dbLogAccess username pure (userInfo, roles)<ivquatch>: &gt; The Arr.sequence function above actually knows nothing about asynchronicity or continuations, it is a general purpose tool that can take a list of some monadic values and give you a monadic list of values. The reason i think this argument falls flat with javascript programmers is because there are no typeclassish interfaces out-of-the box. There are arrays (list monad) and promises (continuation monad kinda), but there's no common monad interface as you pointed out. Also there are no Option, Either, or IO types. I'm sure someone could (and probably has) written an npm module to monkey-patch typeclass interfaces like Monad, Applicative, and Monoid onto common JS types, but AFAIK such a thing is not ubiquitous in the ecosystem the way async/await or promises are. So while combinators like `Arr.sequence` may sound great in theory, what can JS programmers use them on right out-of-the-box without introducing another obscure library dependency? Plus culturally, I think most JS programmers would stick their fingers in their ears once you got up on your soap-box and started extolling the glory of category theory, even if you deliberately try to avoid mentioning the \"M\" word. Things like `Arr.sequence` are so abstract, that you can't really justify them wrt to more familiar things like async/await without going deeper down the FP rabbit hole. And by then, you're in space-burrito land. Good article, btw. I realize I'm splashing cold water on your efforts like a lazy internet troll. People may eventually warm up to these kinds of arguments as FP idioms become more common in Javascript. There's just a little bit of a barrier that FP evangelists have yet to overcome. <dinkandenza>: Hi @ivquatch. I've been using typeclasses and instances quite heavily in JS, and haven't experienced the need to monkey patch anything yet. In fact, IMO JS is actually uniquely well suited to the challenge of equipping existing data with new behavior, without any modification or monkey patching. The bags of functions that typeclass instances in other languages eventually desugar to are quite convenient to work with directly in JS. Here's an example: ``` // :: type Monoid m = { empty: m, append: m -&gt; m -&gt; m } // Some useful monoids // :: Monoid Number const Sum = { empty: 0, append: x =&gt; y =&gt; x + y }; // :: Monoid Number const Product = { empty: 1, append: x =&gt; y =&gt; x * y }; // :: Monoid Number const Min = { empty: Infinity, append: x =&gt; y =&gt; Math.min(x, y) }; // :: Monoid Number const Max = { empty: -Infinity, append: x =&gt; y =&gt; Math.max(x, y) }; const Arr = (() =&gt; { // :: Monoid m -&gt; (a -&gt; m) -&gt; [a] -&gt; m const foldMap = M =&gt; f =&gt; xs =&gt; xs.reduce((p, c) =&gt; M.append(p)(f(c)), M.empty); // :: Monoid m -&gt; [m] -&gt; m const fold = M =&gt; foldMap(M)(x =&gt; x); return { foldMap, fold }; })(); const input = [1, 2, 3, 4, 5]; console.log([ Arr.fold(Sum)(input), // =&gt; 15 Arr.fold(Product)(input), // =&gt; 120 Arr.fold(Min)(input), // =&gt; 1 Arr.fold(Max)(input) // =&gt; 5 ]); ``` This doesn't have any dependencies, and I'd be quite comfortable pasting something like this into a util.js file without worrying too much that an average JS developer will find it impossible to decipher the black magic underlying its workings. The unfamiliar jargon of course is somewhat offputting, which is a valid criticism. I suggest however that this is true of any new and unfamiliar design pattern to some extent.<ivquatch>: Interesting style. I'm going to steal this! The extra `utils.js` file I guess is the \"obscure library dependency\" I was referring to. It's relatively straight forward, but once you start adding `Sum`, `Product`, etc... for strings and arrays, you have to admit it'll start to grow. Also, you'll probably start getting questions like \"why should I use `Arr.fold(Sum)` over strings when `.concat()` is so much more efficient?\" (although i suppose you could use some kind of string builder type as \"zero\", it might make things awkward)", "num_messages": 6, "avg_score": 3.1666666667}
{"start_date": "1544685634", "end_date": "1545028405", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 58, "text": "<kodbraker>: Free Hotel Wifi With Python And Selenium <zsolt691>: Nice write up. For this scenario I there is an alternate solution using tunnels. Might check it out next time something like this happens :) [XFLTReaT](https://github.com/earthquake/XFLTReaT) is a good tool for example.<rickspam>: How would tunneling through a protocol, change your mac address?<zsolt691>: I meant not for the mac changing part. Some hotels/airport wifi's will let some kind of packets through (eg.: ICMP). In that case can you use this. In OP's scenario it depends how the hotel configured their rules.", "num_messages": 4, "avg_score": 14.5}
{"start_date": "1544679238", "end_date": "1544722681", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 79, "text": "<daflyinj>: A tale of 132 e's <TheGift_RGB>: god, how can someone be so fucking full of themselves what a useless post by an useless person about an useless topic, just uselessness all around, literally the sort of person that could die without any negative impact on the world<MikeFightsBears>: Whereas, judging by your post history, someone like you would die and it would be a positive impact on the world.<TheGift_RGB>: Confrontation and aggression are how your immune system works to protect you against pathogens. You should thank people like me and /u/combinatorylogic for consistently calling out shit spam like this.<MikeFightsBears>: You realize that you're the organism spreading disease, right?<TheGift_RGB>: No, I'm the white blood cell working to protect this subreddit from medium.com and other similar diseases.<Dgc2002>: Actually, we're the white blood cells downvoting your toxic bullshit comments.<TheGift_RGB>: Not understanding that you're a pathogen doesn't make you a white blood cell, my clueless little lovely.", "num_messages": 8, "avg_score": 9.875}
{"start_date": "1544764770", "end_date": "1545085829", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1172, "text": "<justinmeiners>: Write your Own Virtual Machine <dazzawazza>: Writing a virtual CPU and a compiler that takes C and targets your virtual PC is a great summer project. I did it on my Amiga too many years ago now. Every programmer should do it at least once.<AttackOfTheThumbs>: Seems like a lot of wasted time.<combinatorylogic>: No you tool, it's just a few hours work, resulting in a very important fundamental understanding.<AttackOfTheThumbs>: Disagree. 99% of programmers simply don't need this.<combinatorylogic>: 100% of programmers absolutely need this, and those who do not agree are simply dumb, ignorant and very inefficient. Every programmer, no matter what they're doing, must implement and use DSLs. And those primitive toy compilers are among the best ways of learning how to do it. <AttackOfTheThumbs>: These are broad reaching statements, and insults. Congratulations.<combinatorylogic>: These are facts. If the objective reality is insulting to you, you can always quit.<AttackOfTheThumbs>: No, you are confused. Disregarding that your opinion isn't fact, you directly insulted me. &gt; 100% of programmers absolutely need this, and those who do not agree are simply dumb, ignorant and very inefficient. That's a direct insult. It doesn't really matter though. I'm all for furthering your own education in the field of programming, but saying that this is fundamental is a joke. It simply isn't. There's plenty of programming fields where the dev won't benefit from exploring this avenue. They'll just end up wasting their time. Those who do not agree are simply dumb, ignorant, and very inefficient. I'm done with your fallacies. Bye.<combinatorylogic>: &gt; Disregarding that your opinion isn't fact, you directly insulted me. This is not an \"opinion\", this is a fact. If you do not agree, it's just a sign of your gross incompetence. Go away, kiddie, and come back when you learn how to program. Adults are talking here. &gt; but saying that this is fundamental is a joke You're incompetent. Stay away from programming until you learn at least the most basic things. &gt; There's plenty of programming fields where the dev won't benefit from exploring this avenue. You're dumb and ignorant. There is no single field in programming where using eDSLs does not bring enormous benefits. If you believe that in your domain that's not the case, you're inefficient, and you should stay away from it. Shit like you shall never be allowed anywhere near any code.<AttackOfTheThumbs>: You truly are an awful human being. I hope you don't need to interact with real people. I would pity them.<combinatorylogic>: So, pointing out a staggering ignorance of someone who dares to pretend to be a \"professional\" is somehow \"awful\" now? Really? In fact, it's you who should be ashamed of yourself. You're ignorant and dumb, and yet, you have a nerve to voice an utterly disgusting, evidently false opinion in public.", "num_messages": 12, "avg_score": 97.6666666667}
{"start_date": "1544775901", "end_date": "1544803854", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 14, "text": "<meysholdt>: Software development should be more like eating at a restaurant <davidk01>: Something about this analogy doesn't add up. The charitable interpretation in my opinion seems to say something about curated development environments optimized for a specific workflow. I'm in favor of this and don't see much wrong with it because most developers after a while develop templates for projects that they reuse. That's kinda like going to a restaurant. The part where it breaks down I think is when the analogy is taken to its logical conclusion. In the limit we get managed services and serverless and this endpoint is actually more work than cooking at home. It's unclear to me which interpretation the author was going for.<F-0X>: &gt; I'm in favor of this and don't see much wrong with it because most developers after a while develop templates for projects that they reuse. That's kinda like going to a restaurant. I don't really agree. My home-cooked templates are probably not on the restaurant's menu, and the best I could achieve is being a pain to the chef and asking for a bunch of substitutions he may not necessarily be prepared for and in the end I get a meal I don't like for expensive when I could have had something much better and cheaper at home.. then I take 5 minutes to put everything in the dishwasher. I think the analogy is ridiculous in the first place though. If a developer were in a restaurant, the only fitting role is a cook.", "num_messages": 3, "avg_score": 4.6666666667}
{"start_date": "1543410049", "end_date": "1543426443", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 1054, "text": "<janvt>: TIL about Mermaid, the \"markdown of diagrams\". Allows you to generate diagrams while keeping them cleanly under source control! <eggn00dles>: why would i use this as opposed to something as simple, versatile and free as lucidchart?<howdhellshouldiknow>: Are you talking about this https://www.lucidchart.com/users/registerLevel ? It is not free/self-hosted/runs-in-the-browser.<eggn00dles>: &gt; Not convinced yet? Explore a free account now for an unlimited amount of time\u2014no credit card required. ...<iamagiantnerd>: The free level of lucid chart isn't very useful. Sure, you can explore the functionality, but you quickly run into limits on the number of documents you can have (3 I think?).<eggn00dles>: I haven't run into any limits, i have about 15 charts on there, all exported to .pdf. My experiences reading programmatically generated documentation is akin to the 9th circle of hell. Can that thing do Venn diagrams? This library just seems like the hard way of doing things that have already been done in a much more practical way.<iamagiantnerd>: Weird, maybe you have some old account and you are grandfathered in? <https>://lucidchart.zendesk.com/hc/en-us/articles/207300296-Account-Types 3 document limit (that's what I found to be true on my free account). <eggn00dles>: I created this account 3 months ago. Whatever, I'm getting downvoted for correcting misinformation at this point, this thread is too stupid to participate in anymore.<iamagiantnerd>: Sorry you feel that this thread is stupid; I wasn't trying to provide misinformation, just relaying my own personal experience. I like lucid chart, just quickly ran into the limitations on (my) free account. <eggn00dles>: I didn't think you were intentionally doing it, yet still, Getting downvoted for pointing people to free alternatives with additional capabilities makes you wonder.", "num_messages": 10, "avg_score": 105.4}
{"start_date": "1542834380", "end_date": "1542909040", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 3060, "text": "<trot-trot>: Linus Torvalds: After big Linux performance hit, Spectre v2 patch needs curbs -- \"Patch is causing as much as a 50 percent drop in performance in some Linux workloads.\" <flexmuzik>: Wouldn't this be expected for a patch that has to circumvent issues with branch prediction, one of the most effective performance optimizations that exist?<BossOfTheGame>: It seems that we have discovered something of a performance / security trade off. It seems to me like there ought to be an option to configure is you'd like fast execution with branch prediction, or you care about security. The context would indicate what the appropriate setting should be (e.g. offline performance computing / security critical servers)<deaddodo>: They can be disabled. However, any cloud (or any shared server, really) service doing so would be introducing a massive security hole. If you're running your own baremetal server with software of your curation running, have at it.<moonsun1987>: Not really. As long as it is a single purpose server and you trust all applications running at that time, you should be ok I think. <bezerker03>: That's exactly what he said. It's the cloud providers that will feel this. <moonsun1987>: How does a cloud provider feel anything? They'll just turn the patches on for shared servers. 1 vCPU is still 1 vCPU. This gives them an option to sell dedicated servers or dedicated time slots on shared servers. It is an opportunity. Like the protagonist says in the pilot in mad men, this affects every Intel server out there. No one can use it against you. <bezerker03>: When meltdown patches launched they enabled the features. It destroyed paravirt instances performance. Epic games has a good write up on a post mortem of what happened. Additionally, aws for example ran into performance issues and needed to add capacity to deliver the same level of performance. Each host became unable to keep up with capacity currently provisioned and there was a shortage of availability for a period of time. <moonsun1987>: But so does everyone else. You don't lose any competitive advantage.<bezerker03>: I'm not sure about that. If that one vcpu is performing certain operations more slowly to comply with security, you need to scale up. Since everyone does , and there may be a supply shortage, you may not be able to scale up. <moonsun1987>: Ah. I think I understand now. It is hard to wrap my head around the scale of operations of AWS. Thank you for bearing with me. <Edit>: Let me see if I understand. I was thinking in terms of well who cares if 1 vCPU after patch does 70% of 1 vCPU before patch if you're selling access to it at the same price but at the end of the day the cloud isn't magic and you have to provision all of them on a machine somewhere and it is not easy to say \"we over provisioned. You'll be down for an hour while we get our house back in order.\" And even if you do, your costs have increased because you need some specific hardware and probably like a 10k units of those and just because you want it the prices have moved up... which sucks if you're on razor thin margins in the first place.", "num_messages": 11, "avg_score": 278.1818181818}
{"start_date": "1544762350", "end_date": "1544801840", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 45, "text": "<NevilleDNZ>: Kevlin Henney - Procedural Programming: It's Back? It Never Went Away - Algol68/LISP/Simula/AWK/C/Groovy/Hamlet <patrixxxx>: Not having watched the video I'd say procedural and functional is the way forward. Objects is a nice concept, but why attach state modifications onto them? Leave that to functions that take objects as parameters. I know this approach can create issues as well and sometimes OOP can be very elegant but I think this is the best general approach and there are no silver bullets.<bcgoss>: You're right that there are no silver bullets. As to why objects modify state, it's usually useful to focus on message passing. If every object has a well defined interface, then other objects can pass messages that describe the goal without caring or knowing how to accomplish that goal. I've been working with billing recently so let's say I have subscription and single purchase charges to process. One approach is to build a function that knows about the different kinds of charges and modifies the data differently depending on what it's dealing with. This can lead to a system that must check everywhere is the charge for a subscription or not? OOP works best when a program is a collection of objects passing messages to one another. One object runs the show and knows what messages to pass. The other objects know what to do when they get these messages. Sending prepare_for_billing works differently depending on the object handling the message, but I don't have to care about the details. I send the message and it works or raises an error. This let's me the programmer think abstractly about a series of general steps without worrying too much about the details. I start by writing \"bill the customer\" then I fill in some steps \"prepare for billing\" \"apply tax\" \"send to payment processor\" \"record invoice\". Finally I get to the details of exactly what happens to the data. Full transparency, this is just a different kind of complexity. There is inherent complexity in building a system that can handle subscription and single charges. OOP has each object define the same method differently, which can be repetitive. Functional systems have to define different functions for each case, or have one function that branches. The best results come from a balanced approach. Have a collection of objects send messages, and make sure the methods invoked by those messages avoid side effects outside of the object that handles it. It's good to try to make methods repeatable and idempotent. It's preferred if their output only depends on the input arguments. All of these are goals of functional programming and are desirable in object oriented programming too.<patrixxxx>: &gt; Have a collection of objects send messages, and make sure the methods invoked by those messages avoid side effects outside of the object that handles it. This is a very good rule of thumb. Thank you.", "num_messages": 4, "avg_score": 11.25}
{"start_date": "1544526804", "end_date": "1544541775", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 2205, "text": "<alecco>: How the Dreamcast copy protection was defeated <justavault>: Nice read. Never had a need to pirate something for DC, after all, there were barely two handful of playworthy games anyways, yet those were awesome. <RubenGM>: A Dreamcast with a VGA cable looked way better than the PS2 ever did. Playing Sega Rally 2 before the console was released out of Japan (a friend had an imported one) was jaw dropping. If you only cared about it when it was dead, yeah, the library might look small compared to the PS2. If you were interested from the start it was a great console hindered by a horrible company. Sega killed Sega. <justavault>: Product white knighting is a thing... it's interesting how people are so emotionally invested to immediately feel like they have to jump into defense and defend something that wasn't even argued. I liked the DC, always was a proponent of it. Totally was into the arcade games like Crazy Taxi or Sega Ralley or Phantasy Star Online and Shenmue, but it were pretty few games after all. <RubenGM>: PSO and Shenmue are arcade, now? Did you just check a list of \"best Dreamcast games\" to name drop a few? <justavault>: oh my... redditors, you have to be super precise because simple transfers are not in it. It is a random list starting with two arcade titles going further on to other titles which randomly jumped to my mind. FFS, redditors are so highly emotionally invest and such strong keyboard warriors. <RubenGM>: Oh, I'm sorry you keep talking about stuff you don't know. I won't do it again. <justavault>: Another emotional reaction... <RubenGM>: Yes, that's why I apologized, you keep getting your feefies hurt. <justavault>: You know that you are the one reacting emotional and always including another condescending note at the end?<RubenGM>: As emotional as one can be on lunch break. Maybe you need to check again what \"emotional\" means? Calling you out for using Shenmue as an example of an arcade game is anything but emotional. Crying if anyone called it boring would be, because it kinda is pretty boring if it's not your type... But it definitely is not arcade at all, sorry. <justavault>: &gt; Yes, that's why I apologized, you keep **getting your feefies hurt.** That's a defensive emotion-driven reaction. It's not a rational and neutral statement, it's emotional. &gt;Calling you out for using Shenmue as an example of an arcade game is anything but emotional Also, I didn't. I just counted some games and started with the first calling it arcade-esque. You projected the first adjective on the whole list. That's a semantic misinterpretation of yours. And you moreover am not able to comprehend my explanation that follows subsequently. Showing you that it only takes a very close easy to make transfer, I was expecting from everyone being able to read and knowing about the games. So, in other words, \"you want to be offended\", because you are highly emotional invested.<RubenGM>: &gt; That's a defensive emotion-driven reaction. It's not a rational and neutral statement, it's emotional. That's laughing at you, not emotional at all. And this too: Dude, you have the emotional stability of a child who just got told Santa isn't real. <Incoming>: *waaaaah, waaaaaah, you're emotionaaaaaal, waaaaaah*", "num_messages": 13, "avg_score": 169.6153846154}
{"start_date": "1543427362", "end_date": "1543695384", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 208, "text": "<craig081785>: How Postgres is more than a relational database: Extensions <grauenwolf>: This is a good introduction to the concept, but I should point out that most relational databases have something similar in intent. A modern relational database server, and by modern I mean made in the last 20 years, is meant to be a foundation that you can build upon. When people say \"not only SQL\" they should be looking at PostgreSQL, SQL Server, or Oracle for the \"not only\" part because they've never been about the SQL, it's about the data. Focusing on SQL is like focusing on REST/JSON when what you really should care about is what the server can do, not how it communicates.<sisyphus>: What other databases have foreign data wrappers; mature, efficient and indexable json, jsonb and xml types with proper query operators, grouping etc; extendability at every level from simple data types to full on sharded distributed transaction processing frameworks and so on? It's one thing to say 'do not just think relationally' but I don't know any other platform that supports all of the ACID properties and also makes doing non-relational things as easy as PG<grauenwolf>: &gt; What other databases have foreign data wrappers; That's called a \"linked server\" in SQL Server. And Microsoft has been doing distributed transactions in everything since the late 90's. <G00dAndPl3nty>: Yeah, but SQL server somehow forgot to implement CSV import. And no, it doesn't actually exist, only a terrible terrible hack in its place<grauenwolf>: CSV import is handled via SSIS or the GUI or Powershell scripts. Yea, it's weird that they don't have a T-SQL option, but that's not the same as not being able to do it at all. <G00dAndPl3nty>: The GUI is not CSV. Its a horrible hack<grauenwolf>: No idiot, you use the GUI to tell SQL Server which CSV file to load. <G00dAndPl3nty>: My point is that it doesnt support actually proper CSV. It doesnt support quoted columns, or columns with commas within them<grauenwolf>: It does if you use SSIS, or the GUI, or PowerShell. Or if you really need it to be in T-SQL, spend five minutes writing an extension in C#. <G00dAndPl3nty>: Nope, the GUI is actually called SSMS, and it does NOT import proper CSV. You can't import columns if they have newlines in the text, or commas because it doesnt handle quoted columns", "num_messages": 11, "avg_score": 18.9090909091}
{"start_date": "1543603726", "end_date": "1543649125", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 2, "text": "<svenefftinge>: A JSFiddle that works for any programming language and large projects <blacwidonsfw>: Like docker?<svenefftinge>: Not like Docker but with Docker and Kubernetes. It runs entirely in the browser.", "num_messages": 3, "avg_score": 0.6666666667}
{"start_date": "1543524934", "end_date": "1543650471", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 311, "text": "<DanielRosenwasser>: Announcing TypeScript 3.2 <DanielShuy>: interface XYZ { x: any; y: any; z: any; } type DropXYZ&lt;T&gt; = Pick&lt;T, Exclude&lt;keyof T, keyof XYZ&gt;&gt;; function dropXYZ&lt;T extends XYZ&gt;(obj: T): DropXYZ&lt;T&gt; { let { x, y, z, ...rest } = obj; return rest; } The power of TypeScript's type system never ceases to amaze me. Its amazing how it can still type infer features like these that were meant for a dynamically typed language.<ThirdEncounter>: Why is this surprising? Genuine question. TypeScript builds on top of a dynamically typed language, after all...<DanielShuy>: Features like the destructuring rest pattern `let { x, y, z, ...rest } = obj` were obviously meant for a dynamically typed language (JavaScript), because it dynamically creates a new type with all the fields of the type of `obj` (lets call it type `T`), except `x`, `y`, `z`. For TypeScript's type inference to continue working after performing after a destructuring rest pattern, it needs a type that can represent this, specifically `Exclude&lt;keyof T, \"x\", \"y\", \"z\"&gt;` (eg. trying to access `rest.x` will result in a compilation error), which is really impressive. Comparing this to a statically typed programming language like Java, you will lose type-safety if you use Reflection in Java. TypeScript however manages to keep type-safety and type inference even with these Reflection-like features.<Tubbers>: This doesn\u2019t really have to do with dynamic vs. static, it just speaks to the strength of TypeScript\u2019s type system. TypeScript IS statically typed, it\u2019s compilation target just happens to be JS.<eras>: I think it does, though, because TypeScript was build to be like a dynamic language, but with static typing. So the design decisions that were easy to take in a dynamic language now become complicated to ensure statically. Vice versa, you can take any static language (I'm waiting for counterproofs..) and make it dynamic by dropping the type annotations (if any). Most everything is simple in a dynamic environment, but can become quite hard when you need to do it before running the program.<JW_00000>: Tangentially related and not really important, but since you asked for it... &gt; you can take any static language (I'm waiting for counterproofs..) and make it dynamic by dropping the type annotations (if any). I don't think you can have overloading in dynamic languages like in statically typed languages. E.g, in pseudo-Java: ``` class A { ... } class B extends A { ... } void f(A a) { print(1); } void f(B b) { print(2); } B b = new B(); // b is a B and has type B A x = b; // x has type A but contains a B f(x); // This will print 1! ``` If you'd erase the types and execute this in a dynamic language with multimethods, this program would print 2. Which method to call is resolved at run time in dynamically typed languages but at compile time in statically typed ones. (I'm not entirely sure about this, so I'm also open to being proven wrong.)<igouy>: ? In actual-Java: class A { void f () { System.out.println(1); } } class B extends A { void f () { System.out.println(2); } } public class drop { public static void main(String[] args) { B b = new B(); A x = b; x.f(); } } $ /opt/src/openjdk-11+28/bin/javac drop.java $ /opt/src/openjdk-11+28/bin/java drop 2 And class A { final void f () { System.out.println(1); } } $ /opt/src/openjdk-11+28/bin/javac drop.java drop.java:6: error: f() in B cannot override f() in A void f () { System.out.println(2); } ^ overridden method is final<JW_00000>: Your snippet demonstrates overriding, not overloading. `x` should be an argument to `f`, not its `this`. Example: ``` class A { } class B extends A { } public class Test { void f (A a) { System.out.println(1); } void f (B b) { System.out.println(2); } public static void main(String[] args) { B b = new B(); A x = b; Test test = new Test(); test.f(x); } } ``` ``` $ javac Test.java $ java Test 1 ``` <Explanation>: in `obj.meth(arg)`, the type of `obj` is looked at dynamically, but which version of `meth` to call based on the types of its arguments is determined statically. Overriding = overwriting a method in a subclass; overloading = having different methods with different signatures.<igouy>: &gt; I don't think you can have overloading in dynamic languages like in statically typed languages. Yes, not if \"like in a statically typed language\" means using type information which won't exist in \"dynamic languages\".<alexeyr>: Right, which is why this was given as a counterexample to &gt;&gt; you can take any static language (I'm waiting for counterproofs..) and make it dynamic by dropping the type annotations (if any)", "num_messages": 11, "avg_score": 28.2727272727}
{"start_date": "1544772063", "end_date": "1544917257", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 12, "text": "<fagnerbrack>: TypeScript \u2013 was it worth? <tanguy_k>: Would be nice to have a publication date for the article<dpash>: That goes without saying for all articles in the internet. :)<Timbit42>: Sometimes I wish Google would down rank articles without dates but then everyone would game that.<dpash>: By adding dates? Isn't that what we want?<Timbit42>: I guess it'd be OK as long as every date ranked the same. <dpash>: Yeah, I don't think higher ranking for recent articles is sensible. Just rank undated articles lower.", "num_messages": 7, "avg_score": 1.7142857143}
{"start_date": "1544753456", "end_date": "1544820413", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 113, "text": "<TebbaVonMathenstein>: The Difference Between Interpreted Languages and Compiled Languages. <Candid_Calligrapher>: There's no such thing as an interpreted or compiled language.<shevegen>: That solely depends on the definition of the words used. Different languages claim to use OOP and from within the context of the respective definition, it's a proper definition. Of course most languages use an awful joke of what they call OOP but that is another issue.<inu-no-policemen>: &gt; That solely depends on the definition of the words used. No. Whether a language is compiled or interpreted is an implementation detail. People have written interpreters for C. And Dart which is usually JIT compiled can be also AOT compiled.<alexwh>: When you refer to a language being compiled/interpreted, you refer to the most popular implementations method, for convenience.<inu-no-policemen>: Okay. For Dart that would be source code JIT, (sort of) bytecode JIT, and AOT. There was also an interpreter at some point. All of these were official implementations by Google. And JavaScript started being interpreted and then many years later there was some JIT and V8 didn't have an interpreter stage until recently and then these engines do also Asm.js and Wasm stuff. Ugh. Yea, very convenient. Another problem is that this distinction doesn't really tell you much. AOT compiled code can be slower than JIT compiled code. And startup with JIT can be perfectly fine if there is an interpreter stage. And the final problem is of course that it's usually wrong. People like to call JS an interpreted language. It's not.<combinatorylogic>: &gt; Another problem is that this distinction doesn't really tell you much. Nope. It does. A lot. If you don't even have enough useful information in compile time to generate efficient code, you have even less information to reason about correctness of the code, to navigate it with precision, and so on. The difference between dynamic (interpreted in runtime, in any possible way, from a JIT compilation to the dumbest possible AST walking interpreter) and static (fully resolved in compilation time, optimised efficiently) is still fundamental.<inu-no-policemen>: &gt; The difference between dynamic [...] and static [...] is still fundamental. That's not the topic, though. This isn't an implementation detail either.<combinatorylogic>: Nope. It's entirely relevant. Dynamic = always *interpreted* parts of the language semantics, no matter how you actually implement this interpretation. Static = can be compiled. <inu-no-policemen>: &gt; Dynamic = always interpreted parts of the language semantics, no matter how you actually implement this interpretation. V8 didn't have an interpreter until recently. V8 used to generate native code right from the get-go. &gt; Static = can be compiled. Same goes for dynamic languages. It just means that more native code needs to be generated, because you have to cover more cases.<combinatorylogic>: &gt; V8 didn't have an interpreter until recently. JIT compiler *is an interpreter*, since it's using information only available in runtime. &gt; It just means that more native code needs to be generated, because you have to cover more cases. Nope. You cannot do it *efficiently*. Go on, implement a Tcl compiler that'd be more efficient than an interpreter.<inu-no-policemen>: &gt; JIT compiler is an interpreter No, these are two separate things. <https>://en.wikipedia.org/wiki/Interpreter_(computing) <https>://en.wikipedia.org/wiki/Just-in-time_compilation Ignition is the name of the interpreter which was added to V8. <https>://v8.dev/docs/ignition &gt; You cannot do it efficiently. That's not the topic.<combinatorylogic>: &gt; No, these are two separate things. You don't know much about interpretation and compilation, do you? Please avoid using wikipedia as an authority, ok? &gt; That's not the topic. It is. An ad hoc interpreter that you unavoidably embed in your \"compiled\" code is still an interpreter. <inu-no-policemen>: So how was V8 able to do JIT for all those years where it didn't have an interpreter? If the Wikipedia article is factually wrong, go ahead and fix it. You surely have plenty of sources to back your statements up. <https>://v8.dev/blog/ignition-interpreter Would you also say that the people who worked on V8 don't know the correct terminology?<combinatorylogic>: &gt; So how was V8 able to do JIT for all those years where it didn't have an interpreter? Again... JIT compiler *is* an interpreter. <inu-no-policemen>: That still isn't the case. You also haven't provided a source for that. <https>://en.wikipedia.org/wiki/Interpreter_(computing) &gt; In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. <https>://en.wikipedia.org/wiki/Just-in-time_compilation &gt; In computing, just-in-time (JIT) compilation, (also dynamic translation or run-time compilations), is a way of executing computer code that involves compilation during execution of a program \u2013 at run time \u2013 rather than prior to execution. Just read the first two paragraphs: <https>://v8.dev/blog/ignition-interpreter<combinatorylogic>: Did not I just tell you to stop referring to Wikipedia? And I'm absolutely not interested in whatever V8 developers have to say on this matter. Their choice of terminology is completely arbitrary - calling a bytecode execution engine an \"interpreter\", for example. JIT compiler is an interpreter. It does not matter at all how exactly interpreter is implemented, what matters is that it's dependent on an information only available in runtime. Even if you fully specialise an interpreter implementation vs. your source code, for a language with too many *dynamic* features the will be enough components of an interpreter remaining (such as, dynamic variable lookups, dynamic method dispatch, and so on) to call even this mode of execution an interpretation.<inu-no-policemen>: Didn't I just ask you to provide some sources? The C2 wiki doesn't agree with you either, by the way. &gt; Their choice of terminology is completely arbitrary - calling a bytecode execution engine an \"interpreter\", for example. Mh? You can interpret bytecode. Executing bytecode instruction by instruction is interpreting it. &gt; what matters is that it's dependent on an information only available in runtime According to whom?<combinatorylogic>: &gt; You can interpret bytecode. Executing bytecode instruction by instruction is interpreting it. Now, *when* exactly do you stop calling it an interpreter? Is an infinite call loop typical for the CPS implementations an \"interpreter\"? It's often called a \"mini-interpreter\", by the way, although it's completely meaningless. The only reasonable way to distinguish an interpretation is by an amount of dynamic dispatch it's doing. If your bytecode interpreter translate your bytecode into a direct threaded code first - is it still an \"interpreter\"? &gt; According to whom? Pretty much any PLT specialist out there.<inu-no-policemen>: &gt; Pretty much any PLT specialist out there. Should be easy then to find a source, right?", "num_messages": 20, "avg_score": 5.65}
{"start_date": "1543651253", "end_date": "1543694016", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 25, "text": "<DanySpin97>: Makefiles, Best Practices <AngularBeginner>: Is there a native Make available for Windows without WSL and Cygwin?<DeliciousIncident>: As the other user pointed out, make is available as part of mingw-w64 installation. It is indeed native.<jcelerier>: Well, mingw-w64 is still a descendant of cygwin<bitwize>: No. It isn't. It's a completely different thing. It doesn't depend on a GPLed POSIX shim.", "num_messages": 5, "avg_score": 5.0}
{"start_date": "1544777304", "end_date": "1544784920", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 0, "text": "<angelos_chalaris>: The State of the Octoverse: new open source projects in 2018 <shevegen>: Microsoft's self-promo knows no boundaries. It's also hilarious how they claim xyz without showing the data to any of these claims. &gt; All of this growth is thanks to the open source community. Right - simple patsies. Where did all that +7 billion dollars flow? Definitely not to the users. To the users only promo flows. &gt; Together, you\u2019ve built and collaborated on a broad spectrum of projects, &gt; from hobbies to professional tools and across varying developer &gt; experience levels. They need Github to do so? &gt; You also had a lot of fun, starring and contributing to gaming projects Wow - they can even analyse the psyche. Now you know that you had a \"lot of fun\"! I wonder which advanced method they employ to come to this conclusion. I myself can not really evaluate much at all from written text alone. &gt; New open source projects also helped you get work done with tools &gt; like denoland/deno for developing in TypeScript Never heard of this before. Or is that another promo? We now need TypeScript to get things done? &gt; We pulled the top 10 projects open sourced in 2018 based on &gt; the total number of stars they accumulated in their first 28 days &gt; on GitHub. That is a flawed way too. Back when I was using Github before the hostile MS take-over, I would use stars primarily to keep track of what may seem interesting but even then I would use this sparingly. That in itself does not say much about usage of any project. People could star but not use a project, so... nodejs react dotnet docker android machine-learning api ios cli vue That list is pretty sad. Are these topics very interesting? Node? Machine \"learning\"? I still don't understand why the word \"learning\" is used there. &gt; Cheers and congratulations to a year of new ideas emerging, &gt; knowledge gained, and continually changing the way we build &gt; and think about software development. Yeah! For example by not depending on [big company dominating the field] here. Strangely enough I have ideas without Github too.<AngularBeginner>: What a salty and jelly comment...", "num_messages": 3, "avg_score": 0.0}
{"start_date": "1543649604", "end_date": "1543771299", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 19, "text": "<nfrankel>: 5 Initiatives to Modernize Jenkins and Kill the Jenkinsteins <orthoxerox>: Jenkins is like democracy, the worst CI/CD tool except for all the others. Even then there are enough annoyances that make building a non-trivial pipeline hard (and if your pipeline is trivial any CI/CD tool works equally well). For example, there's no native way to lock some resource for several consecutive stages. You can work around that by wrapping them in another stage, but then you cannot parallelize them.<oorza>: [Lockable Resources Plugin](https://wiki.jenkins.io/display/JENKINS/Lockable+Resources+Plugin) has an example of doing exactly that: locking a resource and then running parallel jobs while holding it.<orthoxerox>: Sorry for being unclear, I was talking about declarative pipelines.<oorza>: It's a fair amount of boiler plate but you can just use the `script` step inside the `steps` block and still do it: pipeline { stage('Locking example') { agent none; steps { script { lock(label: 'My Lock') { parallel LockBranchA: { echo a }, LockBranchB: { echo b } } } } } }<orthoxerox>: It defeats the whole purpose of having multiple separate stages. My pipeline has to reserve a database, deploy the change, run the tests (in parallel), flashback the change and release the database. Of course I could stuff all of that into a script block, but that's not what a pipeline is for.<oorza>: It sounds like the sort of thing that I wrap up into a function in my Jenkinsfiles and just call the function from the actual pipeline: // shared library /vars/doDatabaseStuff.groovy def call() { lock('resource') { stage('A') { } stage('B') { parallel /* ... */ } stage('C') {} } } pipeline { stage('Do database stuff') { steps { doDatabaseStuff() } } } Generally though, I don't use the declarative pipeline for anything non-trivial because it just winds up causing a bunch more headaches than it's worth.", "num_messages": 7, "avg_score": 2.7142857143}
{"start_date": "1543500679", "end_date": "1545262192", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 38, "text": "<pawel_swiecki>: What covariance, contravariance, and invariance are and why you should know them to write better code. Multiple Python examples included <y_com>: A good way to explain why a List[Dog] is not a subtype of List[Animal] is the Liskov substitution principle: A subtype must offer all methods that the supertype offers, but List[Dog] can't add animals. For immutable or read-only containers this is not a problem, as nothing can be added in the first place.<3dprint_the_world>: I don't see why the Liskov substitution principle is incompatible with this, and I don't see why List{Dog} can't be a subtype of List{Animal}. To me, the sufficient and necessary condition for S to be a subtype of T is then there must be a function from S to T that 'forgets' its specific type. i.e. there must be a function f that takes a Dog as input and returns and Animal - it forgets 'dogness' and gets rid of any dog-specific data, in such a way that is invariant to all animal-specific functions. I.e. animal\\_weight(dog) = animal\\_weight(f(dog)). Such a function can be produced for all types that are subtypes of others. Now consider the functor List that takes types Dog and Animal to List{Dog} and List{Animal}. Then it's easy to see that we can construct such a subtyping function -- just call it for every Dog in the list. In other words, we can definitely take a list of dogs and return a list of animals. Looking at it this way, List{Dog} *can* add animals. The function that does that just takes a List{Dog}, an Animal, and returns a List{Animal}. I don't see what the problem is here. It may be an issue in the context of OO, where objects are mutable, but if all you have are immutable objects, then I really don't see why it causes a problem.<y_com>: &gt; The function that does that just takes a List{Dog}, an Animal, and returns a List{Animal}. I don't see what the problem is here. ~~The problem is that this function should return a List{Dog}. Otherwise you would then be unable to use the dog-specific information of the list-items.~~ <Edit>: Actually you are right, returning a new List{Animal} would satisfy the principle. But the original point still stands, this only works for immutable lists where \"add\" is side-effect-free.<3dprint_the_world>: It just seems to me like something of an arbitrary requirement to say that a subtype must offer all the methods of the parent in such a way that a function with signature f : Parent, (other stuff) ... ---&gt; Parent become f : Child, (other stuff) ---&gt; Child. What is the justification for this? From what I learned from category theory, I can't recall any requirement of this kind in order to have subtypes.<y_com>: Your way works fine with immutable lists, where \"add\" returns a new list. But when \"add\" has the side-effect of modifying the list and no return-value, this will not work.<3dprint_the_world>: Yes I do realize that, and that's what I said in my original comment, I'm just asking why the mutable way is the 'right' way here. <EDIT>: I suppose that that's part of the more general argument of the pros and cons of the functional vs. the object-oriented way, but in OP's article the implication is that it's talking about it not in the context of a specific programming paradigm. Hence my question.", "num_messages": 7, "avg_score": 5.4285714286}
{"start_date": "1543334699", "end_date": "1543955372", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 17, "text": "<jasonswett>: \"We don't have time to write tests\" <DutchmanDavid>: My problem with writing tests is that I just don't know what to test _for!_ I know the XUnit basics and have written two QuickCheck tests, but I usually draw a blank whenever I want to test a function. Thats what I would mean with \"no time to write tests\". <edit>: Thanks for all the protips, guys!<dpash>: Coverage reports can tell you what code is missing tests (but can't tell you what tests are missing). Writing high level tests can get you your most bang for your buck, so start there. Don't start trying to test an individual function. When ever you get a bug report, start by writing a test for it that fails, then fix the code so the code passes. If it's difficult to track down the bug, write tests that cover a smaller and smaller area. If you can't think what values to test with, start by writing method documentation: what parameters does it take? what does it return? what requirements do the parameters have? What exceptions or error codes might happen and under what circumstances? Then use that information to come up with good and bad inputs and check that you get the right responses.<vielga2>: &gt; method documentation: what parameters does it take? what does it return? what requirements do the parameters have? Sounds like a good use case for a good static type system. And I said *good*, as in *not java*.<dpash>: No, this applies to statically typed languages. Why do you think Java has Javadoc?<vielga2>: &gt; Why do you think Java has Javadoc Because java's type system sucks a lot of balls and is totally retarded and useless. Try **good** static type systems.<s73v3r>: No type system in the universe will remove the fact that the developer will need to know what the method takes and what it returns. <vielga2>: &gt; developer will need to know what the method takes and what it returns Yes, that's what static type systems (*good* static type systems, as in, not java) are for. Using proper types, anyone can tell what a method returns and takes and you don't need retarded \"javadoc\" crap (whatever that fucking idiocy means) or any other comment-based bullshit. don't you java idiots ever get tired of being so fucking backwards?<immibis>: What does `mysteryFunction: (UserId, CurrencyAmount) -&gt; DBAction[CurrencyAmount, ReadWrite]` do? How about `mysteryFunction: List[a] -&gt; a`?<vielga2>: &gt; What does mysteryFunction do? It causes me to immediately fire the retard who wrote the function with that stupid useless name, and sue them due to the monetary damage caused to the company, aducing that they willingly attempted to boicott the project they were working on. <immibis>: &gt; Using proper types, anyone can tell what a method returns and takes and you don't need [any] comment-based bullshit. &amp;nbsp; &gt; retard who wrote the function with that stupid useless name Either the static types tell you everything about the function, or they don't. Pick one. You can't have both.<vielga2>: using correct naming and strong, non-retarded (Hindley-Milner) type system, I've never used comments and I've never missed it.<immibis>: It started as \"the type system tells you everything\", now it's \"the type system plus the name tells you everything\". Now how will you know whether `transferMoney :: WithdrawableAccount -&gt; DepositableAccount -&gt; CurrencyAmount -&gt; Maybe DBAction` allows the source account balance to become negative or not?<vielga2>: &gt; Implying the limits of type systems are whatever shit language you're using.<immibis>: What type would you give it?<vielga2>: Stop bothering me. WPF is out at github today. I don't give a fuck about anything anyone has to say.", "num_messages": 16, "avg_score": 1.0625}
{"start_date": "1544551120", "end_date": "1544638460", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 78, "text": "<AlyoshaV>: Java 12 likely will not have raw string literals <vielga2>: lol retarded, pathetic, useless java. Meanwhile we already had this in C# for 10+ years. Don't you java idiots ever get tired of living in 1999?<lbkulinski>: Java has the advantage of being the last mover in some cases. They can see what works and what doesn't.<Ameisen>: The problem is that sometimes they don't move at all. C++ and C# have had this for a decade.<lbkulinski>: Oh, they move. Have you seen what they have delivered? Lambdas, modules, local variable type inference, etc. Have you seen what is in the pipeline? [Value types](https://openjdk.java.net/jeps/169), [specialized generics](https://openjdk.java.net/jeps/218), [pattern matching](https://openjdk.java.net/jeps/305), [data classes](https://cr.openjdk.java.net/~briangoetz/amber/datum.html), fibers, etc.<Ameisen>: They move *compared to what*, though? The only thing on that list that C++ doesn't have is pattern matching. Worse that they change modules in Java9, which is causing a lot of companies not to update because a ton of libs and packages are still dependant on Java 8. I use Java at work. Lombok is disallowed. We are stuck on Java 8. I use C++ at home. The nigh-complete lack of type inferencing and the extreme level of verbosity is just so incredibly frustrating.<lbkulinski>: The reason things are breaking with Java 9 is because people used internal APIs. They've been warned not to use them for 20 years. And what do you mean by lack of type inference? Java has had type inference since 2005 (generic method invocations).<vielga2>: &gt; people used internal APIs And they were forced to do so because java sucks balls and is retarded and what these people needed to achieve was probably not possible otherwise. Also, how come you java idiots don't have a way to simply hide a class from the outside world inside a \"jar\" or whatever shit so that people are not just flying around using the shit in unintended ways? lol the utter stupidity of java just grows and grows endlessly. &gt; has had type inference since 2005 (generic method invocations) Yes, because we all know that this is the only possible kind of type inference that exists in the universe.<dpash>: &gt; Yes, because we all know that this is the only possible kind of type inference that exists in the universe. Java has type inference in four different situations....<vielga2>: The length to which you all java retards will go out of your way to justify your language's utter stupidity is only comparable to the way the voters of the cat president here in my country will justify the most corrupt government in the history of mankind, while getting constantly raped in the ass, spending 50% of their salaries in the electric and gas bills. Similarily, you java idiots spend 80% of the time and effort writing useless boilerplate that shouldn't even exist to begin with, and only 20% useful code. That is why I can port a 10kloc java codebase into a 1000 loc C# one.<lbkulinski>: The length to which you go to discriminate against those who don\u2019t share your beliefs is disgusting. You keep trying to pull out all sorts of cards, but there are many reasons why people continue to use Java and the JVM over your crappy little CLR. Why aren\u2019t methods in C# virtual by default? You talk about boilerplate in Java, yet fail to see where it exists in your own language. And what is up with partial classes? Who thought that was a great idea? Every language has its problems, even copy cat C#.<vielga2>: &gt; Why aren\u2019t methods in C# virtual by default? Because that is [fucking retarded](https://www.artima.com/intv/nonvirtual.html), you clueless monkey. &gt; And what is up with partial classes? They are extremely useful for cases where you have a \"user code\" part of the class plus an autogenerated part of the class, and you can keep both separated, and thus cleaner, such as when using the Windows Forms designer, but of course you're too retarded to even grasp that. &gt; Every language has its problems, even copy cat C#. Of course it has a lot of problems, but funnily enough all problems that C# shows in 2018 are the retarded legacy from the time when it imitated java: `void` and `null` are C#'s worst problems in 2018. None of that makes java less pathetic, retarded, dinosaur, and useless. And none of that changes the fact that people like you who code in java are clueless retards, which is proved by the fact that you don't understand use cases for language features. And again, you keep calling C# \"copy cat\", because you live in 1999. In 2018 the situation is exactly inverse: java cannot stop copying C# because even oracle realizes they're 15 years behind and java is retarded.<lbkulinski>: &gt; Because that is fucking retarded, you clueless monkey. You love that word, don\u2019t you? The JVM is smart enough to perform optimizations, even if the method isn\u2019t marked `final`. &gt; void and null are C#'s worst problems in 2018. Are you an imbecile? These don\u2019t originate from Java... Blame C... You are being so vulgar... I\u2019m sure you act cowardly when you aren\u2019t hidden behind a computer screen. You keep throwing names at us when you\u2019re too dim to even realize that all languages are changing, and all languages copy off of each other. The world doesn\u2019t revolve around C#...<vielga2>: &gt; the JVM is smart enough to perform optimizations Too bad it isn't smart enough to have real generics and proper value types. Oh wait, they're trying to copy that so they can try to look a little less retarded and useless. Too bad it is being put forward by oracle, which means it will be inferior by definition. &gt; The world doesn't revolve around C# Of course not. Hell, there are good languages even in the JVM, such as Kotlin. But java? Ppfffft, that useless obsolete oracle crap does not have even ONE advantage or virtue compared to modern languages. I wouldn't touch that shit unless I got paid a million dollars per hour.<lbkulinski>: There is nothing wrong with Java's generics. Erasure is efficient and pragmatic -- one class file can represent all possible instantiations of `ArrayList`. With C#, `List&lt;int&gt;` and `List&lt;String&gt;` are different types. I still don't understand how you can stand here and criticize Java when C# is based off of it. Both languages are being evolved rapidly, and have more in common than you are willing to admit.<vielga2>: &gt; There is nothing wrong with Java's generics Of course not, that is why every java codebase I've seen in my life is literally littered with `Class class` useless shit and a lot of retarded reflection, to compensate for the language's utter stupidity. That is also why I can port a 10000 kloc java codebase into a 1000 loc C# codebase. &gt; List&lt;int&gt; and List&lt;String&gt; are different types And that is a really good thing. Please observe how we do dependency injection in C# versus the horrendous vomit inducing shit you do in java due to lack of real generics, please. &gt; I still don't understand how you can stand here and criticize Java when C# is based off of it No it isn't: - C# has properties, java doesn't. - C# has real generics, java doesn't. - C# has proper value types, java doesn't. - C# has LINQ, java has a pathetic useless imitation that only supports in memory stores. There is no way to convert a java \"stream\" crap into (for example) a SQL database call or a REST service request (which I've done myself using C# LINQ libraries) - C# has operator overloading, which as I mentioned is actually useful, contrary to your clueless opinion, java doesn't. - C# has async/await, java doesn't. - C# has `?.`, java doesn't. - C# has tuples, java doesn't. - C# has events, java doesn't. - C# has explicit interface implementations, java doesn't. - C# has `yield`, java doesn't. - C# has object initializer syntax, java doesn't. - C# named and optional arguments, java doesn't. - C# has exception filters, java doesn't. - C# has string interpolation, java doesn't. - C# has raw string literals (the whole subject of this thread), java doesn't. - C# has `nameof`, java doesn't. - C# has `[CallerMemberName]`, java doesn't. - C# has indexers, java doesn't. - C# has expression bodies, java doesn't. - C# has extension methods, java doesn't. I'm pretty sure I'm omitting a lot of other stuff here. See? now please show me how \"C# is based off of java\", please. Also, can you please show me the list of language features that java has that didn't already exist at least 10 years ago in C#, please?<lbkulinski>: Just because C# added features along the way does not me it isn\u2019t based off of Java. It\u2019s like Windows getting inspiration from macOS. And you keep throwing around how you can \u201cport a 10000 kloc java codebase into a 1000 loc C# codebase\u201d. That may be true, but the real question is how comprehensible will that codebase be? Reading code is more important that writing code, and while it may be easy to express the same thing in a shorter way in C#, that doesn\u2019t mean it will be as comprehensible to readers of your code like the longer Java version is. The architects do realize that there is some ceremony in the Java language, and [Project Amber](https://openjdk.java.net/projects/amber/) has been taking a look at that.<vielga2>: &gt; That may be true, but the real question is how comprehensible will that codebase be? The resulting C# is a million times more readable than the equivalent java, precisely because C# is a much more expressive language. Also, I eliminated entire parts of the library which do things that are literally given in C# (such as a half-assed, buggy attempt at string interpolation), or being able to serialize/deserialize an `T&lt;A, B&gt;` to and from JSON, which is literally a single line of code in C#, versus a bunch of horrendous hacks in java. Also got rid of like 1500 loc due to the utter stupidity of getters/setters crap, versus real properties in C#. &gt; Project Amber I see absolutely NOTHING of value in that, other than copying C# and Kotlin. **java is useless and has zero value.** I'm still waiting for you to name ONE (1) feature of the java language that doesn't exist in C# and that didn't exist at least a decade ago. <lbkulinski>: C# doesn\u2019t support wildcards. Nor does it support constants, `default` methods, `static` methods, or `private` methods in interfaces. Now don\u2019t go telling me that C# will be getting them soon, and so it \u201cdoesn\u2019t count\u201d. Java had them before C#. They essentially copied the semantics from Java.<vielga2>: &gt; C# doesn\u2019t support wildcards Because, unlike java, C#'s generics are not useless and retarded. Please show me a valid use case for that \"wildcard\" crap that can't be achieved using proper, real generics, please. &gt; constants in interfaces that is a horrible idea. Please show me a valid use case for that too. &gt; default methods Another horrible idea which got more downvotes than upvotes in the github discussion. We're only supporting this due to Xamarin, which unfortunately needs to be compatible with Android's retarded imitation of java. &gt; static methods Oh, you mean extension methods. Yes C# does that, moron. &gt; private methods in interfaces Another horrible idea that breaks OOP in fundamental ways. <Also>: I love how I've been able to mention 20 features and you barely mentioned three, which are really one.<lbkulinski>: Features are features. Whether or not you think they are useful is your own opinion. `private`methods in interfaces are nice for helper methods that may be used in `default` method implementations. I'm not going to keep arguing with you when you aren't open to any ideas that aren't negative about Java. The fact is that many use Java, and it is here to stay. The evolution of the language will ensure that it remains on top.<vielga2>: &gt; The fact is that many use Java, and it is here to stay Yes, the same can be said about php, but that doesn't make it less retarded and useless.<lbkulinski>: And that is your opinion with which I respectfully disagree :)", "num_messages": 23, "avg_score": 3.3913043478}
{"start_date": "1544780467", "end_date": "1544862211", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 75, "text": "<AdequateSource>: We, as an industry, should do our best to ensure law makers understand basic terminology <KHRZ>: Here's an idea. What if law makers do their best to understand basic terminology?<AdequateSource>: Yes - in an ideal world. But we are not making it easy for them. Cloud means 'a computer some arbitrary place', Artificial Intelligence means 'algorithms that take their environment into consideration' and Machine Learning means 'applied statistics'. Our whole terminology is geared towards marketing and investments, not explaining what it is and does.<NotWorthTheRead>: Every field has jargon. Specialists specialize their language to facilitate communicating with each other rather than laypeople because they communicate with each other about the subject matter far more. It's optimization of the common case. There's nothing wrong with that. Even lawyers do it. There are branches of law specifically dedicated to working with lawyer jargon because it's so impenetrable to laypeople. I'm not condoning this--I think 'legalese' is fine in-house but it should be the legal professionals' responsibility to translate at the border so that laypeople can understand for themselves. But that's getting off-track. In any case, even the examples you chose aren't quite right. 'Applied statistics' is a superset if 'machine learning.' You can't substitute one for the other, they don't mean the same thing. I don't expect lawyers and politicians to be subject matter experts--there are a lot of problems with that--but it is not solely our responsibility to become legal experts either. And we would have to become such to translate appropriately. We need to work in concert with each other. Unfortunately, that turns into the system of lobbyists and 'paid experts' we use now, and I have no practical solution for that. <AdequateSource>: Yes, my examples are an oversimplification. My intention was not to blame the field of trying to obscure what we do, but I do understand how it can be interpreted that way. My point is media use words like cloud, algorithms, artificial intelligence and machine learning in often incorrect ways. I am not trying to argue we should avoid using our proper terminology. I am trying to argue that peoples expectations when they hear the (overly) simplified term 'applied statistics' or 'fancy applied statistics' is way more in line with reality than the terminator / HAL 9000 they think of when they hear Machine Learning. My points is not that we should stop using our terminology, but that we should help give people an idea of what the terminology actually covers - even if that might be simplified. All fields have specialized terminology, but ours is being actively misused by congressmen that do not know that Google is not the maker of IPhone...<NotWorthTheRead>: We're not special there, either. Other fields that bump up against regulation face exactly the same problem and have for far longer than us. However you feel about gun regulations, witness the level of 'expertise' applied by some of the politicians involved, and that's with two hundred years and billions of dollars spent by people with way better lobbyists than ours. 'We need to make it clearer' isn't a practical solution when the audience *doesn't care anyway.* It's wasted energy. We can talk about making it clearer when we have a legislative body (and a media, which 'informs' the people who produce it) that cares about getting it right in the first place.<AdequateSource>: Ahh, you are American. No, this would not chance a single thing for you guys. I am talking about countries where public opinion play a key role in politics. But your gun case is perfect. Everyone has an opion on the matter - for or against, because it is quite simple to understand what a gun is. I would argue our field is different, as the majority of people have no clue what it actually 'does' - information technology is a black box to most. Guns are not. Farming is not. Fishing is not and traffic rules is not.", "num_messages": 7, "avg_score": 10.7142857143}
{"start_date": "1543626656", "end_date": "1543720479", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 59, "text": "<davidk01>: \"Categories for the Working Hacker\" by Philip Wadler <Organic_Choice>: Programmers don't need category theory, at all. You should only learn category theory if you're a mathematician who's bored and wants to know what a small group of autistic mathematicians are circlejerking about.<bdtddt>: Category theory is not \u2018needed\u2019 by programmers, but it is extremely useful, illuminating and interesting.<Organic_Choice>: Category theory is an esoteric field of math that has had very little use in real life. Perhaps one day it may be useful, but today is not that day. A knowledge of category theory does little or nothing at all for your programming skills. <NiveaGeForce>: Pure nonsense. Algebraic topology relies heavily on categrory theory, which is not esoteric at all. Knowledge of category theory is very helpful for coming up with good composable programming abstractions.<F-0X>: I mean, have you got any evidence of that claim? I learnt category theory as part of my masters in mathematics. What category theory is useful for, by and large doesn't apply much to practical programming. Programmers being aware of \"products\" and \"sums\" in the categorical sense clearly doesnt do that much good since even C has structs and unions which reify those very concepts (albeit some extra code will be needed compared to modern functional languages purely for type safety) but the concept of unions is something almost no C descendent decided to copy. Monads have become fashionable to basically sandbox propagating state down a pipeline of functions such that the overall function can be essentially pure - this is cool and all, but telling people that you need a bind and map function of certain signatures and how to use them is more practical than diving into natural transformations and making diagrams commute...<Drisku11>: There is no coproduct of monads. However since the free monad functor is left adjoint, the coproduct of two free monads exists as the free monad over the coproduct of underlying endofunctors (which exists because the underlying types category has coproducts). That's a practical question to answer (\"how do I compose monads in the 'or' sense generically\"), with the answer given via category theory (you don't, but here's a generic class for which you can). The monad abstraction is also not really about state. State propagation is one thing it describes, but really it just describes another flavor of composition which also works for asynchronous computations and short-circuiting error handling, among other things. Which really is probably the best way to sum up what category theory is: it's the study of compositional patterns and how they interact. It provides a vocabulary to talk about those patterns, and has the double edged sword that it can describe itself (i.e. it can describe the compositional behavior of compositional patterns), which in some ways simplifies things, but also leads to very abstract concepts very quickly. You don't need diagram chasing and the snake lemma, but it gives a useful vocabulary to talk about concepts that people use all the time, and having that vocabulary enables us to form thoughts and ask questions that we otherwise couldn't, which helps to understand the design space.<F-0X>: Right, so everything you've described there shows it's main place in programming: the creation of new languages. Both your points about what category theory can tell us has implications for the design of programming languages, but these particular details are not necessary for the users of the language - it is a different matter if the user is interested in which case by all means study it, but these details won't make anyone a better programmer in their day to day life, which is the point I would like to make. <Drisku11>: One approach to solving problems is by creating DSLs that describe the business domain, and then using that to implement business logic. Understanding the structure of languages is therefore useful for day-to-day programming as well. Even ignoring that, I've seen people complain about e.g. the lack of explicitness in how association works in do notation, when the point is the theory says that any choices that exist give the same result, so there's no need to be concerned with that as a programmer. Understanding that would free up mental energy for those people. Another way of thinking about it is that, to me, acknowledging that these abstractions are useful in language design but saying programmers don't need to know about them is like saying OOP design patterns are useful for framework design, but programmers don't need to know about them. Recognizing how a language is designed makes it easier to use that language.<earthboundkid>: Floating point numbers aren\u2019t associative. You need to use [Kahan summation](https://en.wikipedia.org/wiki/Kahan_summation_algorithm). So, one very simple, fundamental mathematical property doesn\u2019t apply to numbers as used in most real programming contexts. <Drisku11>: Okay? Function composition and Kleisli arrow composition are associative.", "num_messages": 11, "avg_score": 5.3636363636}
{"start_date": "1544639446", "end_date": "1544933296", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 42, "text": "<mycall>: BlazorFiddle - Blazor .Net Developer Playground &amp; Code Editor in the Browser <SamProf>: Blazor Fiddle now in stage Alpha 3. Main feature of this stage is Really FAST COMPILATION (now in Experimental mode). Later - more. =) [https://blazorfiddle.com/](https://blazorfiddle.com/) <mycall>: Excellent work!", "num_messages": 3, "avg_score": 14.0}
{"start_date": "1543299974", "end_date": "1543565755", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 4496, "text": "<kunalag129>: If it's not fun anymore, you get nothing from maintaining a popular package <MadDoctor5813>: I don\u2019t know if we\u2019re here to roast this guy or sympathize with him, but I think this pretty clearly demonstrates the flaw with \u201copen source means a bunch of people have looked at it, means it\u2019s secure\u201d type logic. Sure people *could* look at it. But if there\u2019s no one to pay them they\u2019ll probably find more productive uses of their time, and we don\u2019t have enough altruistic and competent people to keep the world running.<ojii>: I sympathize. I also used to have a bunch of OSS packages on Github that I maintained (Django, not node, but same thing really). These days I have no interest whatsoever in maintaining them anymore, so every time someone opens a bug report or submits a PR I simply ask them if they want to take over the project, as they are clearly more interested in it than me. Before you rush over to my GH to try to take over all my repos and inject bitcoin miners, all my \"important\" stuff is already transferred to someone I trust.<PandaTheRabbit>: So which unimportant projects can I inject bitcoin miners in without anyone knowing &lt;/pm&gt;<ojii>: May I recommend https://github.com/ojii/segfault ? Though I'm probably too attached to that code to give it away. <Ameisen>: Hmm, that C can be improved a bit.<ojii>: I don't know. That repo made a recruiter contact me because they were impressed with my C skills and offered me a job as a C Programmer, so...<Ameisen>: It won't fail on all ISAs though. There are architectures where address zero is defined. At the very least, you should write to NULL, not 0. NULL is only guaranteed to *compare* as zero, but is not guaranteed to have a value of zero.<flatfinger>: Very little code is, or should be, designed to run usefully on every possible ISA for which a conforming C implementation could be written. Further, relatively little code could run usefully on a freestanding implementation that did not \"behave in a documented fashion characteristic of the environment\" in at least some cases where the Standard imposes no requirements. In order for the Standard to actually be useful, it should recognize a variety of behavioral guarantees that many implementations could offer much more cheaply than programmers could work around their absence, and specify a means by which programs can indicate what guarantees they require [with implementations being free to reject any programs whose needs they cannot fulfill], or test what guarantees are supported. If a program needs to allocate space for 1,000 initially-null pointers, the logic required to support something like: #if __STDC_GUARANTEE(NULL_POINTER_ALL_BITS_ZERO) p = calloc(sizeof *p, 1000); if (!p) OUT_OF_MEMORY(); #else p = malloc(1000 * sizeof *p); if (!p) OUT_OF_MEMORY(); for (int i=0; i&lt;1000; i++) p[i] = 0; #endif would be simpler than the logic required to recognize that the loop could be optimized out of: p = calloc(sizeof *p, 1000); if (!p) OUT_OF_MEMORY(); for (int i=0; i&lt;1000; i++) p[i] = 0; It would be reasonable to criticize code that blithely assumes that all-bits-zero pointers will be null if there were any way for code to test that assumption. It is not reasonable to criticize programmers for assuming things not provided for in the Standard when the Standard fails to provide any reasonable alternatives. <Ameisen>: The standard doesn't say that writing to address `0` is undefined, but it *does* specify that writing to `NULL` is undefined. That's a good first step to making sure you're at least breaking something, possibly. `p = malloc(1000); if (!p) ...` will always work, as `malloc` returns `null` on failure, and `null` is guaranteed to *compare* as zero. It just isn't guaranteed to have a value of zero. The problem here is that he is presuming that *writing* to address `0` will segfault. There is no guarantee of such. Not all ISAs even have segmentation faults/paging faults, and not all ISAs have `0` as an illegal address. `NULL` is *always* an illegal address, though there's no guarantee that writing to it will fail, it's just undefined behavior.<flatfinger>: Many platforms do specify the effects of reading and/or writing address zero, and many implementations, including those targeting such platforms, use all-bits-zero as their representation for a null pointer. If a platform specifies that any attempted access to an address below 0x00010000 will segfault, and an implementation uses address zero to represent a null pointer, those facts would serve to define the behavior of an attempted access within the first 64K of an object identified by a null pointer on implementations that processes pointer accesses in a fashion consistent with their environments' documented characteristic behaviors. Obviously code which relies upon the fact that such accesses will segfault to guard against the possibility of anything worse happening would not be portable, but the authors of the Standard explicitly said that they had no intention of requiring that the language only be used to write portable programs. If all platforms upon which anyone would want to run a piece of code have a null-pointer behavior that would meet requirements, then the most efficient way for code to meet requirements would generally be to use that platform behavior to handle null cases, rather than loading the code down with null checks.<Ameisen>: My point is that while the actual value of a null pointer is implementation defined, it is guaranteed to always compare as zero. Ergo, testing a pointer as NULL, 0, or `!ptr` will always work.<flatfinger>: It will work, but may not always be the most efficient approach that would meet requirements.<Ameisen>: I'm not at all sure what your argument is. `if (!ptr)...` is a canonical way to check a pointer for nullness. If the value of a null pointer is not all-bits-zero, the compiler will handle it as appropriate.<flatfinger>: On many platforms, if one is going to need to access the memory identified by a pointer when it's not null, the cheapest way to force a program to trap if the pointer is null is to proceed with the access and let the hardware perform the check. Using `if (!ptr)...` will be more portable, certainly, but the only way I know of by which optimal machine code can be generated on the platforms where the platforms with hardware trapping is for the programmer to write code that proceeds with the access, oblivious to whether the pointer is null, and use a compiler that refrains from treating the non-nullity of the pointer as a \"jump the rails if violated\" precondition.<Ameisen>: The problem with that is if you start doing a write, and if *only* `NULL` (or an area around it) is treated specially, and they just `malloc` and then write to an offset from it, they might hit valid memory. The idea isn't to have the program hard fail when the address is `NULL`, but give the user the ability to do something in the `NULL` case (usually fail, but sometimes you can clean up objects or such). `malloc` is likely to be performing plenty of branches, so one additional branch on `NULL` isn't going to kill.<flatfinger>: Personally, I think the Standard should have provided allocation functions which would offer the programmer's choice of two guarantees: 1. The function will either return a pointer that can be safely used, or it will safely return null. 2. The function will either return a pointer that can be safely used, a pointer which will trap when it is used, or trap without returning. If a program isn't going to be able to do anything useful in case of an allocation failure, it shouldn't have to be cluttered with a null check after every allocation request. You're correct that from a performance standpoint, the particular of checking after null after an allocation isn't apt to be a performance issue, but it clutters the source code with logic that in most cases shouldn't be necessary to accomplish a program's intended purpose. In many other situations involving null checks, especially in contexts which involve process code which has been machine-translated from some other language, allowing the translator to exploit its knowledge of cases where the environment could safely provide null checks would allow code generation to be more efficient. For example, in C#, given: `someClassTypeReference.foo = 23;` [roughly equivalent to `someStructPointer-&gt;foo = 23;`] in C or C++, the generated machine code will include a null check if the offset of `foo` exceeds 65535, but omit it otherwise. If C or C++ is going to be usable as an efficient back end for languages that promise trap-on-null semantics, it should allow the translator to exploit trap-on-null features of the environment that it knows about.<Ameisen>: &gt; The function will either return a pointer that can be safely used, a pointer which will trap when it is used, or trap without returning. The problem is that the standard doesn't define what a 'trap' is, and not all architectures have a concept of that. In C++, `new` doesn't return `nullptr`, it throws... which is effectively free in the non-exceptional situation (C++ exceptions are faster and smaller than C error codes/value tests on modern architectures in the situation where throwing is rare). Of course, on some ISAs like AVR, exceptions are non-trivial to implement and not exactly 'good'. However... either you're doing the null check, or the library is doing the null check. At some point the value has to be checked.<flatfinger>: The Standard-mandated behavior for narrowing signed integer conversions invokes a similar concept, with the phrase \"raise an Implementation-Defined signal\", though I'd favor \"trap in an Implementation-Defined manner whose *default behavior* will force an abnormal program termination\". If a program calls an allocate-memory function in 50 places, which is going to be more space-efficient or cache-efficient--having one null check in a piece of code which gets called 50 times, or having a separate null check at every call site? Further, in most cases there would be no need for an allocation function to perform a null check unless it chains to a primitive that returns null on failure. If the allocation function is responsible for finding a suitable memory block, it will generally have one piece of code which is executed when a block is found, and a different piece of code that executes when it isn't. The latter code would have to check whether it's supposed to return null or force an abnormal program termination. While the fact that a particular pointer is non-null might trigger a successful return, the fact that a pointer is null wouldn't typically represent a failure condition in and of itself. Instead, it would indicate that a function needs to make additional efforts to find space. <Ameisen>: Sounds like you prefer the C++ approach where `new` throws.<flatfinger>: Requiring that functions maintain the ability to cleanly \"unwind the stack\" in response to an out-of-memory condition can be expensive or problematical, especially if nested calls are made between functions processed using different languages or implementations. Allowing for the possibility that recovery may not be practical in all implementations would seem more useful. More useful than trying to allow recovery from an immediate out-of-memory condition would be a means by which a program could request a reservation of storage which is guaranteed to be sufficient to hold any combination of up to N items totaling up to M bytes, with a successful request returning a token that could be passed to an allocation routine and--after all allocations were complete, to a routine which would release any storage that had been reserved but not allocated. Such a system could be implemented by having the reservation function reserve a block of storage of M+kN bytes, where k is the maximum per-block overhead, and then having the allocation either use space from the reserved block or \"holes\" in non-reserved storage, depending upon which seems more useful. While the reservation requests would have to be checked for success, individual allocations made from reservations would not have to be checked since they would be guaranteed to succeed. Even if programs were to vastly overestimate the amounts of memory they reserve for things, oversized reservation would get adjusted to reflect actual usage once code had allocated everything it was going to from a reservation block, or freed entirely if allocation requests could be satisfied elsewhere; consequently, this approach would waste some storage, but wouldn't continuously leak it.", "num_messages": 21, "avg_score": 214.0952380952}
{"start_date": "1539860016", "end_date": "1539868478", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 8, "text": "<Zhenmia>: 5 Reason Why Angular Js Framework is Popular? <AngularBeginner>: AngularJS != Angular. Two different frameworks. Gaurav Singh clearly doesn't know what he's talking about.<Zhenmia>: Can you please help me? Which lines need correction on that post?<bausscode>: AngularJS is \"pre-Angular2\". Angular is just Angular2. Angular was a complete rewritten version of AngularJS.", "num_messages": 4, "avg_score": 2.0}
{"start_date": "1544761331", "end_date": "1544876225", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 39, "text": "<davidk01>: Types, and why you should care <satchit0>: I am sorry, but is this honestly still a topic in nearly 2019??<blackn1ght>: Yes. There was a topic in /r/javascript not too long ago in regards to TypeScript, and there are devs (I'm guessing they've never used a typed language) who believe that as long as you name your variables correctly, then you don't need a typed language, and that types only holds you back. <CurtainDog>: Languages tend to have terrible support for names, and I'd say that's a contributing factor in the overreach of types.<ketralnis>: What do you mean by this?<CurtainDog>: I can name something in the code, but that name often doesn't survive compilation. And even when it does it's not that easy to get to. So it's little wonder that when you need to know what to call something at runtime you have to turn to the type of that thing. When really what you're looking for is the name of it.<ketralnis>: So you want to compile something, and then in the compiled binary get it back out by name?<CurtainDog>: It's not really just about getting it back out. It's about names being first class so that they're available to the runtime in the same way you or I would use names. To take a concrete example: if you get a null pointer exception in code you get some generic message and a line number. You then go back to the source to discover 'oh, foo is null here'. Why can't the program itself tell you foo is null? Because it has no name for the reference - it's just some memory address. I believe better language support for names could be used to tackle a similar set of problems to that tackled by type systems. The two sets don't completely overlap so there's room for both, but in modern languages there is much more low hanging fruit to be harvested from improving support for names than improving support for types.<detroitmatt>: most compilers have options that generate symbol tables that a debugger can use to translate memory addresses back into names<CurtainDog>: That's still not a first class name. Which language emits an error message (natively) to the effect of 'you tried to dereference foo, but it was null'? My point in this thread has been that in traditional statically typed languages names aren't given equal treatment to types, and as a result we've grown to think that types are more important. But this is just an artifact of the tools, rather than some deeper truth.<p-h>: You want everything to be a dictionary?", "num_messages": 11, "avg_score": 3.5454545455}
{"start_date": "1544781296", "end_date": "1544813019", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 139, "text": "<wprzecho>: How Netflix works: the (hugely simplified) complex stuff that happens every time you hit Play <invisi1407>: Interesting read, but not really _programming_ related. It's more about server architecture and microservices. The very last paragraph: &gt; There is a lot more trickery that goes on behind the scenes: as this interview explains, whenever you hit play on a show, Netflix will locate the 10 closest Open Connect boxes that have the show loaded on them. Your Netflix app/site will then try to detect which one of them is the closest or works fastest on your internet connection, and then load video from there. This is why videos start out blurry but then suddenly sharpen up \u2014 that is Netflix switching servers till it connects to the one that will give you the highest quality of video. That's really interesting to know - I always assumed it was because they were measuring my speed for a while before turning up the quality. <mikehawkisbig>: Maybe I\u2019m totally off-base here, but the blurriness is not from locating the correct open connect box it\u2019s due to the Adaptive Bit Rate doing its job. My understanding of ABR is the server is found, the UI player loads the manifest file (MPEG-DASH or HLS), the player loads the lowest quality files due to a low buffer, and once the buffer starts to fill up, the res switcher in the player will get the proper files based on what bit rate it can handle.<x18n>: Yeah, this sounds much more plausible. That's what we do with HLS (at a much smaller scale obviously). Not sure how he made that assumption. Jumping between servers only makes sense if you switch from Australia to USA to Europe (to give an extreme example), but for a European client it won't matter much whether the stream comes from the UK or Germany.<daxbert>: It does matter where you source content even when it's the same well connected continent. The current network paths to your streaming device can have vastly different streaming performance due to network saturation. Google has an interesting paper on how equal cost multi-path routing algorithms are still not widely adopted. This means a router will often choose one path to send packets rather than choose many concurrent paths. In short, you could be in Italy and the UK path would be less saturated and better performant for you, than the Germany path. This of course can change in the middle of a movie, so having the client always probing is a good thing. &amp;#x200B; I would guess that Netflix is using route data and performance data to actively track network health and even proactively move clients around to faster parts of the internet. &amp;#x200B; &amp;#x200B;", "num_messages": 5, "avg_score": 27.8}
{"start_date": "1544661258", "end_date": "1544796632", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 18, "text": "<DuncanIdahos8thClone>: Female Engineer Chats to James Damore: Sex Differences, Autism, and Advertising <WhereAreWeNowAnon>: The Left doesn't realize that their obsession with these things makes them the bigots that they claim to be fighting.<HalibetLector>: &gt; The Left doesn't realize Yes, they do. They don't care. They're not fighting bigots. They're grabbing power any way they can get it while claiming they aren't. They're exploiting the so called \"bigots\" good natures to do it. The whole thing is paradoxical, but it's working.<HalibetLector>: Is there a randomizer on my point score for this comment? I get a different score every time I refresh. <https>://imgur.com/a/eiDriOc<imguralbumbot>: ^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/uwnawFh.mp4** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ebrqnw0)", "num_messages": 5, "avg_score": 3.6}
{"start_date": "1543663431", "end_date": "1543766906", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 30, "text": "<tdwright>: Becoming a better supporter of women in tech <tchaffee>: Great article. You only have to browse many of the comments here to see how hostile our industry is to women. Or if you've simply asked women in our industry to tell you how they have been treated you'll get plenty of stories that should make you ashamed of how bad it is for women in our industry. The reason you have tech conferences for \"women only\" is because of just how creepy guys in our industry act towards women in a professional setting. I'm fully expecting to get downvoted because there are so many sexist devs out there who hate being confronted with how they act around female devs. But to end on a positive note, things continue to get better and that's thanks to the many devs who do put in the effort to make sure women in our industry have an environment to work in that is welcoming and free from discrimination. Keep up the good work. Some day the ugly stories from women in our industry will be few and far between thanks to those who are pushing for improvements.<markrtoon>: Holy cow this thread is disappointing. The article can basically be summarized at \"don't be a dick\", and yet people are losing their minds in the comments.<warlockface>: If this thread existed in isolation then I'd agree with you, but we all know that it's a small part of a litany of other threads and a much wider political/ideological drive. As with all things of this nature we should welcome opposition, even if we don't agree.<tchaffee>: I've never seen a thread about sexism in programming where the brogrammers don't pile on with yet more sexism, bullshit claims, and / or being dismissive. Can you point us to some threads where that's not the case?<warlockface>: Do you include the many women who dismiss claims of tech being a hotbed of misogyny in this \"brogrammers\" insult? I think a lot of *people* are tired of being accused of foul play and being presented with juvenile kafkatraps. I've said this before, but using the same terminology and tactics against predominantly female fields would be seen - rightly - as straw clutching sexist attacks. eg would you call psychologists \"sischologists\" or \"femchologists\" and *also* get away with accusing them of toxic behaviour and culture that excludes men and needs immediate rectification? Are we confusing equality with blatant chivalry?<tchaffee>: &gt;Can you point us to some threads where that's not the case? I'm still waiting. If it's common it should be super easy to show me discussions about sexism in tech where all the comments are respectful.<warlockface>: This thread currently has over 50% downvotes, which indicates that there are many people who are aware of the landscape. I'm not going to waste time on this pretence that you've never read any woman ever saying that the mild mannered men of tech aren't misogynistic cavemen that put 1970s welders to shame. The very idea is laughable.<tchaffee>: As I predicted. You can't find even one discussion about sexism in tech where the asshole sexist brogrammers don't show up. Carry on.<warlockface>: Your expectation that I was going to spend time searching for something involving many anonymous humans of unknown sex that could almost invariably be cherry picked and framed negatively, considering your loaded and imprecise line of questioning, is a bit na\u00efve. But I am sure that at a majority of readers with roots in the real world are capable of holding a more nuanced position in their minds and can consider the strong possibility that things aren't as certain interested parties present.<tchaffee>: You made the claim that \"but we all know that it's a small part of a litany of other threads and a much wider political/ideological drive\". So my expectation was that you could actually back up your own claim. If you can't, then I'll be happy to dismiss it as just talk. &gt;I am sure that at a majority of readers with roots in the real world Not those of us with science degrees or a love of **science and actual evidence to back up a claim**. Which I would hope is the majority of folks involved with computer science / programming. &amp;#x200B;<warlockface>: I have no need or desire to convince you, only to highlight that this is a multi-faceted issue for non-partisan readers. The scientists within us all should lead them to the truth, which will undoubtedly in between both extremist positions of everything being the fault of men and everything being the fault of women. Perhaps we all share the blame in our own ways and perhaps there is little to no blame to be shared for certain numerical outcomes? I'd much rather the thumbscrews weren't put on young men (many of whom had had very difficult upbringings) in the meantime.<tchaffee>: &gt;I have no need or desire to convince you, only to highlight that this is a multi-faceted issue for non-partisan readers. No one ever said it's not multi-faceted. But you made a specific claim. If you don't want to back up your claim with evidence, that says a lot. &gt;everything being the fault of men and everything being the fault of women No one here made either of those claims. That's a strawman. &gt;I'd much rather the thumbscrews weren't put on young men They aren't being put on the young men who aren't acting like assholes. The OP wrote a nice article about how he'll support women in tech. Zero wrong with that stance. And yet he got a lot of shit for it. That speaks for itself. &amp;#x200B;<warlockface>: You are still pretending that this article as a single entity and not part of a veritable sandblasting of similar articles. Please also don't pretend that young boys and men aren't being figuratively whipped with accusations of \"toxic masculinity\" etc that underpins this accusatory drive. Add to that the quasi-religious Original Sin of alleged historical oppression (again, a much more nuanced situation than typically presented) their forefathers engaged in and you have a recipe for disaffection, wrapped in a void of a complete lack of empathy for them. Sure, deal with the element of the differences in outcome that can be attributed to discrimination (and no reasonable person says this is literally zero for either sex). But don't psychologically abuse kids in the process.", "num_messages": 14, "avg_score": 2.1428571429}
{"start_date": "1543664703", "end_date": "1543687292", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 8, "text": "<MortiferaJ>: AWS Infrastructure as Code using AWS CDK <Bananabarbedwire>: I thought this was the point with cloud services, the ability to \"loan\" some computing power from someone. How is this different from that? Genuine question <MortiferaJ>: So cloud computing gives users the ability to use servers setup by the cloud computing provider (e.g. Google Cloud, Microsoft Azure, or Amazon Web Services). You essentially borrow usage of resources that are physically owned by the provider. The configuration of those resources still needs to be done by the user. And this is what CDK tries to simplify. When cloud computing as a platform first started, this was all manually done through a website. It was laborious and prone to human error. As time went on people wrote complex scripts to do the configurations but these only solved certain scenarios (like launching infrastructure in a new region but not necessarily updating the state of current infrastructure). Then later models like CloudFormation came about allowing infrastructure to be defined in a configuration file. Now this is the next evolution of that where you write code which essentially creates the CloudFormation configuration file. Hope this answers your question. I may have gone too in-depth. Tl;Dr this helps you get configure that computing power you wish to use in a much better way than we have now.<Bananabarbedwire>: The answer is good enough. I have an degree in telecommunications (it's more towards infrastructures) and I have the basic knowledge about the matter, so i when i saw this video the first thing got to my mind was \"hasn't this been done already?\". <digitalboi216>: Glad to hear the approach is intuitive enough that it seems like it should have already been the standard way to define infrastructure! MortiferaJ did a great job describing the switch in cloud infrastructure configuration/definition that's happening here \u2013 the industry has been using large, complex configuration files that are difficult to reuse/share/maintain and we are now seeing a switch to using full programming languages to define infrastructure. It enables easier sharing/reuse/abstraction with the standard language package managers, a more powerful toolset for defining infrastructure, and an improved authoring experience (ex: free IDE support for content assist, code navigation, refactoring, in-line documentation, etc). [https://github.com/awslabs/aws-cdk](https://github.com/awslabs/aws-cdk)", "num_messages": 5, "avg_score": 1.6}
{"start_date": "1543600765", "end_date": "1543665575", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 6, "text": "<f_r_d>: Do your part! Squash bugs for Kdenlive! (Quick reminder) <exorxor>: Why can't the developer that put the bugs there fix them?<nilamo>: wut?<exorxor>: Do you have a reading disability?<nilamo>: Oh ok, I get it now. Everything you say is nonsense.<f_r_d>: Seems to be the case.", "num_messages": 6, "avg_score": 1.0}
{"start_date": "1544568230", "end_date": "1544796660", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 31, "text": "<davidk01>: Firefox Developer Edition <shevegen>: \"This isn\u2019t just an update. This is Firefox Quantum: A brand new Firefox that has been rebuilt from the ground-up to be faster, sleeker, and more powerful than ever.\" Mozilla is good at exactly one thing: - promo.<PristineTransition>: I don\u2019t see you writing a browser.<gott_modus>: This is a naive insult. I'm not a fan of shevegen by any means, but if you're going to flame him you could at least provide an insult with actual value. Otherwise, you're just hopping on a bandwagon (and poorly, at that). <PristineTransition>: Second rate? Compared to Google? Sure. Though I doubt their policies have anything to do with that. But even if, unless you own or run a company that can compete with Mozilla in their space, which I doubt, don\u2019t shit on the thousands of people at Mozilla and Netscape that worked tirelessly to be here. Your attitude for their hard work is second rate.<gott_modus>: &gt;Second rate? Compared to Google? Sure. Though I doubt their policies have anything to do with that. But even if, unless you own or run a company that can compete with Mozilla in their space, which I doubt, don\u2019t shit on the thousands of people at Mozilla and Netscape that worked tirelessly to be here. Your attitude for their hard work is second rate. My answer's been edited, bruh. But ok, sure, let's talk about it. I will say that responding with the \"let's see *you* do better\" is exactly what my first comment was about. And my \"shitting\" on all their \"hard work\" also is irrelevant, even if we choose to debate this. All you have so far is appeal to morality. That means *nothing* in the context of this debate, which is whether or not Mozilla is or isn't 2nd rate as a company. Being judged as \"2nd rate\" in this context is dependent on the value they've added, how their contributions have affected us over time, the mentality behind their past failures, the motivation behind their current stance politically, and the current reasons for their existence. The only thing actually good they've chosen to support is Rust. Firefox *had* value years ago, before Chrome. These days it's more of a social banner, which doesn't equate to anything of real substance.<errorkode>: Kinda hard to follow this debate with edited comments, but I would like to point out that you don't get to decide what is or is not relevant in my choice of web browser. By the same token, you don't get to decide by what people measure the worth of a company by. Case in point, while you're entirely free to think moral issues are irrelevant in this context, that does not mean other people agree with that. The fact of the matter is, Firefox and Chrome are both perfectly serviceable web browsers. Yeah, we can have arguments about how many milliseconds faster one browser loads which page or how your JS console should be structured. We can talk about all the bells and whistles most users never touch and mostly exist so we can have nice tables to show all the things the other browsers can't. I'll freely admit, in many of those metrics Chrome is going to win in a photo-finish. But here's the thing: I've used all the major browsers at some time or other. Developed with Chrome for a while. These days I use Firefox and I'm fine. And I don't mean fine as in \"I can deal with it\", but as in \"I'm just as comfortable as I was before\". As a result, morality is an issue for me, no matter what you think. I like that while Mozilla surely has it's flaws, it has an interest in being transparent and taking the privacy of it's users seriously. It's their mission statement and they'll seriously fuck themselves if the fail. They have nothing to gain from betraying that. I like my privacy. It's a feature for me. Not to mention the fact that at his moment Firefox is the only big browser left that is not beholden to Google. It's vitally important for the market that there be more than one company with a good engine. Otherwise we'll have the IE6 issue all over again. Now, you don't have to care about such things, but don't belittle people who do. As it turns out, people value things differently.<gott_modus>: &gt;Kinda hard to follow this debate with edited comments, but I would like to point out that you don't get to decide what is or is not relevant in my choice of web browser. Where did I say that? I don't care what web browser you use and I also don't care how you determine what web browser you should use. &gt;By the same token, you don't get to decide by what people measure the worth of a company by. Case in point, while you're entirely free to think moral issues are irrelevant in this context, that does not mean other people agree with that. Any company that has to appeal to morality as a survival mechanism in this economy, especially when their competitors blatantly aren't, is essentially second rate. Capitalism doesn't give two fucks about human beings, for better or worse. Like it or not, that's the world we live in, and businesses don't adopt moral policies because they think it's \"the right thing\" to do. If you lap that shit up, you're naive. If you feel the need to defend the company, you're effectively *lobbying* for a business who doesn't care about you. &gt;The fact of the matter is, Firefox and Chrome are both perfectly serviceable web browsers. Yeah, we can have arguments about how many milliseconds faster one browser loads which page or how your JS console should be structured. We can talk about all the bells and whistles most users never touch and mostly exist so we can have nice tables to show all the things the other browsers can't. I agree this means very little. &gt;I'll freely admit, in many of those metrics Chrome is going to win in a photo-finish. But here's the thing: I've used all the major browsers at some time or other. Developed with Chrome for a while. These days I use Firefox and I'm fine. And I don't mean fine as in \"I can deal with it\", but as in \"I'm just as comfortable as I was before\". That's fine. My experience is the same, and I use it as a default in Linux as well. Still doesn't make what I'm saying invalid. &gt;As a result, morality is an issue for me, no matter what you think. I like that while Mozilla surely has it's flaws, it has an interest in being transparent and taking the privacy of it's users seriously. &gt;It's their mission statement and they'll seriously fuck themselves if the fail. And you're so sure they haven't failed already? Do you remember how apathetic they were about LGBT until Eich was appointed CEO and the backlash hit? Everything is tracked on the Internet. If you use Google as a search engine, through duckduckgo or otherwise, they still know what you're viewing. Cookies are trivially read and updated everytime you visit Google. If you delete everything each time you exit, your IP is still logged. Firefox alone isn't what gives you privacy. &gt; &gt;Not to mention the fact that at his moment Firefox is the only big browser left that is not beholden to Google. It's vitally important for the market that there be more than one company with a good engine. Otherwise we'll have the IE6 issue all over again. Firefox is totally necessary, in the same sense that AMD is necessary. Both have done useful things, but neither are leading in their respective domains. &gt;Now, you don't have to care about such things, but don't belittle people who do. First off, this whole discussion resulted from a dumb comment that wasn't even relevant. I *regretfully* chose to throw a snide remark about Mozilla into the critique of their post, mainly because I knew it would lead to this...and then chose to edit it out not long after. Looks as if that didn't particularly make much of a difference in the long run. But I still hold my position on that, regardless. As far as \"belittling\" people who feel otherwise: I'm taking apart bullshit arguments and showing how their wrong. That's all. If you think that's belittling, your perspective is skewed. &gt;As it turns out, people value things differently. And? What nature values is clear. It's perfectly reasonable to derive objective meaning from that alone, because it's representative of reality, and not tact-fueled idealism.<errorkode>: &gt; Any company that has to appeal to morality as a survival mechanism in this economy, especially when their competitors blatantly aren't, is essentially second rate. I don't even know where to start with this... Let's for a moment assume that companies do indeed work the way you seem to imagine. That is to say, any company is essentially an automaton with executives who aren't people but simple profit-automation systems. Even in that case, \"appealing\" to morality is an effective business strategy. Loads of people are completely willing to pay premium for perceived or real morality of their product. Think bio-labels, fair trade and clean energy. Tesla isn't just selling cars because they're cool, but also because it's customer feel good about helping save the climate (how legitimate that actually is when you buy a new car every year and jet around the world isn't usually considered). That being said, this is not actually how the world works. To come back to reddit's darling, mister Musk: Was founding a rocket company from scratch a good idea? No, it wasn't. It was ideologically motivated and still is. No sane share holder would green-light projects like the BFR. So the company isn't selling shares and keeps investing in business plans to could tank the entire company. Companies, as the name kind of implies, boil down to a group of people with a common goal. Those goals could be profit, but can just as well be to provide a service the company thinks is important for ideological reasons. Of course, we live in a capitalist system so at the end of the day you need money, but that doesn't need to mean profit. &gt; Like it or not, that's the world we live in, and businesses don't adopt moral policies because they think it's \"the right thing\" to do. Except they do. Let's take another tech example: The Linux Foundation. Tell me it's not an ideologically motivated company. Of course, the entities sponsoring it are often doing so because they use Linux for profit or simply for good publicity. Where I agree with you is that most publicly held companies these days are purely motivated purely by profit. They will sometimes do the right thing, but only motivated by potential profit. &gt; Capitalism doesn't give two fucks about human beings, for better or worse. Kinda unfortunately in today's climate, a lot of fucks are given about human beings. In the end it's always people who make and spend money. As such companies care about exploiting people, preferably without them knowing it. I think that might have been what you were trying to say, but still: Human beings are the beating heart of capitalism. Either as products, customers or workers. &gt; I agree this means very little. &gt; That's fine. My experience is the same, and I use it as a default in Linux as well. Still doesn't make what I'm saying invalid. &gt; Firefox is totally necessary, in the same sense that AMD is necessary. Both have done useful things, but neither are leading in their respective domains. So you're saying they're equal, one is just more equal. It seems to me, either one is clearly superior (which you say Chrome is) *or* they're roughly equivalent (which you say they are). &gt; Everything is tracked on the Internet. If you use Google as a search engine, through duckduckgo or otherwise, they still know what you're viewing. Cookies are trivially read and updated everytime you visit Google. If you delete everything each time you exit, your IP is still logged. I'm just assuming you tripped up in your sentence, because DuckDuckGo does not use Google. Anyway, I'm not stupid. But there are degrees. I mean you don't jump into a fire because you burnt your finger and now it doesn't matter anymore anyway. My point was simply this: A browser can make it easier to harder for you to keep privacy intrusions to a minimum. Now, which browser do you trust more, the one developed by a company making more than 90% of their revenue via targeted ads, or the independent non-profit? Same reason iOS is better for privacy than your run of the mill Android setup: Apple wants to sell devices, Google wants to sell your data. Mozilla doesn't have a business model that revolves around gathering as much data about me as possible. That's the difference. &gt; I'm taking apart bullshit arguments and showing how their wrong. If I may allow myself my own, regretful, snide remark: You're not doing a very good job. &gt; What nature values is clear. Reproduction? It's kinda the only reason we exist if you want to argue on that level. &gt; It's perfectly reasonable to derive objective meaning from that alone. Yes, I too like sex. But seriously, neither of us arguing \"objective\" truth. We're arguing world views, ideology and economy. None of those topics are in the realm of objective facts or natural laws.<gott_modus>: &gt;Even in that case, \"appealing\" to morality is an effective business strategy. I never said it wasn't. What you're failing to see here is that Mozilla was originally the people behind Netscape, and Netscape was a strictly for-profit business with investors in '94. They _pivoted into open source_ because Microsoft was fucking them. They didn't initially set out to do shit but make money. &gt;Loads of people are completely willing to pay premium for perceived or real morality of their product. Think bio-labels, fair trade and clean energy. Tesla isn't just selling cars because they're cool, but also because it's customer feel good about helping save the climate (how legitimate that actually is when you buy a new car every year and jet around the world isn't usually considered). Poor example for choice of argument: Tesla hasn't been shown to be widely successful yet as a company that's worth investing long-term in. Because that's your only example here, the argument itself means shit. &gt;That being said, this is not actually how the world works. To come back to reddit's darling, mister Musk: Was founding a rocket company from scratch a good idea? No, it wasn't. It was ideologically motivated and still is. No sane share holder would green-light projects like the BFR. So the company isn't selling shares and keeps investing in business plans to could tank the entire company. Yes, and has \"Mister\" Musk _really_ been successful as far as this is concerned? Do we see _actual_ evidence of us making these goals he has set out to achieve? Or has it all just been fun and games? Because I'm seeing the latter. Honestly, how can you take seriously someone who blatantly makes a 420 joke in some market pumping scheme, is accused of market manipulation for said pumping scheme, and then can't even effectively smoke a joint on a talk show in the aftermath from that? What does that say about someone? It clearly says that they don't give a _fuck_ about what they're doing beyond direct appearances. To them, it's all just well-executed publicity stunts that have literally no more value than John McAfee \"running\" for president in 2016. &gt;Except they do. Let's take another tech example: The Linux Foundation. Tell me it's not an ideologically motivated company. Oh, sure. But it's also one that has contributed more than Mozilla is capable of, at least currently. I'm not the biggest fan of Stallman, but I'll say that GCC et al is a huge reason alone to keep the FSF alive. They provide something of _value_ that's been _proven_ in industry, (that also hasn't created a crippled landscape of shitty programmers as a byproduct of their choices). They also started _out_ as the FSF, and didn't merely become one out of reaction. &gt;Of course, the entities sponsoring it are often doing so because they use Linux for profit or simply for good publicity. I see little reason for any business to use it for \"good publicity\": it's effectively dominated enough markets now on merit and financial advantage alone. &gt;Where I agree with you is that most publicly held companies these days are purely motivated purely by profit. They will sometimes do the right thing, but only motivated by potential profit. Yes, and since most of these are what exist in the landscape, they effectively have more influence over the climate than non-profits. &gt;&gt;Any company that has to appeal to morality as a survival mechanism in this economy, especially when their competitors blatantly aren't, is essentially second rate. &gt;I don't even know where to start with this... &gt;Let's for a moment assume that companies do indeed work the way you seem to imagine. That is to say, any company is essentially an automaton with executives who aren't people but simple profit-automation systems. By stating this, you're effectively stating that executives _aren't_ like this. Time and time again they've been shown to operate this way; i.e., the decisions they make always wind up being what effects their bottom line. And rightly so: that's how this world works. Organizations \"care\" about people in terms of how they can convince them that what they offer is of primary value over competing organizations. Usually this means putting on as much of a moral facade as absolutely necessary, but no more. &gt;Even in that case, \"appealing\" to morality is an effective business strategy. Loads of people are completely willing to pay premium for perceived or real morality of their product. Think bio-labels, fair trade and clean energy. Tesla isn't just selling cars because they're cool, but also because it's customer feel good about helping save the climate (how legitimate that actually is when you buy a new car every year and jet around the world isn't usually considered). I never said it was ineffective, and I _also_ never said that other businesses don't follow this. What I did say was that Mozilla _has_ to use this in order to survive. It's one of their business _assets_. Google's business model is a search engine and whatever other domain-of-the-half-decade they've chosen to attempt to monopolize legally. Their \"don't be evil\" slogan is really more of a meme now, than anything else. They blatantly and legally racketeer against other companies through entities like Project Zero. And yet, clearly, we see other companies who _don't_ operate solely on this principle. Oracle and Facebook are excellent examples of this. Facebook is_highly_ successful, because they offer something of value that other people struggle without, despite blatantly immoral practices on people's privacy. &gt;&gt;I agree this means very little. &gt;&gt;That's fine. My experience is the same, and I use it as a default in Linux as well. Still doesn't make what I'm saying invalid. &gt;&gt;Firefox is totally necessary, in the same sense that AMD is necessary. Both have done useful things, but neither are leading in their respective domains. &gt;So you're saying they're equal, one is just more equal. It seems to me, either one is clearly superior (which you say Chrome is) or they're roughly equivalent (which you say they are). No, I'm not saying they're equal. Note that we're discussing _companies_ and not _browsers_. There's a difference, here. Firefox works - I never said it didn't. On Linux I use it because I've been too busy to think \"oh, I should install Chrome\", and so far I've had zero reason to do so. &gt;I'm just assuming you tripped up in your sentence, because DuckDuckGo does not use Google. You're assuming wrong, because `!g` is a common enough prefix in DuckDuckGo searches, which causes the search to be encrypted. &gt;Anyway, I'm not stupid. But there are degrees. I mean you don't jump into a fire because you burnt your finger and now it doesn't matter anymore anyway. This is a poor analogy: you're assuming that in this case the fire hasn't already engulfed you. In the case of the Internet, it more or less has. &gt;My point was simply this: A browser can make it easier to harder for you to keep privacy intrusions to a minimum. Now, which browser do you trust more, the one developed by a company making more than 90% of their revenue via targeted ads, or the independent non-profit? The independent non-profit who accepts donations in the milllions from other large, for-profit businesses (Google included)? You _do_ realize that a large reason why Mozilla exists is to prevent companies from Google being accused of monopoly, right? I trust neither company, because, as far as this is concerned, neither have made decisions which show any form of integrity.", "num_messages": 10, "avg_score": 3.1}
{"start_date": "1544783731", "end_date": "1544863317", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 439, "text": "<mrcalm99>: Evelyn Berezin Word processor pioneer dies aged 93 <Loonacy>: The title makes it seem like her name is Evelyn Berezin Word and she was a processor pioneer.<Waldiboy>: I honestly thought they named the program Word after her for a second... I am not a smart man.<jones_supa>: I thought she was the pioneer of Microsoft Word.", "num_messages": 4, "avg_score": 109.75}
{"start_date": "1543667017", "end_date": "1543696245", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 9, "text": "<Little_Gap>: Why wouldn\u2019t they make an exchange fully decentralized? <AffectionateTell>: There\u2019re more than enough of various decentralized exchanges in the market, but they work slowly and no one needs them.<bxct>: &gt; There's more than enough of various decentralized ~~exchanges~~ cryptocurrencies in the market, but they work slowly and no one needs them.", "num_messages": 3, "avg_score": 3.0}
{"start_date": "1543640514", "end_date": "1543673028", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 48, "text": "<davidk01>: \"A Little Taste of Dependent Types\" by David Christiansen <tzjmetron>: I didn't like it much. I watched this yesterday, and it started out promising enough, but it seemed like too much of build-up towards Dependent Types (?) - stopped watching halfway through. Also, creating a new dummy language just for this sort of exercise (\"Pie\") seemed kind of forced - as if the presenter wanted to inject a bit of his Lisp-love into the whole matter. Haskell or Idris would have more than sufficed instead of a pseudo-Scheme.<bjzaba>: &gt; Also, creating a new dummy language just for this sort of exercise (\"Pie\") seemed kind of forced - as if the presenter wanted to inject a bit of his Lisp-love into the whole matter. Haskell or Idris would have more than sufficed instead of a pseudo-Scheme. In the book it works, because Pie gets rid of a lot of the complexities of Idris allowing you to learn the simple core the lies at the heart of all dependently typed languages. The important thing is learning how the dance between evaluation and type checking works, and Pie combined with the book does a good job at that. You can then open up the implementation to see how it works, and David Christiansen has [a nice implementation tutorial](http://davidchristiansen.dk/tutorials/nbe/) on his website. That said, the book probably takes longer than many people would have patience for to get up to steam - but it's worth it if you want to know how these kinds of type systems work but are having difficulty diving straight into the papers.", "num_messages": 3, "avg_score": 16.0}
{"start_date": "1543452165", "end_date": "1543673160", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 39, "text": "<carlosdavidepto>: De-facto closed source: the case for understandable software <combinatorylogic>: Bloated code = obfuscated code. Any deranged gopher, python fanboy or java monkey out there must take a note. What they're so proudly doing is not writing a \"simple\" code, it's shitting out tons upon tons of an obfuscated garbage with very little real substance inside.<Femaref>: which project are you talking about in terms of go(phers)?<combinatorylogic>: I am talking about the very ideology of this stupid language. They believe that by artificially limiting the level of abstraction available to the users, by dumbing down the language, they're somehow making code \"simpler\", while in fact they're just contributing to code bloat. Every piece of Go code is overbloated by design, and should have been much smaller and simpler (if written in an adequate language).<Femaref>: what's your adequate language of choice?<combinatorylogic>: Any language that is not deliberately built to hold your hands. At least, compile-time metaprogramming is a must.<carlosdavidepto>: *sensing the whiff of LISP in the air* What constitutes handholding? Checking arrays for boundary violations? Garbage collection? Typed variables? Everything in a programming language is an abstraction over the complexity of something else. Powerful, well implemented abstractions are priceless. It's just a matter of choosing the right ones for each job. C is nice for bootloaders, not so much for web applications. Go has compile time metaprogramming xD : //go:generate <combinatorylogic>: &gt; What constitutes handholding? Language design that makes it nearly impossible to implement higher level abstractions that do not leak. &gt; Powerful, well implemented abstractions are priceless. Some languages allow you to implement whatever abstractions you want. The other languages do everything possible in order to take this power away from you. Go and Python are of the second kind. &gt; Go has compile time metaprogramming xD : //go:generate Nope, shitty text generation does not count. <carlosdavidepto>: I like Scheme and Racket too. They are fun to play with. But I would only allow for a team higher level meta-abstractors to write code that relies heavily on meta-programming if they agree to do L3 support 24/7 themselves. When that thing fails in prod at 2am on a Saturday (and it will), I don't want clever metaprogramming. I want the generated text, thank you very much, so I can read it and make a mental model of what's going on. As much as I love to bash on PHP due to it being a crap platform in general, when you really need to solve problems you can't beat being able to sprinkle your live code with `mail()` calls or similar. The success that Go is enjoying is not due to the awesome features of the programming language, but to how pragmatic it is with respect to the fact that programmers are human and make mistakes left and right. It puts \"clever\" and \"dumb\" developers on an equal ground, gets them to communicate with the same constructs, and in general prevents you from writing downright stupid code. As a practical result of this, we get systems which are more reliable. Always a nice thing to have.<combinatorylogic>: &gt; But I would only allow for a team higher level meta-abstractors to write code that relies heavily on meta-programming if they agree to do L3 support 24/7 themselves. The main reason to use metaprogramming is to implement *eDSLs*. Meaning that each and every layer of abstraction should be dead simple and easy to understand for everyone with a bit of a domain knowledge. You do not need any special expertise to apply this methodology. You need a *discipline*. &gt; I want the generated text, thank you very much, so I can read it and make a mental model of what's going on. You must be insane to prefer text generation to an AST-based metaprogramming. The latter is dumb, very easily verifiable, easily debuggable and gives you all the tools support imaginable. Text generation is a buggy mess that's impossible to debug. Good luck trying to fix even a trivial issue with, say, Autotools. Also, good luck doing anything mildly non-trivial with C preprocessor. Can you have your IDE support for your generated code, for free? No way. Can you have a full debugger support? No way. &gt; but to how pragmatic it is with respect to the fact that programmers are human and make mistakes left and right. This is a lie that gophers spread. There is no tiniest bit of truth in it. Go makes it *easier* for humans to screw up, by forcing them to operate on a very low level of abstraction, severely obscuring the actual meaning of what they're doing. &gt; As a practical result of this, we get systems which are more reliable. Once again, this is just a lie that fanboys spread. Cannot be any further from the reality.<carlosdavidepto>: &gt; The main reason to use metaprogramming is to implement eDSLs. [...] You do not need any special expertise to apply this methodology. You need a discipline. I don't want to learn or create and maintain a new DSL for each new project. That's silly and wasteful. I prefer compiler enforced discipline to programmer enforced discipline. If, for nothing else, because I too get brain-fogged and lazy sometimes. &gt; You must be insane to prefer text generation to an AST-based metaprogramming. The latter is dumb, very easily verifiable, easily debuggable and gives you all the tools support imaginable. TIL I am bonkers o.O You're looking at it from a pure dev perspective, I'm looking at it from a support/maintenance role perspective too. &gt; Text generation is a buggy mess that's impossible to debug. Good luck trying to fix even a trivial issue with, say, Autotools. Also, good luck doing anything mildly non-trivial with C preprocessor. Can you have your IDE support for your generated code, for free? No way. Can you have a full debugger support? No way. I'm not a big fan of C's macros or Autotools. You're comparing C's preprocessor to go:generate, but they don't do exactly the same thing. go:generate output is plain, normal, top level go code which is part of the code tree (not magically created at compile time, only available in memory or randomly interspersed wherever in the compilation pipeline). IDE support for generate go code is easy, though: File -&gt; Open. Also, for all your quirky REPL, macro and AST needs: &lt;https://github.com/cosmos72/gomacro&gt; &gt; This is a lie that gophers spread. There is no tiniest bit of truth in it. Go makes it easier for humans to screw up, by forcing them to operate on a very low level of abstraction, severely obscuring the actual meaning of what they're doing. [...] Cannot be any further from the reality. Metrics which support your claim, please. Otherwise it's just an opinion. You may not like Go. That's ok. But reality is based on facts, not opinions. Fact is, people who write infrastructure code tend to like Go, because we have an unpleasant set of constraints imposed on us: - We jump across 10 different codebases per day. On a good day. - We wake up to fix things while people responsible for the non-existent unit test sleep. - Our code is never seen by end users. Our true users are other devs, so we have two different sources of support requests, with different, often competing priorities. Given this context, yes, I prefer to limit the number of DSLs to a bare minimum. Just give me boring code. <combinatorylogic>: &gt; I don't want to learn or create and maintain a new DSL for each new project. This makes you a far less efficient developer. Why would you want to be so inefficient? &gt; That's silly and wasteful. It's silly and wasteful not to implement DSLs for every tiny task you have. &gt; TIL I am bonkers o.O Of course you are, and your outlandish statement above (\"silly and wasteful\") confirms it. &gt; I prefer compiler enforced discipline to programmer enforced discipline. If, for nothing else, because I too get brain-fogged and lazy sometimes. And you're inconsistent as well. Do you realise that this is exactly one of the main reasons to *always* use DSLs? Can you enforce a discipline for *library* users? Nope. You can only do that with a DSL compiler restrictions. &gt; You're looking at it from a pure dev perspective, I'm looking at it from a support/maintenance role perspective too. Does not look like you understand the real costs of the long term maintenance. DSLs make maintenance orders of magnitude simpler and cheaper. &gt; IDE support for generate go code is easy, though: File -&gt; Open. And you call it an \"IDE support\"? Really? IDE support should include, at the very least: * Syntax highlighting * Type information (for statically typed DSLs) * Jump to declaration * Indentation * Docstrings Good luck doing it with some crappy ad hoc text generation tool like this go generate. And you conveniently skipped the debugger topic - good luck tracing the actual origin of a generated code when debugging. &gt; Metrics which support your claim, please. Otherwise it's just an opinion. LOC counts for any trivial task in Go are your metrics. Go is super verbose. &gt; Fact is, people who write infrastructure code tend to like Go, Look, now I don't even understand what you're talking about. I cannot even imagine the kind of \"infrastructure code\" where a shit like Go is usable. What exactly do you call \"infrastructure\"? I was under impression that I was working on the \"infrastructure\"-related stuff mostly for many years, which includes compilers, device drivers, low latency / real time networking and processing, HPC infrastructure, and so on. And all of it was very evidently much more efficient with higher level languages and tools - even if it's just something as simple as C++ template metaprogramming, it's already making a *huge* difference. &gt; We jump across 10 different codebases per day. On a good day. Sounds weird. What kind of an \"infrastructure\" is that? Anyway, this is exactly an area where DSLs shine. DSLs allow much easier context switching than different libraries on top of a single common language - they're much closer to a pseudocode and allow to understand things quickly without even knowing any details. &gt; We wake up to fix things while people responsible for the non-existent unit test sleep. Unit fucking testing is fucking useless anyway. Do the integration testing already. Also, debugging a code based on DSLs is orders of magnitude easier than debugging shitty low level code in a hand-holding dumb language. With such a dumb language you must keep a track of a much larger context when debugging, and if you're not very familiar with the code base, it takes an awful lot of time. &gt; Given this context, yes, I prefer to limit the number of DSLs to a bare minimum. There is absolutely no logical way to come to this insane conclusion. It's exactly the opposite - in such a situation you need as many DSLs as humanely possible. &gt; Just give me boring code. Why don't you code in assembly then? If you're so convinced that a very low level code is easier to maintain?<carlosdavidepto>: &gt; This makes you a far less efficient developer. Why would you want to be so inefficient? Because I'm not a machine and I do way more than developing software. I optimize for a variable balance of engagement, enjoyment, learning, results and lack of discomfort. You get to optimize for efficiency, if you want, but you don't get to tell me what to do. &gt; It's silly and wasteful not to implement DSLs for every tiny task you have. I have very broad knowledge of how tons of different software packages can be installed and configured on a specific system or set of systems for a given application/task and how they connect together in terms of system architecture, how to setup and operate CI/CD pipelines, and how to use all of the above to deploy applications for which I typically write plumbing code (middleware, error handling and reporting, tracking/monitoring, that sort of stuff) and assist with higher level SwEng stuff (architecture, refactoring, etc.). But I don't specialize in any particular domain. In other words, I see at least 3 different languages on any given day. Given this context, I really don't want another DSL. You can keep them. I have enough cognitive load already. &gt; &gt; TIL I am bonkers o.O &gt; Of course you are, and your outlandish statement above (\"silly and wasteful\") confirms it. Seriously? Do you behave like this at your workplace, or only on reddit? &gt; And you're inconsistent as well. Do you realise that this is exactly one of the main reasons to always use DSLs? Can you enforce a discipline for library users? Nope. You can only do that with a DSL compiler restrictions. You can do it in many different ways. But that is besides the point. The point is that tooling which prevents you from making mistakes is beneficial, DSL based or not. &gt; Does not look like you understand the real costs of the long term maintenance. DSLs make maintenance orders of magnitude simpler and cheaper. [...] Does not look like you understand the real costs of the long term maintenance. DSLs make maintenance orders of magnitude simpler and cheaper. Enlighten me then. I'll believe it when I see it. Show me a side to side comparison of eDSL based projects vs. say, a run of the mill PHP web app or something similar, one that goes beyond the basic SLOC count and feature implementation time. SLAs, bugs per feature introduced or modification and MTBFs at a bare minimum. &gt; IDE support should include, at the very least: &gt; &gt; - Syntax highlighting &gt; - Type information (for statically typed DSLs) &gt; - Jump to declaration &gt; - Indentation &gt; - Docstrings That's not IDE support. That's VIM's default settings (almost, you need ctags for JTD). And yes, all of those are supported for the generated code because it is just plain go code. It is part of the repository, as I said. Actual files on disk. Debuggers are a different story. \"In devop we are not believe in debugger, we are propose TDD... and print_r(); die();\" /s &gt; I cannot even imagine the kind of \"infrastructure code\" where a shit like Go is usable. What exactly do you call \"infrastructure\"? Kubernetes. Docker. Terraform. Vault. Nomad. All writen in Go. Ansible, which is written in Python (not infra, but manages it). And all the other things your web app needs but isn't your app really: Apache/NGINX, MySQL/PostGRE, Redis, ElasticSearch, HAProxy, etc. Infrastructure, in the context of web development. &gt; Sounds weird. What kind of an \"infrastructure\" is that? Anyway, this is exactly an area where DSLs shine. DSLs allow much easier context switching than different libraries on top of a single common language - they're much closer to a pseudocode and allow to understand things quickly without even knowing any details. I need to know the details when I'm troubleshooting at 3am. Again, the DSL won't help me there. All it will do is get in the way. Don't throw the L3 guy under the bus for the promise of \"better\" development using a DSL unless you can also promise and deliver a 0% bug rate in prod. &gt; Unit fucking testing is fucking useless anyway. Do the integration testing already. Integration testing, though crucial for a healthy codebase, is not helpfull during TDD and refactoring. That's why you need both. For small projects, I agree though, integration testing is sufficient for validation. &gt; Also, debugging a code based on DSLs is orders of magnitude easier than debugging shitty low level code in a hand-holding dumb language. With such a dumb language you must keep a track of a much larger context when debugging, and if you're not very familiar with the code base, it takes an awful lot of time. Not with proper use of encapsulation (in an OO environment) or locality (in procedural style). I get the feeling that you think Go is just C with a garbage collector bolted on. Not exactly the case. &gt; There is absolutely no logical way to come to this insane conclusion. It's exactly the opposite - in such a situation you need as many DSLs as humanely possible. You do that then. I'll try to lower my cognitive load instead so that I can perform better without burning out. &gt; Why don't you code in assembly then? If you're so convinced that a very low level code is easier to maintain? I said boring, not devoid of abstraction. For people who use go, it provides an adequate level of abstraction for the type of work being done. I wouldn't want to program a CI runner in Assembly, but I'm also not going to use Haskell for that, even though I like it a lot.<combinatorylogic>: &gt; Because I'm not a machine and I do way more than developing software. And this is exactly why you should want to reduce complexity as much as possible. To do your shit quickly and go home. &gt; I optimize for a variable balance of engagement, enjoyment, learning, results and lack of discomfort. You get to optimize for efficiency, if you want, but you don't get to tell me what to do. Efficiency is a prerequisite for all of the above. If you waste all your time on busywork (and you do, even if you don't want to admit it) instead of automating it all, everything else is beyond your reach. &gt; I have very broad knowledge of how tons of different software packages can be installed and configured on a specific system or set of systems for a given application/task and how they connect together in terms of system architecture, how to setup and operate CI/CD pipelines, and how to use all of the above to deploy applications for which I typically write plumbing code (middleware, error handling and reporting, tracking/monitoring, that sort of stuff) and assist with higher level SwEng stuff (architecture, refactoring, etc.). Yes, I was always laughing at you devops types for this. This kind of stuff that we, normal developers, do with expert systems and constraint solvers in no time, you're sweating out manually. If your goal is to look busy - then fine, keep doing it, but if you want to finish your work quickly and go home - than you're evidently doing it all wrong. If Cucumber was the best thing you devops could come up with, I really pity you. &gt; But I don't specialize in any particular domain. In other words, I see at least 3 different languages on any given day. And I see and use few dozens every day. This way I don't do too much hard work and keep my cognitive load low. &gt; Given this context, I really don't want another DSL. You can keep them. I have enough cognitive load already. How many times shall I repeat the obvious: the more DSLs you use, the *lower* your cognitive overload is. That's the very reason to use DSLs - to reduce complexity and cognitive load. That's the way human minds work - we always construct *languages* to hide away complexity. &gt; Seriously? Obviously. Say insane or ignorant things - be perceived as insane or ignorant. This is how it works. Your views are very high on the insanity scale. &gt; You can do it in many different ways. But that is besides the point. The point is that tooling which prevents you from making mistakes is beneficial, DSL based or not. And this is a good example of insanity. No, you cannot enforce discipline any other way. Either you do it on *semantics* level, or you fuck it up in runtime. No other ways, at least, no other ways discovered so far. &gt; 'll believe it when I see it. Show me a side to side comparison of eDSL based projects vs. say, a run of the mill PHP web app or something similar, one that goes beyond the basic SLOC count and feature implementation time. SLAs, bugs per feature introduced or modification and MTBFs at a bare minimum. Nope. I'd rather stay away from anything that reeks of the web shit. I have zero interest in even thinking about web, not just discussing it. I can give examples from the areas that matter (i.e., not web) - CAD/CAE, HPC, hardware design, software-hardware co-design, OS, real-time networking, signal processing, embedded, finance and so on. Leave the stinky web crap to the special school dropouts. &gt; That's not IDE support. That's VIM's default settings (almost, you need ctags for JTD). You have some DSL, which is a source for some Go (or whatever) code generated by a text processing tool. How the fuck you'll get a support for all of the above in Vim or whatever other IDE, without *implementing* all this shit on your own? &gt; And yes, all of those are supported for the generated code because it is just plain go code. It is part of the repository, as I said. Actual files on disk. I do not care about the generated boilerplate shit. I do not want to read it (unless I fucked up really majestically, and have to debug the code generation process itself). I want to read the *source* that was used to generate all that code. E.g., if it's a parser, I want to read BNF. If it's some ORM boilerplate, I want to read the database schema. If it's a rule engine, I do not want to see those shitty `if` ladders - I want to read the rules. And when this code is running, I want the debugger to point me to the actual source, not the generated intermediate shit, which, as I said above, I do not want to see, I do not want to think about it, I do not want to waste a second of my time with it - I want to finish my work quickly and go home. All the tooling help me with doing things with as little cognitive load as possible - debuggers included. &gt; Debuggers are a different story. \"In devop we are not believe in debugger, we are propose TDD... and print_r(); die();\" It's getting more and more hilarious. All the hipstor crap like docker and kubernetes first, now TDD and unit tests. You're evidently not interested at all in getting things done as quickly and simply as possible, you just want to look busy. &gt; For small projects, I agree though, integration testing is sufficient for validation. Unit testing in general and TDD in particular are absolutely useless (and often harmful) for most of the projects, regardless of their size. &gt; Not with proper use of encapsulation (in an OO environment) or locality (in procedural style). I get the feeling that you think Go is just C with a garbage collector bolted on. Not exactly the case. It'd be hilarious to see how you're debugging a horrible `if` ladder in your dumb pitiful Go, when in fact the underlying logic is just a couple of dozen of rules that won't take more than a hundred lines of code in any decent expert system. &gt; I'll try to lower my cognitive load instead so that I can perform better without burning out. You have no faintest idea of what you're talking about. DSLs are the *only* way known to our civilisation that allows to massively reduce your cognitive load. &gt; For people who use go, it provides an adequate level of abstraction for the type of work being done. No, it does not, and you know it. Go is never an adequate level of abstraction. Your problem domains do not even have such terms as \"function\", \"variable\", \"data type\", \"for loop\" and so on, all those dumb primitive constructs you're operating with when you're dealing with Go code. None of the stuff you described above can be adequately expressed in such terms. It must be expressed as rules, patterns and strategies, for example. <carlosdavidepto>: &gt; This kind of stuff that we, normal developers, do with expert systems and constraint solvers in no time, you're sweating out manually. You get to go home. The people who get paged when your l33t code fails don't, not in the \"I'm not working\" sense of the word. Maybe we shouldn't have 24/7 availability as a rule, or maybe we should require anyone who writes code for any particular code base to be available for the on-call rotation of it's deployment. Maybe. I don't know. What I do know is that you lack empathy and willingness to understand other people's problems. You're not even making an effort. It's your call, but the discussion goes nowhere in this case. DevOps exists because developers were being dicks to sysadmins. &gt; Yes, I was always laughing at you devops types for this. &gt; Leave the stinky web crap to the special school dropouts. &gt; Obviously. Say insane or ignorant things - be perceived as insane or ignorant. This is how it works. Your views are very high on the insanity scale. And I'm out. I'm not a \"devops type\". And I dropped out, yes, but of the tail end of a master's degree. You can disagree with me, and you can have a fundamentally different view of the world because your world (at least in terms of development) is not my world. But just so you know, I came from your world. I did implement distributed systems, filesystems and that sort of stuff back in Uni. When you finally have the solution to the problems we are facing in web development and are willing to share your vast and superior knowledge with the rest of us idiots (in a professional way or at least without being extremely unpleasant), let us know. We'd love to hear from you. Until then, I'll pass.<combinatorylogic>: &gt; The people who get paged when your l33t code fails don't, not in the \"I'm not working\" sense of the word. Maybe they cannot go home, but they still must want to do the troubleshooting as fast as possible. And only a pervasive use of DSLs can help with it. Your assumption that only the original developers would be able to understand such a code is plain ignorant - the main reason to do so is to make code readable for *everyone*. &gt; What I do know is that you lack empathy and willingness to understand other people's problems. I perfectly understand your problem - it's *ignorance*. &gt; You can disagree with me, and you can have a fundamentally different view of the world because your world (at least in terms of development) is not my world. Look, I'am too in a busyness where I can get a call any moment to go and fix some shit quickly, as any minute of downtime or an incorrect behaviour can cost literally millions. You cannot accuse me of not understanding your concerns. I perfectly understand them, and dismiss them as they're based on your ignorance, not on your experience. &gt; When you finally have the solution to the problems we are facing in web development Your stupid web development is just full of self-inflicted problems. There is not that much *natural* complexity in it, you created all of it on your own, and you deserve to suffer for what you've done. Docker, for fucks sake - who could have even invented such a crappy way of doing things?!? While a plain old chroot + debootstrap + layered FS do it all much better and in a much more controlled and *secure* way. Every single thing you mentioned is just a hipstor crap that should have never existed in the first place.", "num_messages": 16, "avg_score": 2.4375}
{"start_date": "1543596102", "end_date": "1543703566", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 7, "text": "<Graggee>: How to ILMerge unmanaged DLL? <birdbrainswagtrain>: How to &lt;name of some obscure software as a verb&gt;. Of all the ways you could have chosen to advertise this thing, this is the one you picked.<ivquatch>: ILMerge is a .net thing that most .net people would be familiar with.", "num_messages": 3, "avg_score": 2.3333333333}
{"start_date": "1543671034", "end_date": "1543719862", "thread_id": null, "subreddit": "programming", "subreddit_id": "t5_2fwo", "total_score": 679, "text": "<s0lly>: My Attempt at a Shadow Casting Algorithm <policjant>: It looks like it's a 3d scene with infinitely tall walls<s0lly>: Heh, true. If I mapped a texture / colour to those portions, it might look like actual walls too... ideas abound.<Treyzania>: Making the edges a little more diffuse would help with that too.<s0lly>: Thanks, I'll look into that.<SirClueless>: In particular, real shadows have what's called a \"[Penumbra](https://en.wikipedia.org/wiki/Umbra,_penumbra_and_antumbra),\" because real light sources are not usually perfect point sources. You can either try to model that accurately (for example by modeling your light source as a cluster of point lights) or you could just try and blur the edges of the shadow to simulate this. [Example.](https://en.wikipedia.org/wiki/Monaco:_What%27s_Yours_Is_Mine#/media/File:Monaco_What%27s_Yours_Is_Mine_-_Screenshot_4_(Casino_Heist\\).png) [deleted]: This only applies if it's supposed to be a shadow and not what the character can see from their position. In the latter case you only need to cast the shadow from one point since it's not like there is a penumbra or diffusion when you're looking past a wall, there's a hard boundary between what you can and can't see. <JezusTheCarpenter>: Hmm, technically it's not correct because we have two eyes sperated some distance from each other. So even if it is not strictly a soft transition, it's not really a perfectly sharp line. [deleted]: You still wouldn't have a penumbra in that case, you would more accurately model it by calculating what each eye can see and then let the player see everything that either eye can see (look past a wall with both eyes and with either eye - when looking with both you see what you see with the eye that has a better angle for looking past the wall). Practically speaking, your eyes are so close together that it's more economical to just use a single midpoint for calculation.", "num_messages": 9, "avg_score": 75.4444444444}
